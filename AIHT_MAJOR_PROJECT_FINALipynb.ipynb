{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Nd3LUAeoZSyQ",
        "outputId": "b42de313-a48f-43ba-f58a-7299675012b1"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Initial GPU memory: 16.25 MB\n",
            "Loading pre-computed features from features_session_1.pt...\n",
            "Loaded features with shape 31950\n",
            "Loading pre-computed features from features_session_2.pt...\n",
            "Loaded features with shape 31900\n",
            "Loading pre-computed combined features from features_combined.pt...\n",
            "Loaded combined features with shape 63850\n",
            "GPU memory before training: 16.25 MB\n",
            "Feature dimension: 2114\n",
            "Number of classes: 80\n",
            "Class weights: [0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 1.06400665 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 1.06400665 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 1.06400665 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total model parameters: 24,272,902\n",
            "\n",
            "Training model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/500 [Train]: 100%|██████████| 2794/2794 [00:47<00:00, 58.91it/s, loss=5.8834, acc=1.59%]\n",
            "Epoch 1/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 269.38it/s, loss=5.6759, acc=2.20%]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/500\n",
            "Train Loss: 5.3126, Train Acc: 1.59%\n",
            "Val Loss: 4.7420, Val Acc: 2.20%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.00      0.00      0.00       120\n",
            "           1       0.00      0.00      0.00       120\n",
            "           2       0.00      0.00      0.00       120\n",
            "           3       0.00      0.00      0.00       120\n",
            "           4       1.00      0.01      0.02       120\n",
            "           5       0.00      0.00      0.00       120\n",
            "           6       0.00      0.00      0.00       120\n",
            "           7       0.03      0.01      0.01       120\n",
            "           8       0.02      0.10      0.04       120\n",
            "           9       0.00      0.00      0.00       120\n",
            "          10       0.09      0.01      0.02       120\n",
            "          11       0.00      0.00      0.00       120\n",
            "          12       0.00      0.00      0.00       120\n",
            "          13       0.06      0.02      0.03       120\n",
            "          14       0.00      0.00      0.00       120\n",
            "          15       0.00      0.00      0.00       120\n",
            "          16       0.02      0.03      0.02       120\n",
            "          17       0.05      0.17      0.07       120\n",
            "          18       0.00      0.00      0.00       120\n",
            "          19       0.00      0.00      0.00       120\n",
            "          20       0.00      0.00      0.00       120\n",
            "          21       0.02      0.07      0.03       120\n",
            "          22       0.02      0.01      0.01       120\n",
            "          23       0.00      0.00      0.00       120\n",
            "          24       0.00      0.00      0.00       120\n",
            "          25       0.02      0.10      0.03       120\n",
            "          26       0.03      0.03      0.03       120\n",
            "          27       0.00      0.00      0.00       120\n",
            "          28       0.00      0.00      0.00       120\n",
            "          29       0.00      0.00      0.00       120\n",
            "          30       0.00      0.00      0.00       120\n",
            "          31       0.00      0.00      0.00       120\n",
            "          32       0.02      0.07      0.03       120\n",
            "          33       0.03      0.05      0.04       120\n",
            "          34       0.00      0.00      0.00       120\n",
            "          35       0.00      0.00      0.00       120\n",
            "          36       0.00      0.00      0.00       120\n",
            "          37       0.01      0.07      0.02       120\n",
            "          38       0.00      0.00      0.00       120\n",
            "          39       0.02      0.22      0.03       120\n",
            "          40       0.00      0.00      0.00       120\n",
            "          41       0.00      0.00      0.00       120\n",
            "          42       0.00      0.00      0.00       120\n",
            "          43       0.00      0.00      0.00       112\n",
            "          44       0.03      0.03      0.03       120\n",
            "          45       0.00      0.00      0.00       120\n",
            "          46       0.04      0.01      0.01       120\n",
            "          47       0.00      0.00      0.00       120\n",
            "          48       0.30      0.03      0.05       120\n",
            "          49       0.40      0.02      0.03       120\n",
            "          50       0.00      0.00      0.00       120\n",
            "          51       0.00      0.00      0.00       120\n",
            "          52       0.02      0.02      0.02       120\n",
            "          53       0.02      0.12      0.03       120\n",
            "          54       0.00      0.00      0.00       120\n",
            "          55       0.00      0.00      0.00       120\n",
            "          56       0.00      0.00      0.00       120\n",
            "          57       0.13      0.04      0.06       120\n",
            "          58       0.00      0.00      0.00       120\n",
            "          59       0.00      0.00      0.00       120\n",
            "          60       0.01      0.03      0.01       120\n",
            "          61       0.00      0.00      0.00       120\n",
            "          62       0.00      0.00      0.00       120\n",
            "          63       0.00      0.00      0.00       120\n",
            "          64       0.12      0.02      0.03       112\n",
            "          65       0.00      0.00      0.00       120\n",
            "          66       0.00      0.00      0.00       120\n",
            "          67       0.00      0.00      0.00       120\n",
            "          68       0.03      0.19      0.05       120\n",
            "          69       0.00      0.00      0.00       120\n",
            "          70       0.02      0.15      0.03       113\n",
            "          71       0.00      0.00      0.00       120\n",
            "          72       0.00      0.00      0.00       120\n",
            "          73       0.00      0.00      0.00       120\n",
            "          74       0.01      0.01      0.01       120\n",
            "          75       0.09      0.12      0.10       120\n",
            "          76       0.00      0.00      0.00       120\n",
            "          77       0.01      0.01      0.01       120\n",
            "          78       0.02      0.01      0.01       120\n",
            "          79       0.01      0.03      0.02       120\n",
            "\n",
            "    accuracy                           0.02      9577\n",
            "   macro avg       0.03      0.02      0.01      9577\n",
            "weighted avg       0.03      0.02      0.01      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/500 [Train]: 100%|██████████| 2794/2794 [00:47<00:00, 58.96it/s, loss=4.6732, acc=2.19%]\n",
            "Epoch 2/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 298.44it/s, loss=4.5763, acc=2.95%]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/500\n",
            "Train Loss: 4.7283, Train Acc: 2.19%\n",
            "Val Loss: 4.3887, Val Acc: 2.95%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.02      0.03      0.03       120\n",
            "           1       0.03      0.10      0.04       120\n",
            "           2       0.00      0.00      0.00       120\n",
            "           3       0.02      0.10      0.03       120\n",
            "           4       0.00      0.00      0.00       120\n",
            "           5       0.00      0.00      0.00       120\n",
            "           6       0.00      0.00      0.00       120\n",
            "           7       0.04      0.03      0.03       120\n",
            "           8       0.00      0.00      0.00       120\n",
            "           9       0.00      0.00      0.00       120\n",
            "          10       0.01      0.01      0.01       120\n",
            "          11       0.02      0.04      0.03       120\n",
            "          12       0.02      0.01      0.01       120\n",
            "          13       0.01      0.04      0.02       120\n",
            "          14       0.07      0.05      0.06       120\n",
            "          15       0.05      0.03      0.04       120\n",
            "          16       0.00      0.00      0.00       120\n",
            "          17       0.00      0.00      0.00       120\n",
            "          18       0.07      0.02      0.03       120\n",
            "          19       0.05      0.04      0.05       120\n",
            "          20       0.12      0.03      0.04       120\n",
            "          21       0.11      0.01      0.02       120\n",
            "          22       0.00      0.00      0.00       120\n",
            "          23       0.00      0.00      0.00       120\n",
            "          24       0.00      0.00      0.00       120\n",
            "          25       0.02      0.02      0.02       120\n",
            "          26       0.04      0.01      0.01       120\n",
            "          27       0.00      0.00      0.00       120\n",
            "          28       0.02      0.12      0.03       120\n",
            "          29       0.02      0.03      0.02       120\n",
            "          30       0.00      0.00      0.00       120\n",
            "          31       0.03      0.02      0.02       120\n",
            "          32       0.04      0.08      0.06       120\n",
            "          33       0.12      0.07      0.09       120\n",
            "          34       0.05      0.04      0.04       120\n",
            "          35       0.07      0.02      0.03       120\n",
            "          36       0.04      0.04      0.04       120\n",
            "          37       0.08      0.01      0.02       120\n",
            "          38       0.12      0.02      0.03       120\n",
            "          39       0.04      0.01      0.01       120\n",
            "          40       0.08      0.06      0.07       120\n",
            "          41       0.02      0.23      0.04       120\n",
            "          42       0.00      0.00      0.00       120\n",
            "          43       0.04      0.20      0.06       112\n",
            "          44       0.20      0.01      0.02       120\n",
            "          45       0.00      0.00      0.00       120\n",
            "          46       0.02      0.07      0.03       120\n",
            "          47       0.00      0.00      0.00       120\n",
            "          48       0.06      0.03      0.04       120\n",
            "          49       0.09      0.07      0.08       120\n",
            "          50       0.05      0.04      0.05       120\n",
            "          51       0.00      0.00      0.00       120\n",
            "          52       0.02      0.04      0.03       120\n",
            "          53       0.04      0.01      0.01       120\n",
            "          54       0.00      0.00      0.00       120\n",
            "          55       0.08      0.03      0.05       120\n",
            "          56       0.10      0.03      0.05       120\n",
            "          57       0.00      0.00      0.00       120\n",
            "          58       0.00      0.00      0.00       120\n",
            "          59       0.00      0.00      0.00       120\n",
            "          60       0.00      0.00      0.00       120\n",
            "          61       0.00      0.00      0.00       120\n",
            "          62       0.00      0.00      0.00       120\n",
            "          63       0.00      0.00      0.00       120\n",
            "          64       0.04      0.04      0.04       112\n",
            "          65       0.00      0.00      0.00       120\n",
            "          66       0.00      0.00      0.00       120\n",
            "          67       0.00      0.00      0.00       120\n",
            "          68       0.06      0.12      0.08       120\n",
            "          69       0.00      0.00      0.00       120\n",
            "          70       0.00      0.00      0.00       113\n",
            "          71       0.00      0.00      0.00       120\n",
            "          72       0.00      0.00      0.00       120\n",
            "          73       0.20      0.03      0.04       120\n",
            "          74       0.10      0.01      0.02       120\n",
            "          75       0.02      0.28      0.05       120\n",
            "          76       0.07      0.03      0.05       120\n",
            "          77       0.00      0.00      0.00       120\n",
            "          78       0.03      0.03      0.03       120\n",
            "          79       0.00      0.00      0.00       120\n",
            "\n",
            "    accuracy                           0.03      9577\n",
            "   macro avg       0.03      0.03      0.02      9577\n",
            "weighted avg       0.03      0.03      0.02      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/500 [Train]: 100%|██████████| 2794/2794 [00:47<00:00, 58.42it/s, loss=4.5177, acc=2.88%]\n",
            "Epoch 3/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 293.55it/s, loss=4.0045, acc=4.20%]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/500\n",
            "Train Loss: 4.4821, Train Acc: 2.88%\n",
            "Val Loss: 4.1975, Val Acc: 4.20%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.05      0.06      0.05       120\n",
            "           1       0.07      0.06      0.07       120\n",
            "           2       0.00      0.00      0.00       120\n",
            "           3       0.04      0.09      0.05       120\n",
            "           4       0.01      0.03      0.02       120\n",
            "           5       0.04      0.01      0.01       120\n",
            "           6       0.09      0.03      0.05       120\n",
            "           7       0.05      0.02      0.02       120\n",
            "           8       0.05      0.10      0.06       120\n",
            "           9       0.02      0.01      0.01       120\n",
            "          10       0.00      0.00      0.00       120\n",
            "          11       0.03      0.10      0.05       120\n",
            "          12       0.11      0.07      0.09       120\n",
            "          13       0.04      0.01      0.01       120\n",
            "          14       0.05      0.17      0.07       120\n",
            "          15       0.04      0.04      0.04       120\n",
            "          16       0.04      0.04      0.04       120\n",
            "          17       0.07      0.03      0.04       120\n",
            "          18       0.00      0.00      0.00       120\n",
            "          19       0.05      0.01      0.01       120\n",
            "          20       0.15      0.07      0.10       120\n",
            "          21       0.03      0.06      0.04       120\n",
            "          22       0.00      0.00      0.00       120\n",
            "          23       0.04      0.04      0.04       120\n",
            "          24       0.04      0.05      0.04       120\n",
            "          25       0.08      0.03      0.04       120\n",
            "          26       0.03      0.06      0.04       120\n",
            "          27       0.00      0.00      0.00       120\n",
            "          28       0.04      0.18      0.07       120\n",
            "          29       0.00      0.00      0.00       120\n",
            "          30       0.09      0.01      0.02       120\n",
            "          31       0.07      0.02      0.03       120\n",
            "          32       0.00      0.00      0.00       120\n",
            "          33       0.00      0.00      0.00       120\n",
            "          34       0.03      0.09      0.05       120\n",
            "          35       0.04      0.12      0.07       120\n",
            "          36       0.01      0.02      0.01       120\n",
            "          37       0.03      0.07      0.04       120\n",
            "          38       0.04      0.01      0.01       120\n",
            "          39       0.03      0.11      0.05       120\n",
            "          40       0.08      0.06      0.07       120\n",
            "          41       0.07      0.01      0.01       120\n",
            "          42       0.03      0.03      0.03       120\n",
            "          43       0.01      0.01      0.01       112\n",
            "          44       0.00      0.00      0.00       120\n",
            "          45       0.00      0.00      0.00       120\n",
            "          46       0.06      0.03      0.04       120\n",
            "          47       0.06      0.03      0.04       120\n",
            "          48       0.09      0.04      0.06       120\n",
            "          49       0.10      0.05      0.07       120\n",
            "          50       0.05      0.08      0.06       120\n",
            "          51       0.00      0.00      0.00       120\n",
            "          52       0.06      0.02      0.03       120\n",
            "          53       0.03      0.03      0.03       120\n",
            "          54       0.00      0.00      0.00       120\n",
            "          55       0.02      0.07      0.04       120\n",
            "          56       0.02      0.03      0.02       120\n",
            "          57       0.05      0.03      0.04       120\n",
            "          58       0.05      0.02      0.02       120\n",
            "          59       0.01      0.01      0.01       120\n",
            "          60       0.03      0.13      0.06       120\n",
            "          61       0.00      0.00      0.00       120\n",
            "          62       0.00      0.00      0.00       120\n",
            "          63       0.00      0.00      0.00       120\n",
            "          64       0.05      0.05      0.05       112\n",
            "          65       0.00      0.00      0.00       120\n",
            "          66       0.05      0.12      0.07       120\n",
            "          67       0.02      0.02      0.02       120\n",
            "          68       0.08      0.07      0.08       120\n",
            "          69       0.08      0.04      0.06       120\n",
            "          70       0.04      0.03      0.03       113\n",
            "          71       0.03      0.04      0.03       120\n",
            "          72       0.02      0.02      0.02       120\n",
            "          73       0.14      0.06      0.08       120\n",
            "          74       0.00      0.00      0.00       120\n",
            "          75       0.05      0.17      0.08       120\n",
            "          76       0.06      0.03      0.03       120\n",
            "          77       0.04      0.09      0.06       120\n",
            "          78       0.03      0.02      0.02       120\n",
            "          79       0.13      0.12      0.13       120\n",
            "\n",
            "    accuracy                           0.04      9577\n",
            "   macro avg       0.04      0.04      0.03      9577\n",
            "weighted avg       0.04      0.04      0.03      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.67it/s, loss=4.2250, acc=4.16%]\n",
            "Epoch 4/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 291.77it/s, loss=3.9720, acc=6.69%]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/500\n",
            "Train Loss: 4.2747, Train Acc: 4.16%\n",
            "Val Loss: 4.0396, Val Acc: 6.69%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.12      0.04      0.06       120\n",
            "           1       0.00      0.00      0.00       120\n",
            "           2       0.09      0.03      0.04       120\n",
            "           3       0.05      0.10      0.07       120\n",
            "           4       0.05      0.30      0.08       120\n",
            "           5       0.04      0.09      0.06       120\n",
            "           6       0.10      0.12      0.11       120\n",
            "           7       0.00      0.00      0.00       120\n",
            "           8       0.11      0.01      0.02       120\n",
            "           9       0.00      0.00      0.00       120\n",
            "          10       0.03      0.09      0.04       120\n",
            "          11       0.00      0.00      0.00       120\n",
            "          12       0.04      0.30      0.08       120\n",
            "          13       0.03      0.08      0.04       120\n",
            "          14       0.07      0.07      0.07       120\n",
            "          15       0.07      0.07      0.07       120\n",
            "          16       0.07      0.07      0.07       120\n",
            "          17       0.05      0.01      0.01       120\n",
            "          18       0.04      0.08      0.05       120\n",
            "          19       0.06      0.05      0.05       120\n",
            "          20       0.29      0.12      0.17       120\n",
            "          21       0.07      0.10      0.08       120\n",
            "          22       0.06      0.05      0.05       120\n",
            "          23       0.06      0.08      0.07       120\n",
            "          24       0.08      0.05      0.06       120\n",
            "          25       0.11      0.12      0.12       120\n",
            "          26       0.10      0.04      0.06       120\n",
            "          27       0.13      0.03      0.05       120\n",
            "          28       0.06      0.15      0.08       120\n",
            "          29       0.13      0.06      0.08       120\n",
            "          30       0.05      0.07      0.06       120\n",
            "          31       0.10      0.07      0.08       120\n",
            "          32       0.08      0.12      0.10       120\n",
            "          33       0.12      0.10      0.11       120\n",
            "          34       0.05      0.07      0.06       120\n",
            "          35       0.09      0.08      0.09       120\n",
            "          36       0.04      0.07      0.05       120\n",
            "          37       0.07      0.06      0.06       120\n",
            "          38       0.03      0.05      0.04       120\n",
            "          39       0.06      0.04      0.05       120\n",
            "          40       0.13      0.06      0.08       120\n",
            "          41       0.24      0.05      0.08       120\n",
            "          42       0.09      0.09      0.09       120\n",
            "          43       0.00      0.00      0.00       112\n",
            "          44       0.08      0.03      0.04       120\n",
            "          45       0.06      0.01      0.01       120\n",
            "          46       0.06      0.04      0.05       120\n",
            "          47       0.17      0.05      0.08       120\n",
            "          48       0.20      0.14      0.17       120\n",
            "          49       0.06      0.06      0.06       120\n",
            "          50       0.00      0.00      0.00       120\n",
            "          51       0.12      0.02      0.03       120\n",
            "          52       0.00      0.00      0.00       120\n",
            "          53       0.07      0.07      0.07       120\n",
            "          54       0.04      0.02      0.02       120\n",
            "          55       0.17      0.01      0.02       120\n",
            "          56       0.04      0.02      0.02       120\n",
            "          57       0.00      0.00      0.00       120\n",
            "          58       0.06      0.02      0.03       120\n",
            "          59       0.02      0.01      0.01       120\n",
            "          60       0.04      0.05      0.04       120\n",
            "          61       0.09      0.01      0.02       120\n",
            "          62       0.08      0.07      0.08       120\n",
            "          63       0.07      0.07      0.07       120\n",
            "          64       0.16      0.12      0.14       112\n",
            "          65       0.05      0.03      0.04       120\n",
            "          66       0.05      0.04      0.05       120\n",
            "          67       0.05      0.04      0.05       120\n",
            "          68       0.15      0.09      0.11       120\n",
            "          69       0.04      0.19      0.07       120\n",
            "          70       0.06      0.01      0.02       113\n",
            "          71       0.10      0.11      0.10       120\n",
            "          72       0.21      0.10      0.14       120\n",
            "          73       0.23      0.12      0.15       120\n",
            "          74       0.08      0.03      0.05       120\n",
            "          75       0.14      0.19      0.16       120\n",
            "          76       0.08      0.09      0.08       120\n",
            "          77       0.10      0.03      0.04       120\n",
            "          78       0.08      0.07      0.08       120\n",
            "          79       0.13      0.13      0.13       120\n",
            "\n",
            "    accuracy                           0.07      9577\n",
            "   macro avg       0.08      0.07      0.06      9577\n",
            "weighted avg       0.08      0.07      0.06      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 58.21it/s, loss=3.5542, acc=6.06%]\n",
            "Epoch 5/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 288.86it/s, loss=4.1259, acc=9.31%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/500\n",
            "Train Loss: 4.0902, Train Acc: 6.06%\n",
            "Val Loss: 3.8306, Val Acc: 9.31%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.08      0.05      0.06       120\n",
            "           1       0.10      0.04      0.06       120\n",
            "           2       0.16      0.04      0.07       120\n",
            "           3       0.06      0.09      0.07       120\n",
            "           4       0.12      0.13      0.13       120\n",
            "           5       0.00      0.00      0.00       120\n",
            "           6       0.15      0.12      0.13       120\n",
            "           7       0.13      0.10      0.11       120\n",
            "           8       0.14      0.06      0.08       120\n",
            "           9       0.05      0.05      0.05       120\n",
            "          10       0.00      0.00      0.00       120\n",
            "          11       0.11      0.07      0.09       120\n",
            "          12       0.06      0.21      0.09       120\n",
            "          13       0.10      0.09      0.09       120\n",
            "          14       0.18      0.14      0.16       120\n",
            "          15       0.13      0.02      0.03       120\n",
            "          16       0.13      0.07      0.09       120\n",
            "          17       0.06      0.15      0.08       120\n",
            "          18       0.06      0.17      0.09       120\n",
            "          19       0.09      0.12      0.10       120\n",
            "          20       0.16      0.13      0.15       120\n",
            "          21       0.06      0.04      0.05       120\n",
            "          22       0.10      0.11      0.10       120\n",
            "          23       0.07      0.12      0.09       120\n",
            "          24       0.08      0.04      0.05       120\n",
            "          25       0.14      0.10      0.12       120\n",
            "          26       0.35      0.05      0.09       120\n",
            "          27       0.10      0.09      0.09       120\n",
            "          28       0.11      0.10      0.11       120\n",
            "          29       0.10      0.20      0.13       120\n",
            "          30       0.10      0.10      0.10       120\n",
            "          31       0.13      0.13      0.13       120\n",
            "          32       0.13      0.07      0.09       120\n",
            "          33       0.15      0.07      0.09       120\n",
            "          34       0.03      0.07      0.05       120\n",
            "          35       0.07      0.12      0.09       120\n",
            "          36       0.08      0.17      0.11       120\n",
            "          37       0.10      0.13      0.12       120\n",
            "          38       0.05      0.02      0.02       120\n",
            "          39       0.09      0.08      0.08       120\n",
            "          40       0.09      0.23      0.13       120\n",
            "          41       0.11      0.18      0.13       120\n",
            "          42       0.05      0.17      0.08       120\n",
            "          43       0.08      0.07      0.07       112\n",
            "          44       0.16      0.12      0.13       120\n",
            "          45       0.04      0.07      0.05       120\n",
            "          46       0.09      0.07      0.08       120\n",
            "          47       0.12      0.10      0.11       120\n",
            "          48       0.18      0.12      0.15       120\n",
            "          49       0.03      0.03      0.03       120\n",
            "          50       0.10      0.06      0.07       120\n",
            "          51       0.12      0.06      0.08       120\n",
            "          52       0.11      0.04      0.06       120\n",
            "          53       0.12      0.07      0.09       120\n",
            "          54       0.07      0.03      0.04       120\n",
            "          55       0.05      0.04      0.05       120\n",
            "          56       0.13      0.13      0.13       120\n",
            "          57       0.11      0.04      0.06       120\n",
            "          58       0.04      0.14      0.06       120\n",
            "          59       0.05      0.05      0.05       120\n",
            "          60       0.04      0.06      0.05       120\n",
            "          61       0.09      0.07      0.08       120\n",
            "          62       0.21      0.12      0.15       120\n",
            "          63       0.00      0.00      0.00       120\n",
            "          64       0.21      0.14      0.17       112\n",
            "          65       0.22      0.05      0.08       120\n",
            "          66       0.20      0.14      0.17       120\n",
            "          67       0.09      0.07      0.08       120\n",
            "          68       0.17      0.13      0.15       120\n",
            "          69       0.12      0.13      0.12       120\n",
            "          70       0.12      0.02      0.03       113\n",
            "          71       0.10      0.02      0.03       120\n",
            "          72       0.12      0.07      0.09       120\n",
            "          73       0.27      0.15      0.19       120\n",
            "          74       0.05      0.03      0.04       120\n",
            "          75       0.18      0.24      0.21       120\n",
            "          76       0.09      0.15      0.11       120\n",
            "          77       0.21      0.05      0.08       120\n",
            "          78       0.09      0.15      0.11       120\n",
            "          79       0.14      0.16      0.15       120\n",
            "\n",
            "    accuracy                           0.09      9577\n",
            "   macro avg       0.11      0.09      0.09      9577\n",
            "weighted avg       0.11      0.09      0.09      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/500 [Train]: 100%|██████████| 2794/2794 [00:47<00:00, 58.23it/s, loss=4.2787, acc=7.87%]\n",
            "Epoch 6/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 239.92it/s, loss=3.1859, acc=11.21%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/500\n",
            "Train Loss: 3.9304, Train Acc: 7.87%\n",
            "Val Loss: 3.6781, Val Acc: 11.21%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.19      0.03      0.06       120\n",
            "           1       0.06      0.03      0.04       120\n",
            "           2       0.06      0.04      0.05       120\n",
            "           3       0.10      0.04      0.06       120\n",
            "           4       0.07      0.12      0.09       120\n",
            "           5       0.12      0.07      0.09       120\n",
            "           6       0.16      0.10      0.12       120\n",
            "           7       0.14      0.18      0.16       120\n",
            "           8       0.20      0.17      0.19       120\n",
            "           9       0.14      0.10      0.12       120\n",
            "          10       0.05      0.08      0.06       120\n",
            "          11       0.15      0.12      0.13       120\n",
            "          12       0.13      0.20      0.16       120\n",
            "          13       0.05      0.06      0.05       120\n",
            "          14       0.16      0.16      0.16       120\n",
            "          15       0.25      0.07      0.12       120\n",
            "          16       0.09      0.05      0.06       120\n",
            "          17       0.14      0.09      0.11       120\n",
            "          18       0.06      0.17      0.09       120\n",
            "          19       0.17      0.11      0.13       120\n",
            "          20       0.08      0.12      0.10       120\n",
            "          21       0.06      0.07      0.07       120\n",
            "          22       0.07      0.03      0.04       120\n",
            "          23       0.09      0.18      0.12       120\n",
            "          24       0.17      0.23      0.20       120\n",
            "          25       0.14      0.19      0.16       120\n",
            "          26       0.13      0.07      0.09       120\n",
            "          27       0.32      0.06      0.10       120\n",
            "          28       0.10      0.23      0.14       120\n",
            "          29       0.10      0.30      0.15       120\n",
            "          30       0.13      0.15      0.14       120\n",
            "          31       0.15      0.16      0.15       120\n",
            "          32       0.19      0.06      0.09       120\n",
            "          33       0.19      0.07      0.11       120\n",
            "          34       0.06      0.15      0.09       120\n",
            "          35       0.13      0.16      0.14       120\n",
            "          36       0.09      0.07      0.08       120\n",
            "          37       0.13      0.07      0.09       120\n",
            "          38       0.11      0.07      0.09       120\n",
            "          39       0.15      0.13      0.14       120\n",
            "          40       0.13      0.17      0.15       120\n",
            "          41       0.11      0.15      0.12       120\n",
            "          42       0.14      0.14      0.14       120\n",
            "          43       0.16      0.18      0.17       112\n",
            "          44       0.31      0.12      0.17       120\n",
            "          45       0.05      0.02      0.03       120\n",
            "          46       0.06      0.09      0.07       120\n",
            "          47       0.07      0.13      0.09       120\n",
            "          48       0.21      0.23      0.22       120\n",
            "          49       0.08      0.15      0.10       120\n",
            "          50       0.20      0.12      0.15       120\n",
            "          51       0.13      0.19      0.15       120\n",
            "          52       0.20      0.05      0.08       120\n",
            "          53       0.08      0.07      0.08       120\n",
            "          54       0.13      0.08      0.10       120\n",
            "          55       0.17      0.07      0.10       120\n",
            "          56       0.16      0.06      0.09       120\n",
            "          57       0.25      0.04      0.07       120\n",
            "          58       0.05      0.18      0.08       120\n",
            "          59       0.06      0.03      0.04       120\n",
            "          60       0.03      0.04      0.03       120\n",
            "          61       0.15      0.07      0.09       120\n",
            "          62       0.15      0.06      0.08       120\n",
            "          63       0.07      0.03      0.04       120\n",
            "          64       0.13      0.09      0.11       112\n",
            "          65       0.29      0.16      0.20       120\n",
            "          66       0.32      0.08      0.13       120\n",
            "          67       0.08      0.08      0.08       120\n",
            "          68       0.22      0.13      0.17       120\n",
            "          69       0.12      0.12      0.12       120\n",
            "          70       0.18      0.03      0.05       113\n",
            "          71       0.15      0.17      0.16       120\n",
            "          72       0.12      0.05      0.07       120\n",
            "          73       0.33      0.13      0.19       120\n",
            "          74       0.14      0.04      0.06       120\n",
            "          75       0.10      0.31      0.16       120\n",
            "          76       0.13      0.17      0.15       120\n",
            "          77       0.06      0.12      0.08       120\n",
            "          78       0.12      0.16      0.14       120\n",
            "          79       0.10      0.08      0.09       120\n",
            "\n",
            "    accuracy                           0.11      9577\n",
            "   macro avg       0.14      0.11      0.11      9577\n",
            "weighted avg       0.14      0.11      0.11      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/500 [Train]: 100%|██████████| 2794/2794 [00:47<00:00, 58.25it/s, loss=3.4547, acc=9.35%]\n",
            "Epoch 7/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 260.29it/s, loss=4.1180, acc=13.13%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/500\n",
            "Train Loss: 3.7924, Train Acc: 9.35%\n",
            "Val Loss: 3.5249, Val Acc: 13.13%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.15      0.17      0.16       120\n",
            "           1       0.08      0.06      0.07       120\n",
            "           2       0.11      0.05      0.07       120\n",
            "           3       0.14      0.14      0.14       120\n",
            "           4       0.05      0.20      0.08       120\n",
            "           5       0.09      0.04      0.06       120\n",
            "           6       0.11      0.14      0.13       120\n",
            "           7       0.38      0.13      0.20       120\n",
            "           8       0.25      0.17      0.20       120\n",
            "           9       0.15      0.09      0.11       120\n",
            "          10       0.08      0.16      0.11       120\n",
            "          11       0.33      0.18      0.24       120\n",
            "          12       0.15      0.28      0.20       120\n",
            "          13       0.06      0.08      0.07       120\n",
            "          14       0.11      0.17      0.13       120\n",
            "          15       0.10      0.10      0.10       120\n",
            "          16       0.12      0.07      0.09       120\n",
            "          17       0.07      0.10      0.09       120\n",
            "          18       0.11      0.11      0.11       120\n",
            "          19       0.24      0.14      0.18       120\n",
            "          20       0.11      0.25      0.15       120\n",
            "          21       0.11      0.15      0.13       120\n",
            "          22       0.20      0.03      0.06       120\n",
            "          23       0.09      0.13      0.11       120\n",
            "          24       0.42      0.16      0.23       120\n",
            "          25       0.16      0.08      0.11       120\n",
            "          26       0.11      0.07      0.09       120\n",
            "          27       0.16      0.12      0.14       120\n",
            "          28       0.13      0.28      0.18       120\n",
            "          29       0.22      0.10      0.14       120\n",
            "          30       0.20      0.17      0.19       120\n",
            "          31       0.39      0.06      0.10       120\n",
            "          32       0.18      0.10      0.13       120\n",
            "          33       0.17      0.17      0.17       120\n",
            "          34       0.13      0.07      0.09       120\n",
            "          35       0.30      0.16      0.21       120\n",
            "          36       0.07      0.11      0.08       120\n",
            "          37       0.12      0.13      0.13       120\n",
            "          38       0.05      0.04      0.05       120\n",
            "          39       0.12      0.15      0.13       120\n",
            "          40       0.18      0.23      0.20       120\n",
            "          41       0.11      0.18      0.13       120\n",
            "          42       0.08      0.13      0.10       120\n",
            "          43       0.30      0.21      0.24       112\n",
            "          44       0.14      0.17      0.15       120\n",
            "          45       0.15      0.07      0.09       120\n",
            "          46       0.15      0.22      0.17       120\n",
            "          47       0.08      0.17      0.11       120\n",
            "          48       0.39      0.18      0.25       120\n",
            "          49       0.15      0.12      0.13       120\n",
            "          50       0.18      0.05      0.08       120\n",
            "          51       0.22      0.15      0.18       120\n",
            "          52       0.08      0.05      0.06       120\n",
            "          53       0.11      0.18      0.14       120\n",
            "          54       0.13      0.14      0.14       120\n",
            "          55       0.13      0.06      0.08       120\n",
            "          56       0.15      0.06      0.08       120\n",
            "          57       0.15      0.08      0.11       120\n",
            "          58       0.08      0.12      0.10       120\n",
            "          59       0.08      0.07      0.08       120\n",
            "          60       0.08      0.15      0.10       120\n",
            "          61       0.11      0.12      0.12       120\n",
            "          62       0.40      0.05      0.09       120\n",
            "          63       0.08      0.03      0.04       120\n",
            "          64       0.24      0.21      0.22       112\n",
            "          65       0.42      0.13      0.20       120\n",
            "          66       0.21      0.19      0.20       120\n",
            "          67       0.10      0.12      0.11       120\n",
            "          68       0.18      0.25      0.21       120\n",
            "          69       0.12      0.22      0.16       120\n",
            "          70       0.23      0.03      0.05       113\n",
            "          71       0.11      0.17      0.13       120\n",
            "          72       0.13      0.10      0.11       120\n",
            "          73       0.15      0.22      0.18       120\n",
            "          74       0.11      0.13      0.12       120\n",
            "          75       0.18      0.20      0.19       120\n",
            "          76       0.15      0.13      0.14       120\n",
            "          77       0.16      0.06      0.09       120\n",
            "          78       0.21      0.16      0.18       120\n",
            "          79       0.12      0.07      0.08       120\n",
            "\n",
            "    accuracy                           0.13      9577\n",
            "   macro avg       0.16      0.13      0.13      9577\n",
            "weighted avg       0.16      0.13      0.13      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.58it/s, loss=5.2696, acc=11.10%]\n",
            "Epoch 8/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 281.78it/s, loss=3.2453, acc=15.14%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/500\n",
            "Train Loss: 3.6619, Train Acc: 11.10%\n",
            "Val Loss: 3.3700, Val Acc: 15.14%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.17      0.10      0.13       120\n",
            "           1       0.12      0.16      0.13       120\n",
            "           2       0.18      0.17      0.18       120\n",
            "           3       0.12      0.13      0.13       120\n",
            "           4       0.06      0.15      0.09       120\n",
            "           5       0.15      0.12      0.13       120\n",
            "           6       0.16      0.16      0.16       120\n",
            "           7       0.14      0.15      0.15       120\n",
            "           8       0.16      0.15      0.16       120\n",
            "           9       0.10      0.10      0.10       120\n",
            "          10       0.11      0.17      0.13       120\n",
            "          11       0.17      0.17      0.17       120\n",
            "          12       0.17      0.30      0.22       120\n",
            "          13       0.20      0.10      0.13       120\n",
            "          14       0.13      0.07      0.09       120\n",
            "          15       0.21      0.15      0.18       120\n",
            "          16       0.08      0.07      0.08       120\n",
            "          17       0.09      0.12      0.10       120\n",
            "          18       0.09      0.11      0.10       120\n",
            "          19       0.18      0.14      0.16       120\n",
            "          20       0.12      0.14      0.13       120\n",
            "          21       0.14      0.12      0.13       120\n",
            "          22       0.20      0.10      0.13       120\n",
            "          23       0.16      0.17      0.16       120\n",
            "          24       0.28      0.25      0.26       120\n",
            "          25       0.27      0.15      0.19       120\n",
            "          26       0.22      0.17      0.19       120\n",
            "          27       0.16      0.19      0.17       120\n",
            "          28       0.07      0.12      0.09       120\n",
            "          29       0.14      0.20      0.16       120\n",
            "          30       0.18      0.17      0.17       120\n",
            "          31       0.26      0.21      0.23       120\n",
            "          32       0.19      0.21      0.20       120\n",
            "          33       0.23      0.04      0.07       120\n",
            "          34       0.21      0.03      0.06       120\n",
            "          35       0.12      0.24      0.16       120\n",
            "          36       0.13      0.10      0.11       120\n",
            "          37       0.16      0.32      0.21       120\n",
            "          38       0.19      0.20      0.19       120\n",
            "          39       0.21      0.21      0.21       120\n",
            "          40       0.14      0.33      0.20       120\n",
            "          41       0.17      0.15      0.16       120\n",
            "          42       0.12      0.12      0.12       120\n",
            "          43       0.21      0.28      0.24       112\n",
            "          44       0.14      0.18      0.16       120\n",
            "          45       0.15      0.07      0.10       120\n",
            "          46       0.16      0.09      0.12       120\n",
            "          47       0.12      0.07      0.09       120\n",
            "          48       0.24      0.17      0.20       120\n",
            "          49       0.15      0.07      0.10       120\n",
            "          50       0.12      0.15      0.14       120\n",
            "          51       0.17      0.18      0.18       120\n",
            "          52       0.15      0.07      0.10       120\n",
            "          53       0.07      0.14      0.09       120\n",
            "          54       0.14      0.17      0.15       120\n",
            "          55       0.15      0.11      0.13       120\n",
            "          56       0.12      0.06      0.08       120\n",
            "          57       0.23      0.13      0.17       120\n",
            "          58       0.13      0.11      0.12       120\n",
            "          59       0.07      0.07      0.07       120\n",
            "          60       0.08      0.06      0.07       120\n",
            "          61       0.09      0.10      0.09       120\n",
            "          62       0.13      0.25      0.17       120\n",
            "          63       0.15      0.08      0.11       120\n",
            "          64       0.16      0.11      0.13       112\n",
            "          65       0.36      0.19      0.25       120\n",
            "          66       0.23      0.27      0.25       120\n",
            "          67       0.13      0.16      0.14       120\n",
            "          68       0.25      0.23      0.24       120\n",
            "          69       0.13      0.15      0.14       120\n",
            "          70       0.17      0.10      0.12       113\n",
            "          71       0.07      0.04      0.05       120\n",
            "          72       0.20      0.15      0.17       120\n",
            "          73       0.24      0.12      0.16       120\n",
            "          74       0.21      0.12      0.15       120\n",
            "          75       0.23      0.40      0.29       120\n",
            "          76       0.19      0.28      0.22       120\n",
            "          77       0.27      0.17      0.21       120\n",
            "          78       0.13      0.15      0.14       120\n",
            "          79       0.18      0.13      0.15       120\n",
            "\n",
            "    accuracy                           0.15      9577\n",
            "   macro avg       0.16      0.15      0.15      9577\n",
            "weighted avg       0.16      0.15      0.15      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.69it/s, loss=3.8806, acc=13.01%]\n",
            "Epoch 9/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 281.87it/s, loss=3.2084, acc=16.06%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/500\n",
            "Train Loss: 3.5575, Train Acc: 13.01%\n",
            "Val Loss: 3.3007, Val Acc: 16.06%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.12      0.16       120\n",
            "           1       0.15      0.14      0.15       120\n",
            "           2       0.17      0.23      0.20       120\n",
            "           3       0.12      0.14      0.13       120\n",
            "           4       0.08      0.09      0.08       120\n",
            "           5       0.17      0.17      0.17       120\n",
            "           6       0.15      0.23      0.19       120\n",
            "           7       0.30      0.11      0.16       120\n",
            "           8       0.17      0.24      0.20       120\n",
            "           9       0.13      0.03      0.04       120\n",
            "          10       0.11      0.08      0.09       120\n",
            "          11       0.19      0.21      0.20       120\n",
            "          12       0.24      0.27      0.25       120\n",
            "          13       0.10      0.26      0.15       120\n",
            "          14       0.19      0.21      0.20       120\n",
            "          15       0.17      0.18      0.17       120\n",
            "          16       0.09      0.12      0.10       120\n",
            "          17       0.21      0.17      0.19       120\n",
            "          18       0.12      0.17      0.14       120\n",
            "          19       0.23      0.14      0.17       120\n",
            "          20       0.16      0.22      0.19       120\n",
            "          21       0.09      0.09      0.09       120\n",
            "          22       0.19      0.21      0.20       120\n",
            "          23       0.20      0.08      0.12       120\n",
            "          24       0.25      0.19      0.22       120\n",
            "          25       0.21      0.13      0.16       120\n",
            "          26       0.13      0.16      0.14       120\n",
            "          27       0.20      0.17      0.18       120\n",
            "          28       0.12      0.24      0.16       120\n",
            "          29       0.16      0.19      0.18       120\n",
            "          30       0.17      0.20      0.19       120\n",
            "          31       0.31      0.15      0.20       120\n",
            "          32       0.28      0.13      0.18       120\n",
            "          33       0.21      0.13      0.16       120\n",
            "          34       0.17      0.10      0.13       120\n",
            "          35       0.25      0.27      0.26       120\n",
            "          36       0.09      0.16      0.12       120\n",
            "          37       0.24      0.07      0.10       120\n",
            "          38       0.23      0.17      0.19       120\n",
            "          39       0.12      0.06      0.08       120\n",
            "          40       0.18      0.31      0.23       120\n",
            "          41       0.15      0.20      0.17       120\n",
            "          42       0.16      0.05      0.08       120\n",
            "          43       0.18      0.18      0.18       112\n",
            "          44       0.21      0.12      0.16       120\n",
            "          45       0.10      0.03      0.04       120\n",
            "          46       0.11      0.23      0.15       120\n",
            "          47       0.12      0.12      0.12       120\n",
            "          48       0.27      0.19      0.22       120\n",
            "          49       0.14      0.12      0.13       120\n",
            "          50       0.15      0.17      0.16       120\n",
            "          51       0.10      0.24      0.15       120\n",
            "          52       0.19      0.08      0.11       120\n",
            "          53       0.28      0.12      0.17       120\n",
            "          54       0.12      0.14      0.13       120\n",
            "          55       0.17      0.07      0.10       120\n",
            "          56       0.17      0.09      0.12       120\n",
            "          57       0.47      0.12      0.20       120\n",
            "          58       0.15      0.17      0.16       120\n",
            "          59       0.15      0.17      0.16       120\n",
            "          60       0.08      0.15      0.11       120\n",
            "          61       0.09      0.08      0.09       120\n",
            "          62       0.27      0.06      0.10       120\n",
            "          63       0.09      0.07      0.08       120\n",
            "          64       0.22      0.17      0.19       112\n",
            "          65       0.21      0.27      0.24       120\n",
            "          66       0.19      0.29      0.23       120\n",
            "          67       0.11      0.14      0.12       120\n",
            "          68       0.19      0.18      0.19       120\n",
            "          69       0.16      0.20      0.18       120\n",
            "          70       0.06      0.12      0.08       113\n",
            "          71       0.21      0.10      0.14       120\n",
            "          72       0.17      0.17      0.17       120\n",
            "          73       0.25      0.21      0.23       120\n",
            "          74       0.09      0.11      0.10       120\n",
            "          75       0.25      0.34      0.29       120\n",
            "          76       0.27      0.22      0.24       120\n",
            "          77       0.15      0.15      0.15       120\n",
            "          78       0.26      0.20      0.22       120\n",
            "          79       0.17      0.23      0.19       120\n",
            "\n",
            "    accuracy                           0.16      9577\n",
            "   macro avg       0.18      0.16      0.16      9577\n",
            "weighted avg       0.18      0.16      0.16      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.24it/s, loss=3.2131, acc=14.40%]\n",
            "Epoch 10/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 277.07it/s, loss=3.3059, acc=18.02%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/500\n",
            "Train Loss: 3.4647, Train Acc: 14.40%\n",
            "Val Loss: 3.1981, Val Acc: 18.02%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.14      0.18      0.16       120\n",
            "           1       0.17      0.06      0.09       120\n",
            "           2       0.24      0.14      0.18       120\n",
            "           3       0.29      0.18      0.23       120\n",
            "           4       0.15      0.14      0.14       120\n",
            "           5       0.18      0.10      0.13       120\n",
            "           6       0.18      0.33      0.24       120\n",
            "           7       0.18      0.10      0.13       120\n",
            "           8       0.21      0.20      0.21       120\n",
            "           9       0.20      0.04      0.07       120\n",
            "          10       0.16      0.12      0.14       120\n",
            "          11       0.20      0.23      0.21       120\n",
            "          12       0.18      0.31      0.23       120\n",
            "          13       0.17      0.22      0.19       120\n",
            "          14       0.16      0.24      0.19       120\n",
            "          15       0.17      0.05      0.08       120\n",
            "          16       0.13      0.15      0.14       120\n",
            "          17       0.19      0.18      0.19       120\n",
            "          18       0.11      0.12      0.12       120\n",
            "          19       0.24      0.33      0.28       120\n",
            "          20       0.18      0.19      0.19       120\n",
            "          21       0.14      0.16      0.15       120\n",
            "          22       0.25      0.18      0.21       120\n",
            "          23       0.19      0.13      0.16       120\n",
            "          24       0.26      0.31      0.28       120\n",
            "          25       0.15      0.23      0.18       120\n",
            "          26       0.15      0.17      0.16       120\n",
            "          27       0.37      0.14      0.20       120\n",
            "          28       0.18      0.18      0.18       120\n",
            "          29       0.16      0.14      0.15       120\n",
            "          30       0.17      0.23      0.20       120\n",
            "          31       0.32      0.15      0.20       120\n",
            "          32       0.27      0.23      0.25       120\n",
            "          33       0.22      0.21      0.21       120\n",
            "          34       0.21      0.20      0.20       120\n",
            "          35       0.36      0.28      0.31       120\n",
            "          36       0.13      0.17      0.15       120\n",
            "          37       0.27      0.25      0.26       120\n",
            "          38       0.20      0.12      0.15       120\n",
            "          39       0.34      0.17      0.23       120\n",
            "          40       0.15      0.23      0.18       120\n",
            "          41       0.18      0.13      0.15       120\n",
            "          42       0.17      0.20      0.18       120\n",
            "          43       0.14      0.30      0.19       112\n",
            "          44       0.20      0.24      0.22       120\n",
            "          45       0.13      0.06      0.08       120\n",
            "          46       0.14      0.23      0.17       120\n",
            "          47       0.09      0.20      0.13       120\n",
            "          48       0.31      0.31      0.31       120\n",
            "          49       0.18      0.14      0.16       120\n",
            "          50       0.19      0.25      0.21       120\n",
            "          51       0.12      0.16      0.14       120\n",
            "          52       0.23      0.11      0.15       120\n",
            "          53       0.14      0.14      0.14       120\n",
            "          54       0.12      0.06      0.08       120\n",
            "          55       0.17      0.17      0.17       120\n",
            "          56       0.14      0.08      0.10       120\n",
            "          57       0.20      0.12      0.15       120\n",
            "          58       0.10      0.09      0.09       120\n",
            "          59       0.17      0.13      0.15       120\n",
            "          60       0.11      0.15      0.13       120\n",
            "          61       0.19      0.07      0.11       120\n",
            "          62       0.13      0.19      0.15       120\n",
            "          63       0.10      0.07      0.08       120\n",
            "          64       0.24      0.16      0.19       112\n",
            "          65       0.28      0.29      0.29       120\n",
            "          66       0.26      0.36      0.30       120\n",
            "          67       0.20      0.10      0.13       120\n",
            "          68       0.19      0.26      0.22       120\n",
            "          69       0.11      0.22      0.14       120\n",
            "          70       0.11      0.15      0.13       113\n",
            "          71       0.14      0.19      0.16       120\n",
            "          72       0.23      0.10      0.14       120\n",
            "          73       0.28      0.29      0.28       120\n",
            "          74       0.10      0.07      0.09       120\n",
            "          75       0.27      0.32      0.29       120\n",
            "          76       0.20      0.16      0.18       120\n",
            "          77       0.21      0.26      0.23       120\n",
            "          78       0.21      0.24      0.23       120\n",
            "          79       0.12      0.13      0.13       120\n",
            "\n",
            "    accuracy                           0.18      9577\n",
            "   macro avg       0.19      0.18      0.18      9577\n",
            "weighted avg       0.19      0.18      0.18      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.27it/s, loss=4.0672, acc=15.73%]\n",
            "Epoch 11/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 275.70it/s, loss=2.7119, acc=19.07%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11/500\n",
            "Train Loss: 3.3801, Train Acc: 15.73%\n",
            "Val Loss: 3.1416, Val Acc: 19.07%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.19      0.22       120\n",
            "           1       0.16      0.28      0.21       120\n",
            "           2       0.15      0.25      0.19       120\n",
            "           3       0.24      0.22      0.23       120\n",
            "           4       0.18      0.20      0.19       120\n",
            "           5       0.44      0.12      0.19       120\n",
            "           6       0.22      0.17      0.19       120\n",
            "           7       0.28      0.21      0.24       120\n",
            "           8       0.30      0.32      0.31       120\n",
            "           9       0.18      0.20      0.19       120\n",
            "          10       0.28      0.06      0.10       120\n",
            "          11       0.25      0.17      0.21       120\n",
            "          12       0.29      0.28      0.28       120\n",
            "          13       0.16      0.23      0.19       120\n",
            "          14       0.22      0.28      0.24       120\n",
            "          15       0.23      0.08      0.12       120\n",
            "          16       0.17      0.16      0.16       120\n",
            "          17       0.15      0.22      0.18       120\n",
            "          18       0.13      0.26      0.17       120\n",
            "          19       0.29      0.14      0.19       120\n",
            "          20       0.21      0.28      0.24       120\n",
            "          21       0.13      0.12      0.12       120\n",
            "          22       0.20      0.07      0.11       120\n",
            "          23       0.18      0.19      0.19       120\n",
            "          24       0.23      0.19      0.21       120\n",
            "          25       0.19      0.16      0.17       120\n",
            "          26       0.18      0.09      0.12       120\n",
            "          27       0.28      0.14      0.19       120\n",
            "          28       0.20      0.28      0.23       120\n",
            "          29       0.28      0.18      0.22       120\n",
            "          30       0.20      0.16      0.18       120\n",
            "          31       0.37      0.18      0.24       120\n",
            "          32       0.21      0.29      0.24       120\n",
            "          33       0.29      0.17      0.21       120\n",
            "          34       0.12      0.13      0.13       120\n",
            "          35       0.19      0.28      0.23       120\n",
            "          36       0.15      0.14      0.15       120\n",
            "          37       0.36      0.10      0.16       120\n",
            "          38       0.16      0.38      0.22       120\n",
            "          39       0.18      0.23      0.21       120\n",
            "          40       0.22      0.14      0.17       120\n",
            "          41       0.24      0.17      0.20       120\n",
            "          42       0.21      0.16      0.18       120\n",
            "          43       0.23      0.29      0.26       112\n",
            "          44       0.17      0.26      0.20       120\n",
            "          45       0.12      0.15      0.13       120\n",
            "          46       0.18      0.18      0.18       120\n",
            "          47       0.13      0.33      0.18       120\n",
            "          48       0.16      0.24      0.19       120\n",
            "          49       0.18      0.17      0.17       120\n",
            "          50       0.25      0.16      0.19       120\n",
            "          51       0.21      0.11      0.14       120\n",
            "          52       0.38      0.14      0.21       120\n",
            "          53       0.18      0.17      0.18       120\n",
            "          54       0.12      0.17      0.15       120\n",
            "          55       0.26      0.18      0.22       120\n",
            "          56       0.16      0.17      0.17       120\n",
            "          57       0.22      0.20      0.21       120\n",
            "          58       0.13      0.07      0.09       120\n",
            "          59       0.13      0.19      0.15       120\n",
            "          60       0.12      0.12      0.12       120\n",
            "          61       0.11      0.09      0.10       120\n",
            "          62       0.16      0.12      0.14       120\n",
            "          63       0.12      0.13      0.12       120\n",
            "          64       0.29      0.17      0.21       112\n",
            "          65       0.22      0.40      0.29       120\n",
            "          66       0.26      0.23      0.24       120\n",
            "          67       0.13      0.17      0.14       120\n",
            "          68       0.32      0.26      0.29       120\n",
            "          69       0.08      0.14      0.10       120\n",
            "          70       0.18      0.12      0.15       113\n",
            "          71       0.22      0.16      0.18       120\n",
            "          72       0.19      0.17      0.18       120\n",
            "          73       0.38      0.25      0.30       120\n",
            "          74       0.10      0.10      0.10       120\n",
            "          75       0.25      0.13      0.17       120\n",
            "          76       0.25      0.27      0.26       120\n",
            "          77       0.19      0.24      0.21       120\n",
            "          78       0.17      0.39      0.24       120\n",
            "          79       0.35      0.13      0.19       120\n",
            "\n",
            "    accuracy                           0.19      9577\n",
            "   macro avg       0.21      0.19      0.19      9577\n",
            "weighted avg       0.21      0.19      0.19      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.35it/s, loss=3.5964, acc=16.83%]\n",
            "Epoch 12/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 256.11it/s, loss=3.0108, acc=20.52%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12/500\n",
            "Train Loss: 3.3247, Train Acc: 16.83%\n",
            "Val Loss: 3.0836, Val Acc: 20.52%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.21      0.21       120\n",
            "           1       0.21      0.08      0.12       120\n",
            "           2       0.21      0.27      0.24       120\n",
            "           3       0.24      0.20      0.22       120\n",
            "           4       0.14      0.15      0.14       120\n",
            "           5       0.22      0.17      0.19       120\n",
            "           6       0.26      0.23      0.25       120\n",
            "           7       0.24      0.23      0.24       120\n",
            "           8       0.24      0.33      0.28       120\n",
            "           9       0.10      0.04      0.06       120\n",
            "          10       0.20      0.15      0.17       120\n",
            "          11       0.35      0.27      0.30       120\n",
            "          12       0.20      0.26      0.23       120\n",
            "          13       0.20      0.26      0.22       120\n",
            "          14       0.23      0.23      0.23       120\n",
            "          15       0.30      0.15      0.20       120\n",
            "          16       0.12      0.30      0.17       120\n",
            "          17       0.14      0.22      0.17       120\n",
            "          18       0.14      0.26      0.18       120\n",
            "          19       0.26      0.23      0.24       120\n",
            "          20       0.23      0.33      0.27       120\n",
            "          21       0.19      0.14      0.16       120\n",
            "          22       0.33      0.14      0.20       120\n",
            "          23       0.16      0.19      0.17       120\n",
            "          24       0.32      0.23      0.26       120\n",
            "          25       0.21      0.20      0.20       120\n",
            "          26       0.21      0.12      0.16       120\n",
            "          27       0.25      0.21      0.23       120\n",
            "          28       0.17      0.26      0.20       120\n",
            "          29       0.26      0.17      0.21       120\n",
            "          30       0.18      0.20      0.19       120\n",
            "          31       0.34      0.33      0.33       120\n",
            "          32       0.36      0.22      0.27       120\n",
            "          33       0.21      0.19      0.20       120\n",
            "          34       0.15      0.07      0.09       120\n",
            "          35       0.37      0.40      0.39       120\n",
            "          36       0.16      0.15      0.16       120\n",
            "          37       0.23      0.24      0.23       120\n",
            "          38       0.19      0.23      0.21       120\n",
            "          39       0.24      0.28      0.26       120\n",
            "          40       0.22      0.20      0.21       120\n",
            "          41       0.21      0.31      0.25       120\n",
            "          42       0.21      0.14      0.17       120\n",
            "          43       0.18      0.29      0.22       112\n",
            "          44       0.25      0.38      0.30       120\n",
            "          45       0.14      0.14      0.14       120\n",
            "          46       0.22      0.16      0.18       120\n",
            "          47       0.16      0.28      0.20       120\n",
            "          48       0.21      0.27      0.24       120\n",
            "          49       0.23      0.10      0.14       120\n",
            "          50       0.21      0.20      0.20       120\n",
            "          51       0.14      0.12      0.13       120\n",
            "          52       0.24      0.17      0.20       120\n",
            "          53       0.15      0.28      0.19       120\n",
            "          54       0.17      0.13      0.15       120\n",
            "          55       0.19      0.11      0.14       120\n",
            "          56       0.15      0.22      0.18       120\n",
            "          57       0.21      0.13      0.16       120\n",
            "          58       0.16      0.07      0.09       120\n",
            "          59       0.33      0.14      0.20       120\n",
            "          60       0.22      0.17      0.19       120\n",
            "          61       0.15      0.12      0.14       120\n",
            "          62       0.25      0.13      0.17       120\n",
            "          63       0.18      0.11      0.13       120\n",
            "          64       0.24      0.29      0.26       112\n",
            "          65       0.27      0.25      0.26       120\n",
            "          66       0.23      0.23      0.23       120\n",
            "          67       0.12      0.17      0.14       120\n",
            "          68       0.39      0.23      0.28       120\n",
            "          69       0.08      0.19      0.12       120\n",
            "          70       0.02      0.01      0.01       113\n",
            "          71       0.20      0.32      0.25       120\n",
            "          72       0.15      0.25      0.19       120\n",
            "          73       0.38      0.17      0.24       120\n",
            "          74       0.29      0.18      0.22       120\n",
            "          75       0.27      0.27      0.27       120\n",
            "          76       0.32      0.25      0.28       120\n",
            "          77       0.19      0.11      0.14       120\n",
            "          78       0.24      0.43      0.31       120\n",
            "          79       0.24      0.17      0.20       120\n",
            "\n",
            "    accuracy                           0.21      9577\n",
            "   macro avg       0.22      0.21      0.20      9577\n",
            "weighted avg       0.22      0.21      0.20      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.63it/s, loss=3.5330, acc=17.66%]\n",
            "Epoch 13/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 245.15it/s, loss=3.1395, acc=21.65%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13/500\n",
            "Train Loss: 3.2656, Train Acc: 17.66%\n",
            "Val Loss: 2.9941, Val Acc: 21.65%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.34      0.28       120\n",
            "           1       0.20      0.35      0.25       120\n",
            "           2       0.26      0.29      0.27       120\n",
            "           3       0.32      0.19      0.24       120\n",
            "           4       0.16      0.17      0.17       120\n",
            "           5       0.18      0.26      0.21       120\n",
            "           6       0.17      0.31      0.22       120\n",
            "           7       0.26      0.20      0.23       120\n",
            "           8       0.29      0.24      0.26       120\n",
            "           9       0.16      0.15      0.16       120\n",
            "          10       0.21      0.16      0.18       120\n",
            "          11       0.31      0.27      0.29       120\n",
            "          12       0.27      0.30      0.28       120\n",
            "          13       0.26      0.21      0.23       120\n",
            "          14       0.30      0.17      0.22       120\n",
            "          15       0.16      0.22      0.19       120\n",
            "          16       0.14      0.15      0.15       120\n",
            "          17       0.13      0.31      0.18       120\n",
            "          18       0.13      0.21      0.16       120\n",
            "          19       0.24      0.26      0.25       120\n",
            "          20       0.19      0.38      0.25       120\n",
            "          21       0.17      0.17      0.17       120\n",
            "          22       0.17      0.12      0.15       120\n",
            "          23       0.21      0.24      0.23       120\n",
            "          24       0.39      0.23      0.29       120\n",
            "          25       0.24      0.17      0.20       120\n",
            "          26       0.16      0.14      0.15       120\n",
            "          27       0.24      0.14      0.18       120\n",
            "          28       0.21      0.21      0.21       120\n",
            "          29       0.24      0.12      0.16       120\n",
            "          30       0.17      0.23      0.20       120\n",
            "          31       0.29      0.23      0.26       120\n",
            "          32       0.29      0.22      0.25       120\n",
            "          33       0.27      0.31      0.29       120\n",
            "          34       0.13      0.17      0.15       120\n",
            "          35       0.35      0.31      0.33       120\n",
            "          36       0.26      0.12      0.16       120\n",
            "          37       0.19      0.45      0.27       120\n",
            "          38       0.38      0.13      0.20       120\n",
            "          39       0.28      0.19      0.23       120\n",
            "          40       0.20      0.28      0.24       120\n",
            "          41       0.27      0.23      0.25       120\n",
            "          42       0.18      0.22      0.20       120\n",
            "          43       0.22      0.13      0.17       112\n",
            "          44       0.30      0.33      0.32       120\n",
            "          45       0.20      0.16      0.18       120\n",
            "          46       0.23      0.21      0.22       120\n",
            "          47       0.17      0.19      0.18       120\n",
            "          48       0.17      0.33      0.22       120\n",
            "          49       0.20      0.26      0.23       120\n",
            "          50       0.37      0.18      0.25       120\n",
            "          51       0.24      0.18      0.21       120\n",
            "          52       0.22      0.13      0.17       120\n",
            "          53       0.17      0.17      0.17       120\n",
            "          54       0.21      0.27      0.24       120\n",
            "          55       0.25      0.12      0.17       120\n",
            "          56       0.15      0.23      0.18       120\n",
            "          57       0.27      0.12      0.16       120\n",
            "          58       0.23      0.07      0.11       120\n",
            "          59       0.31      0.12      0.17       120\n",
            "          60       0.20      0.14      0.17       120\n",
            "          61       0.15      0.17      0.16       120\n",
            "          62       0.14      0.23      0.17       120\n",
            "          63       0.15      0.09      0.11       120\n",
            "          64       0.22      0.21      0.22       112\n",
            "          65       0.46      0.26      0.33       120\n",
            "          66       0.31      0.34      0.32       120\n",
            "          67       0.23      0.12      0.16       120\n",
            "          68       0.22      0.27      0.24       120\n",
            "          69       0.19      0.14      0.16       120\n",
            "          70       0.21      0.10      0.13       113\n",
            "          71       0.19      0.17      0.18       120\n",
            "          72       0.19      0.28      0.23       120\n",
            "          73       0.23      0.30      0.26       120\n",
            "          74       0.21      0.13      0.16       120\n",
            "          75       0.32      0.19      0.24       120\n",
            "          76       0.21      0.28      0.24       120\n",
            "          77       0.30      0.26      0.28       120\n",
            "          78       0.33      0.37      0.35       120\n",
            "          79       0.22      0.28      0.25       120\n",
            "\n",
            "    accuracy                           0.22      9577\n",
            "   macro avg       0.23      0.22      0.21      9577\n",
            "weighted avg       0.23      0.22      0.21      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.31it/s, loss=3.7421, acc=18.45%]\n",
            "Epoch 14/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 268.07it/s, loss=3.6064, acc=21.44%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14/500\n",
            "Train Loss: 3.2242, Train Acc: 18.45%\n",
            "Val Loss: 3.0220, Val Acc: 21.44%\n",
            "Learning rate: 0.000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 57.01it/s, loss=4.0110, acc=19.56%]\n",
            "Epoch 15/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 270.10it/s, loss=2.5036, acc=23.03%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15/500\n",
            "Train Loss: 3.1698, Train Acc: 19.56%\n",
            "Val Loss: 2.9289, Val Acc: 23.03%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.18      0.20       120\n",
            "           1       0.17      0.25      0.21       120\n",
            "           2       0.38      0.28      0.32       120\n",
            "           3       0.29      0.13      0.18       120\n",
            "           4       0.17      0.19      0.18       120\n",
            "           5       0.24      0.16      0.19       120\n",
            "           6       0.21      0.23      0.22       120\n",
            "           7       0.32      0.24      0.27       120\n",
            "           8       0.27      0.23      0.25       120\n",
            "           9       0.25      0.12      0.16       120\n",
            "          10       0.16      0.14      0.15       120\n",
            "          11       0.25      0.31      0.27       120\n",
            "          12       0.21      0.31      0.25       120\n",
            "          13       0.20      0.28      0.23       120\n",
            "          14       0.18      0.28      0.21       120\n",
            "          15       0.20      0.27      0.23       120\n",
            "          16       0.16      0.15      0.15       120\n",
            "          17       0.20      0.21      0.20       120\n",
            "          18       0.13      0.12      0.12       120\n",
            "          19       0.23      0.28      0.25       120\n",
            "          20       0.27      0.39      0.32       120\n",
            "          21       0.20      0.19      0.19       120\n",
            "          22       0.16      0.19      0.18       120\n",
            "          23       0.26      0.11      0.15       120\n",
            "          24       0.27      0.35      0.31       120\n",
            "          25       0.30      0.22      0.25       120\n",
            "          26       0.19      0.19      0.19       120\n",
            "          27       0.21      0.23      0.22       120\n",
            "          28       0.25      0.17      0.20       120\n",
            "          29       0.24      0.23      0.24       120\n",
            "          30       0.26      0.13      0.18       120\n",
            "          31       0.42      0.32      0.36       120\n",
            "          32       0.25      0.28      0.26       120\n",
            "          33       0.22      0.33      0.26       120\n",
            "          34       0.31      0.18      0.23       120\n",
            "          35       0.40      0.35      0.37       120\n",
            "          36       0.12      0.28      0.17       120\n",
            "          37       0.40      0.22      0.28       120\n",
            "          38       0.37      0.24      0.29       120\n",
            "          39       0.25      0.23      0.24       120\n",
            "          40       0.22      0.23      0.23       120\n",
            "          41       0.23      0.23      0.23       120\n",
            "          42       0.23      0.19      0.21       120\n",
            "          43       0.27      0.25      0.26       112\n",
            "          44       0.28      0.24      0.26       120\n",
            "          45       0.21      0.17      0.19       120\n",
            "          46       0.21      0.32      0.25       120\n",
            "          47       0.18      0.30      0.22       120\n",
            "          48       0.22      0.30      0.25       120\n",
            "          49       0.17      0.26      0.20       120\n",
            "          50       0.26      0.23      0.24       120\n",
            "          51       0.30      0.12      0.18       120\n",
            "          52       0.22      0.18      0.20       120\n",
            "          53       0.20      0.29      0.24       120\n",
            "          54       0.28      0.19      0.23       120\n",
            "          55       0.30      0.13      0.18       120\n",
            "          56       0.21      0.18      0.19       120\n",
            "          57       0.25      0.12      0.17       120\n",
            "          58       0.26      0.09      0.13       120\n",
            "          59       0.30      0.11      0.16       120\n",
            "          60       0.19      0.12      0.15       120\n",
            "          61       0.23      0.19      0.21       120\n",
            "          62       0.16      0.26      0.19       120\n",
            "          63       0.18      0.16      0.17       120\n",
            "          64       0.21      0.34      0.26       112\n",
            "          65       0.49      0.26      0.34       120\n",
            "          66       0.37      0.31      0.34       120\n",
            "          67       0.15      0.22      0.18       120\n",
            "          68       0.28      0.35      0.31       120\n",
            "          69       0.17      0.30      0.22       120\n",
            "          70       0.24      0.10      0.14       113\n",
            "          71       0.23      0.15      0.18       120\n",
            "          72       0.28      0.18      0.22       120\n",
            "          73       0.33      0.35      0.34       120\n",
            "          74       0.18      0.21      0.19       120\n",
            "          75       0.31      0.21      0.25       120\n",
            "          76       0.29      0.43      0.35       120\n",
            "          77       0.23      0.37      0.28       120\n",
            "          78       0.29      0.33      0.30       120\n",
            "          79       0.21      0.28      0.24       120\n",
            "\n",
            "    accuracy                           0.23      9577\n",
            "   macro avg       0.24      0.23      0.23      9577\n",
            "weighted avg       0.24      0.23      0.23      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.82it/s, loss=3.0184, acc=19.96%]\n",
            "Epoch 16/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 276.59it/s, loss=2.6250, acc=23.44%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16/500\n",
            "Train Loss: 3.1305, Train Acc: 19.96%\n",
            "Val Loss: 2.9025, Val Acc: 23.44%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.21      0.29      0.25       120\n",
            "           1       0.24      0.30      0.27       120\n",
            "           2       0.21      0.28      0.24       120\n",
            "           3       0.32      0.20      0.25       120\n",
            "           4       0.18      0.17      0.17       120\n",
            "           5       0.32      0.16      0.21       120\n",
            "           6       0.20      0.20      0.20       120\n",
            "           7       0.21      0.23      0.22       120\n",
            "           8       0.31      0.28      0.29       120\n",
            "           9       0.17      0.12      0.14       120\n",
            "          10       0.25      0.18      0.21       120\n",
            "          11       0.43      0.28      0.34       120\n",
            "          12       0.28      0.33      0.30       120\n",
            "          13       0.18      0.19      0.19       120\n",
            "          14       0.25      0.23      0.24       120\n",
            "          15       0.22      0.16      0.19       120\n",
            "          16       0.16      0.16      0.16       120\n",
            "          17       0.25      0.16      0.19       120\n",
            "          18       0.29      0.10      0.15       120\n",
            "          19       0.35      0.23      0.27       120\n",
            "          20       0.34      0.27      0.30       120\n",
            "          21       0.24      0.17      0.20       120\n",
            "          22       0.23      0.15      0.18       120\n",
            "          23       0.21      0.12      0.16       120\n",
            "          24       0.36      0.27      0.31       120\n",
            "          25       0.22      0.18      0.20       120\n",
            "          26       0.34      0.17      0.22       120\n",
            "          27       0.28      0.31      0.29       120\n",
            "          28       0.21      0.21      0.21       120\n",
            "          29       0.15      0.12      0.14       120\n",
            "          30       0.23      0.20      0.21       120\n",
            "          31       0.28      0.38      0.33       120\n",
            "          32       0.26      0.33      0.29       120\n",
            "          33       0.27      0.24      0.25       120\n",
            "          34       0.26      0.15      0.19       120\n",
            "          35       0.30      0.41      0.35       120\n",
            "          36       0.22      0.25      0.24       120\n",
            "          37       0.31      0.25      0.28       120\n",
            "          38       0.16      0.38      0.22       120\n",
            "          39       0.31      0.23      0.27       120\n",
            "          40       0.32      0.27      0.29       120\n",
            "          41       0.22      0.26      0.24       120\n",
            "          42       0.20      0.17      0.18       120\n",
            "          43       0.25      0.23      0.24       112\n",
            "          44       0.28      0.41      0.33       120\n",
            "          45       0.15      0.21      0.17       120\n",
            "          46       0.24      0.30      0.27       120\n",
            "          47       0.18      0.28      0.22       120\n",
            "          48       0.25      0.23      0.24       120\n",
            "          49       0.19      0.33      0.24       120\n",
            "          50       0.26      0.27      0.26       120\n",
            "          51       0.21      0.26      0.23       120\n",
            "          52       0.25      0.36      0.30       120\n",
            "          53       0.17      0.18      0.18       120\n",
            "          54       0.30      0.23      0.26       120\n",
            "          55       0.23      0.26      0.24       120\n",
            "          56       0.17      0.17      0.17       120\n",
            "          57       0.23      0.17      0.19       120\n",
            "          58       0.31      0.11      0.16       120\n",
            "          59       0.21      0.12      0.15       120\n",
            "          60       0.12      0.27      0.17       120\n",
            "          61       0.19      0.12      0.15       120\n",
            "          62       0.18      0.25      0.21       120\n",
            "          63       0.26      0.15      0.19       120\n",
            "          64       0.22      0.26      0.24       112\n",
            "          65       0.37      0.24      0.29       120\n",
            "          66       0.28      0.36      0.32       120\n",
            "          67       0.17      0.15      0.16       120\n",
            "          68       0.35      0.28      0.31       120\n",
            "          69       0.15      0.28      0.19       120\n",
            "          70       0.12      0.08      0.10       113\n",
            "          71       0.24      0.27      0.25       120\n",
            "          72       0.31      0.19      0.24       120\n",
            "          73       0.25      0.30      0.27       120\n",
            "          74       0.16      0.22      0.18       120\n",
            "          75       0.32      0.21      0.25       120\n",
            "          76       0.26      0.37      0.31       120\n",
            "          77       0.25      0.29      0.27       120\n",
            "          78       0.25      0.39      0.31       120\n",
            "          79       0.30      0.27      0.28       120\n",
            "\n",
            "    accuracy                           0.23      9577\n",
            "   macro avg       0.25      0.23      0.23      9577\n",
            "weighted avg       0.25      0.23      0.23      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.16it/s, loss=5.0416, acc=20.93%]\n",
            "Epoch 17/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 268.99it/s, loss=2.6806, acc=24.16%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17/500\n",
            "Train Loss: 3.0987, Train Acc: 20.93%\n",
            "Val Loss: 2.8518, Val Acc: 24.16%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.19      0.23      0.20       120\n",
            "           1       0.26      0.24      0.25       120\n",
            "           2       0.28      0.17      0.22       120\n",
            "           3       0.42      0.16      0.23       120\n",
            "           4       0.17      0.21      0.19       120\n",
            "           5       0.27      0.27      0.27       120\n",
            "           6       0.30      0.28      0.29       120\n",
            "           7       0.26      0.30      0.28       120\n",
            "           8       0.25      0.23      0.24       120\n",
            "           9       0.16      0.12      0.14       120\n",
            "          10       0.22      0.24      0.23       120\n",
            "          11       0.29      0.33      0.31       120\n",
            "          12       0.22      0.26      0.24       120\n",
            "          13       0.25      0.34      0.29       120\n",
            "          14       0.21      0.37      0.27       120\n",
            "          15       0.33      0.12      0.18       120\n",
            "          16       0.12      0.19      0.15       120\n",
            "          17       0.17      0.28      0.21       120\n",
            "          18       0.22      0.22      0.22       120\n",
            "          19       0.29      0.28      0.29       120\n",
            "          20       0.27      0.36      0.31       120\n",
            "          21       0.20      0.21      0.20       120\n",
            "          22       0.20      0.23      0.21       120\n",
            "          23       0.18      0.14      0.16       120\n",
            "          24       0.20      0.28      0.23       120\n",
            "          25       0.29      0.10      0.15       120\n",
            "          26       0.21      0.22      0.21       120\n",
            "          27       0.22      0.20      0.21       120\n",
            "          28       0.28      0.17      0.22       120\n",
            "          29       0.30      0.23      0.26       120\n",
            "          30       0.23      0.22      0.22       120\n",
            "          31       0.25      0.33      0.29       120\n",
            "          32       0.30      0.32      0.31       120\n",
            "          33       0.29      0.22      0.25       120\n",
            "          34       0.16      0.13      0.14       120\n",
            "          35       0.29      0.39      0.33       120\n",
            "          36       0.25      0.21      0.23       120\n",
            "          37       0.35      0.36      0.35       120\n",
            "          38       0.35      0.18      0.24       120\n",
            "          39       0.35      0.28      0.31       120\n",
            "          40       0.41      0.30      0.35       120\n",
            "          41       0.32      0.23      0.26       120\n",
            "          42       0.30      0.23      0.26       120\n",
            "          43       0.22      0.33      0.26       112\n",
            "          44       0.26      0.41      0.31       120\n",
            "          45       0.23      0.12      0.15       120\n",
            "          46       0.16      0.32      0.21       120\n",
            "          47       0.19      0.27      0.22       120\n",
            "          48       0.46      0.21      0.29       120\n",
            "          49       0.18      0.28      0.22       120\n",
            "          50       0.26      0.29      0.27       120\n",
            "          51       0.22      0.28      0.24       120\n",
            "          52       0.30      0.20      0.24       120\n",
            "          53       0.24      0.17      0.20       120\n",
            "          54       0.21      0.25      0.23       120\n",
            "          55       0.25      0.13      0.17       120\n",
            "          56       0.17      0.17      0.17       120\n",
            "          57       0.32      0.14      0.20       120\n",
            "          58       0.18      0.19      0.18       120\n",
            "          59       0.28      0.27      0.27       120\n",
            "          60       0.20      0.17      0.19       120\n",
            "          61       0.20      0.28      0.23       120\n",
            "          62       0.20      0.23      0.21       120\n",
            "          63       0.20      0.17      0.19       120\n",
            "          64       0.23      0.35      0.28       112\n",
            "          65       0.41      0.30      0.35       120\n",
            "          66       0.27      0.31      0.29       120\n",
            "          67       0.26      0.17      0.21       120\n",
            "          68       0.36      0.28      0.31       120\n",
            "          69       0.18      0.17      0.17       120\n",
            "          70       0.19      0.07      0.10       113\n",
            "          71       0.42      0.11      0.17       120\n",
            "          72       0.23      0.25      0.24       120\n",
            "          73       0.29      0.31      0.30       120\n",
            "          74       0.15      0.09      0.11       120\n",
            "          75       0.32      0.27      0.29       120\n",
            "          76       0.27      0.39      0.32       120\n",
            "          77       0.23      0.33      0.27       120\n",
            "          78       0.31      0.34      0.32       120\n",
            "          79       0.22      0.35      0.27       120\n",
            "\n",
            "    accuracy                           0.24      9577\n",
            "   macro avg       0.25      0.24      0.24      9577\n",
            "weighted avg       0.25      0.24      0.24      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.34it/s, loss=2.9741, acc=21.49%]\n",
            "Epoch 18/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 252.24it/s, loss=3.7819, acc=23.80%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18/500\n",
            "Train Loss: 3.0633, Train Acc: 21.49%\n",
            "Val Loss: 2.8606, Val Acc: 23.80%\n",
            "Learning rate: 0.000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.11it/s, loss=3.3231, acc=22.31%]\n",
            "Epoch 19/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 261.25it/s, loss=2.2506, acc=23.90%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19/500\n",
            "Train Loss: 3.0329, Train Acc: 22.31%\n",
            "Val Loss: 2.8480, Val Acc: 23.90%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.23      0.26      0.24       120\n",
            "           1       0.17      0.25      0.20       120\n",
            "           2       0.21      0.28      0.24       120\n",
            "           3       0.23      0.21      0.22       120\n",
            "           4       0.21      0.28      0.24       120\n",
            "           5       0.31      0.28      0.29       120\n",
            "           6       0.21      0.27      0.23       120\n",
            "           7       0.43      0.21      0.28       120\n",
            "           8       0.33      0.27      0.30       120\n",
            "           9       0.21      0.17      0.19       120\n",
            "          10       0.14      0.22      0.17       120\n",
            "          11       0.28      0.38      0.32       120\n",
            "          12       0.38      0.31      0.34       120\n",
            "          13       0.30      0.25      0.27       120\n",
            "          14       0.27      0.26      0.26       120\n",
            "          15       0.23      0.17      0.20       120\n",
            "          16       0.19      0.14      0.16       120\n",
            "          17       0.21      0.24      0.23       120\n",
            "          18       0.16      0.10      0.12       120\n",
            "          19       0.15      0.38      0.21       120\n",
            "          20       0.24      0.27      0.25       120\n",
            "          21       0.21      0.23      0.22       120\n",
            "          22       0.28      0.21      0.24       120\n",
            "          23       0.21      0.22      0.21       120\n",
            "          24       0.21      0.35      0.26       120\n",
            "          25       0.33      0.17      0.23       120\n",
            "          26       0.23      0.27      0.24       120\n",
            "          27       0.30      0.28      0.29       120\n",
            "          28       0.24      0.23      0.23       120\n",
            "          29       0.33      0.17      0.23       120\n",
            "          30       0.30      0.19      0.23       120\n",
            "          31       0.36      0.11      0.17       120\n",
            "          32       0.26      0.28      0.27       120\n",
            "          33       0.34      0.24      0.28       120\n",
            "          34       0.22      0.09      0.13       120\n",
            "          35       0.26      0.42      0.32       120\n",
            "          36       0.19      0.27      0.22       120\n",
            "          37       0.34      0.38      0.35       120\n",
            "          38       0.19      0.15      0.17       120\n",
            "          39       0.34      0.23      0.28       120\n",
            "          40       0.29      0.26      0.27       120\n",
            "          41       0.27      0.30      0.29       120\n",
            "          42       0.44      0.15      0.22       120\n",
            "          43       0.19      0.20      0.19       112\n",
            "          44       0.28      0.43      0.34       120\n",
            "          45       0.18      0.22      0.20       120\n",
            "          46       0.29      0.25      0.27       120\n",
            "          47       0.22      0.26      0.23       120\n",
            "          48       0.27      0.26      0.26       120\n",
            "          49       0.20      0.32      0.25       120\n",
            "          50       0.32      0.29      0.30       120\n",
            "          51       0.24      0.17      0.20       120\n",
            "          52       0.41      0.17      0.25       120\n",
            "          53       0.21      0.17      0.19       120\n",
            "          54       0.38      0.17      0.23       120\n",
            "          55       0.14      0.24      0.18       120\n",
            "          56       0.34      0.17      0.23       120\n",
            "          57       0.18      0.16      0.17       120\n",
            "          58       0.12      0.23      0.16       120\n",
            "          59       0.26      0.19      0.22       120\n",
            "          60       0.23      0.12      0.16       120\n",
            "          61       0.23      0.23      0.23       120\n",
            "          62       0.25      0.15      0.19       120\n",
            "          63       0.19      0.06      0.09       120\n",
            "          64       0.34      0.36      0.35       112\n",
            "          65       0.37      0.35      0.36       120\n",
            "          66       0.28      0.31      0.29       120\n",
            "          67       0.16      0.16      0.16       120\n",
            "          68       0.36      0.29      0.32       120\n",
            "          69       0.15      0.36      0.22       120\n",
            "          70       0.10      0.04      0.06       113\n",
            "          71       0.20      0.20      0.20       120\n",
            "          72       0.23      0.23      0.23       120\n",
            "          73       0.43      0.22      0.29       120\n",
            "          74       0.25      0.23      0.24       120\n",
            "          75       0.22      0.38      0.28       120\n",
            "          76       0.27      0.24      0.26       120\n",
            "          77       0.20      0.32      0.24       120\n",
            "          78       0.26      0.35      0.30       120\n",
            "          79       0.27      0.26      0.27       120\n",
            "\n",
            "    accuracy                           0.24      9577\n",
            "   macro avg       0.26      0.24      0.24      9577\n",
            "weighted avg       0.26      0.24      0.24      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.74it/s, loss=3.1598, acc=22.63%]\n",
            "Epoch 20/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 274.64it/s, loss=3.0895, acc=24.46%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20/500\n",
            "Train Loss: 2.9982, Train Acc: 22.63%\n",
            "Val Loss: 2.8107, Val Acc: 24.46%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.23      0.24       120\n",
            "           1       0.28      0.12      0.16       120\n",
            "           2       0.41      0.10      0.16       120\n",
            "           3       0.22      0.38      0.28       120\n",
            "           4       0.21      0.20      0.21       120\n",
            "           5       0.32      0.22      0.26       120\n",
            "           6       0.21      0.23      0.22       120\n",
            "           7       0.32      0.23      0.26       120\n",
            "           8       0.35      0.31      0.33       120\n",
            "           9       0.32      0.14      0.20       120\n",
            "          10       0.24      0.27      0.25       120\n",
            "          11       0.38      0.33      0.35       120\n",
            "          12       0.26      0.34      0.29       120\n",
            "          13       0.28      0.36      0.32       120\n",
            "          14       0.27      0.29      0.28       120\n",
            "          15       0.17      0.08      0.11       120\n",
            "          16       0.19      0.11      0.14       120\n",
            "          17       0.23      0.30      0.26       120\n",
            "          18       0.21      0.25      0.23       120\n",
            "          19       0.23      0.38      0.29       120\n",
            "          20       0.30      0.27      0.28       120\n",
            "          21       0.20      0.29      0.23       120\n",
            "          22       0.23      0.33      0.27       120\n",
            "          23       0.13      0.16      0.14       120\n",
            "          24       0.27      0.23      0.25       120\n",
            "          25       0.19      0.17      0.18       120\n",
            "          26       0.20      0.26      0.23       120\n",
            "          27       0.20      0.26      0.22       120\n",
            "          28       0.28      0.31      0.29       120\n",
            "          29       0.29      0.19      0.23       120\n",
            "          30       0.23      0.19      0.21       120\n",
            "          31       0.30      0.34      0.32       120\n",
            "          32       0.15      0.31      0.21       120\n",
            "          33       0.26      0.25      0.25       120\n",
            "          34       0.27      0.14      0.18       120\n",
            "          35       0.40      0.46      0.43       120\n",
            "          36       0.40      0.23      0.29       120\n",
            "          37       0.36      0.41      0.38       120\n",
            "          38       0.23      0.21      0.22       120\n",
            "          39       0.30      0.34      0.32       120\n",
            "          40       0.39      0.27      0.32       120\n",
            "          41       0.20      0.18      0.19       120\n",
            "          42       0.19      0.34      0.24       120\n",
            "          43       0.21      0.21      0.21       112\n",
            "          44       0.30      0.17      0.22       120\n",
            "          45       0.19      0.17      0.18       120\n",
            "          46       0.25      0.30      0.27       120\n",
            "          47       0.17      0.23      0.19       120\n",
            "          48       0.34      0.26      0.29       120\n",
            "          49       0.26      0.27      0.26       120\n",
            "          50       0.32      0.27      0.29       120\n",
            "          51       0.29      0.21      0.24       120\n",
            "          52       0.33      0.27      0.29       120\n",
            "          53       0.20      0.17      0.18       120\n",
            "          54       0.28      0.31      0.29       120\n",
            "          55       0.33      0.16      0.21       120\n",
            "          56       0.22      0.20      0.21       120\n",
            "          57       0.23      0.12      0.15       120\n",
            "          58       0.12      0.16      0.14       120\n",
            "          59       0.25      0.16      0.19       120\n",
            "          60       0.17      0.20      0.19       120\n",
            "          61       0.20      0.14      0.17       120\n",
            "          62       0.18      0.12      0.14       120\n",
            "          63       0.19      0.18      0.19       120\n",
            "          64       0.30      0.27      0.28       112\n",
            "          65       0.37      0.31      0.33       120\n",
            "          66       0.30      0.38      0.34       120\n",
            "          67       0.15      0.17      0.16       120\n",
            "          68       0.27      0.34      0.30       120\n",
            "          69       0.24      0.41      0.30       120\n",
            "          70       0.17      0.19      0.18       113\n",
            "          71       0.17      0.14      0.15       120\n",
            "          72       0.16      0.22      0.18       120\n",
            "          73       0.32      0.28      0.30       120\n",
            "          74       0.29      0.18      0.22       120\n",
            "          75       0.23      0.37      0.29       120\n",
            "          76       0.35      0.18      0.24       120\n",
            "          77       0.24      0.30      0.26       120\n",
            "          78       0.34      0.23      0.28       120\n",
            "          79       0.20      0.35      0.26       120\n",
            "\n",
            "    accuracy                           0.24      9577\n",
            "   macro avg       0.26      0.24      0.24      9577\n",
            "weighted avg       0.26      0.24      0.24      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.52it/s, loss=3.1633, acc=23.28%]\n",
            "Epoch 21/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 270.88it/s, loss=2.8716, acc=25.48%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 21/500\n",
            "Train Loss: 2.9766, Train Acc: 23.28%\n",
            "Val Loss: 2.8079, Val Acc: 25.48%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.30      0.27       120\n",
            "           1       0.17      0.27      0.21       120\n",
            "           2       0.28      0.38      0.32       120\n",
            "           3       0.29      0.25      0.27       120\n",
            "           4       0.23      0.30      0.26       120\n",
            "           5       0.14      0.33      0.20       120\n",
            "           6       0.18      0.17      0.18       120\n",
            "           7       0.27      0.19      0.22       120\n",
            "           8       0.30      0.38      0.33       120\n",
            "           9       0.31      0.14      0.19       120\n",
            "          10       0.26      0.23      0.25       120\n",
            "          11       0.36      0.35      0.35       120\n",
            "          12       0.32      0.21      0.25       120\n",
            "          13       0.23      0.28      0.25       120\n",
            "          14       0.27      0.27      0.27       120\n",
            "          15       0.22      0.13      0.16       120\n",
            "          16       0.10      0.10      0.10       120\n",
            "          17       0.32      0.28      0.30       120\n",
            "          18       0.13      0.19      0.15       120\n",
            "          19       0.23      0.38      0.28       120\n",
            "          20       0.24      0.48      0.32       120\n",
            "          21       0.19      0.22      0.20       120\n",
            "          22       0.28      0.25      0.26       120\n",
            "          23       0.21      0.25      0.23       120\n",
            "          24       0.31      0.33      0.32       120\n",
            "          25       0.29      0.28      0.28       120\n",
            "          26       0.23      0.21      0.22       120\n",
            "          27       0.26      0.25      0.25       120\n",
            "          28       0.24      0.37      0.29       120\n",
            "          29       0.29      0.23      0.26       120\n",
            "          30       0.25      0.28      0.26       120\n",
            "          31       0.50      0.27      0.35       120\n",
            "          32       0.32      0.31      0.31       120\n",
            "          33       0.30      0.28      0.29       120\n",
            "          34       0.21      0.28      0.24       120\n",
            "          35       0.32      0.36      0.34       120\n",
            "          36       0.28      0.22      0.24       120\n",
            "          37       0.27      0.38      0.32       120\n",
            "          38       0.32      0.16      0.21       120\n",
            "          39       0.29      0.18      0.22       120\n",
            "          40       0.26      0.30      0.28       120\n",
            "          41       0.32      0.17      0.23       120\n",
            "          42       0.44      0.20      0.28       120\n",
            "          43       0.27      0.22      0.25       112\n",
            "          44       0.27      0.26      0.26       120\n",
            "          45       0.21      0.17      0.19       120\n",
            "          46       0.24      0.27      0.25       120\n",
            "          47       0.35      0.14      0.20       120\n",
            "          48       0.24      0.28      0.26       120\n",
            "          49       0.19      0.28      0.22       120\n",
            "          50       0.41      0.18      0.25       120\n",
            "          51       0.21      0.22      0.22       120\n",
            "          52       0.22      0.33      0.27       120\n",
            "          53       0.26      0.17      0.21       120\n",
            "          54       0.41      0.20      0.27       120\n",
            "          55       0.21      0.23      0.22       120\n",
            "          56       0.22      0.28      0.24       120\n",
            "          57       0.28      0.23      0.25       120\n",
            "          58       0.13      0.23      0.17       120\n",
            "          59       0.29      0.20      0.24       120\n",
            "          60       0.24      0.17      0.20       120\n",
            "          61       0.22      0.14      0.17       120\n",
            "          62       0.15      0.12      0.14       120\n",
            "          63       0.22      0.16      0.18       120\n",
            "          64       0.26      0.29      0.28       112\n",
            "          65       0.33      0.41      0.37       120\n",
            "          66       0.42      0.28      0.33       120\n",
            "          67       0.24      0.17      0.20       120\n",
            "          68       0.29      0.42      0.34       120\n",
            "          69       0.28      0.16      0.20       120\n",
            "          70       0.18      0.10      0.13       113\n",
            "          71       0.21      0.26      0.23       120\n",
            "          72       0.28      0.19      0.23       120\n",
            "          73       0.31      0.42      0.36       120\n",
            "          74       0.23      0.21      0.22       120\n",
            "          75       0.52      0.28      0.37       120\n",
            "          76       0.28      0.30      0.29       120\n",
            "          77       0.30      0.37      0.33       120\n",
            "          78       0.28      0.43      0.34       120\n",
            "          79       0.41      0.27      0.32       120\n",
            "\n",
            "    accuracy                           0.25      9577\n",
            "   macro avg       0.27      0.25      0.25      9577\n",
            "weighted avg       0.27      0.25      0.25      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/500 [Train]: 100%|██████████| 2794/2794 [00:50<00:00, 55.36it/s, loss=3.6551, acc=23.94%]\n",
            "Epoch 22/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 254.66it/s, loss=3.1617, acc=25.21%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 22/500\n",
            "Train Loss: 2.9438, Train Acc: 23.94%\n",
            "Val Loss: 2.7933, Val Acc: 25.21%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.20      0.23       120\n",
            "           1       0.20      0.28      0.23       120\n",
            "           2       0.33      0.22      0.26       120\n",
            "           3       0.26      0.26      0.26       120\n",
            "           4       0.27      0.18      0.22       120\n",
            "           5       0.26      0.13      0.18       120\n",
            "           6       0.26      0.28      0.27       120\n",
            "           7       0.23      0.32      0.27       120\n",
            "           8       0.37      0.27      0.31       120\n",
            "           9       0.24      0.13      0.17       120\n",
            "          10       0.25      0.15      0.19       120\n",
            "          11       0.28      0.41      0.33       120\n",
            "          12       0.29      0.31      0.30       120\n",
            "          13       0.28      0.28      0.28       120\n",
            "          14       0.25      0.30      0.27       120\n",
            "          15       0.22      0.26      0.24       120\n",
            "          16       0.17      0.25      0.20       120\n",
            "          17       0.25      0.28      0.26       120\n",
            "          18       0.12      0.12      0.12       120\n",
            "          19       0.21      0.41      0.27       120\n",
            "          20       0.29      0.35      0.32       120\n",
            "          21       0.26      0.17      0.20       120\n",
            "          22       0.21      0.19      0.20       120\n",
            "          23       0.30      0.27      0.28       120\n",
            "          24       0.24      0.31      0.27       120\n",
            "          25       0.25      0.25      0.25       120\n",
            "          26       0.23      0.22      0.23       120\n",
            "          27       0.29      0.23      0.26       120\n",
            "          28       0.23      0.25      0.24       120\n",
            "          29       0.29      0.33      0.31       120\n",
            "          30       0.21      0.28      0.24       120\n",
            "          31       0.28      0.29      0.28       120\n",
            "          32       0.27      0.27      0.27       120\n",
            "          33       0.28      0.23      0.25       120\n",
            "          34       0.21      0.20      0.21       120\n",
            "          35       0.51      0.38      0.43       120\n",
            "          36       0.17      0.33      0.23       120\n",
            "          37       0.40      0.27      0.32       120\n",
            "          38       0.25      0.25      0.25       120\n",
            "          39       0.26      0.22      0.24       120\n",
            "          40       0.27      0.36      0.30       120\n",
            "          41       0.22      0.29      0.25       120\n",
            "          42       0.26      0.15      0.19       120\n",
            "          43       0.39      0.29      0.33       112\n",
            "          44       0.32      0.29      0.30       120\n",
            "          45       0.14      0.23      0.18       120\n",
            "          46       0.24      0.19      0.21       120\n",
            "          47       0.25      0.23      0.24       120\n",
            "          48       0.26      0.20      0.23       120\n",
            "          49       0.33      0.19      0.24       120\n",
            "          50       0.36      0.29      0.32       120\n",
            "          51       0.18      0.34      0.23       120\n",
            "          52       0.30      0.19      0.23       120\n",
            "          53       0.16      0.19      0.17       120\n",
            "          54       0.28      0.26      0.27       120\n",
            "          55       0.39      0.16      0.22       120\n",
            "          56       0.24      0.17      0.20       120\n",
            "          57       0.31      0.25      0.28       120\n",
            "          58       0.15      0.09      0.11       120\n",
            "          59       0.25      0.21      0.23       120\n",
            "          60       0.19      0.17      0.18       120\n",
            "          61       0.18      0.21      0.19       120\n",
            "          62       0.22      0.23      0.23       120\n",
            "          63       0.28      0.13      0.18       120\n",
            "          64       0.34      0.29      0.32       112\n",
            "          65       0.45      0.31      0.37       120\n",
            "          66       0.35      0.39      0.37       120\n",
            "          67       0.20      0.19      0.20       120\n",
            "          68       0.31      0.26      0.28       120\n",
            "          69       0.20      0.15      0.17       120\n",
            "          70       0.14      0.16      0.15       113\n",
            "          71       0.25      0.29      0.27       120\n",
            "          72       0.26      0.17      0.21       120\n",
            "          73       0.34      0.37      0.35       120\n",
            "          74       0.27      0.20      0.23       120\n",
            "          75       0.30      0.39      0.34       120\n",
            "          76       0.20      0.32      0.24       120\n",
            "          77       0.20      0.39      0.26       120\n",
            "          78       0.29      0.40      0.34       120\n",
            "          79       0.29      0.28      0.29       120\n",
            "\n",
            "    accuracy                           0.25      9577\n",
            "   macro avg       0.26      0.25      0.25      9577\n",
            "weighted avg       0.26      0.25      0.25      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/500 [Train]: 100%|██████████| 2794/2794 [00:50<00:00, 55.28it/s, loss=2.8625, acc=24.29%]\n",
            "Epoch 23/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 263.80it/s, loss=2.1168, acc=24.98%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 23/500\n",
            "Train Loss: 2.9306, Train Acc: 24.29%\n",
            "Val Loss: 2.7954, Val Acc: 24.98%\n",
            "Learning rate: 0.000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.26it/s, loss=3.0766, acc=24.73%]\n",
            "Epoch 24/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 262.75it/s, loss=2.7322, acc=25.59%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 24/500\n",
            "Train Loss: 2.9080, Train Acc: 24.73%\n",
            "Val Loss: 2.7790, Val Acc: 25.59%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.22      0.28      0.24       120\n",
            "           1       0.35      0.17      0.23       120\n",
            "           2       0.35      0.35      0.35       120\n",
            "           3       0.29      0.16      0.20       120\n",
            "           4       0.19      0.28      0.23       120\n",
            "           5       0.25      0.30      0.27       120\n",
            "           6       0.29      0.27      0.28       120\n",
            "           7       0.26      0.30      0.28       120\n",
            "           8       0.34      0.33      0.33       120\n",
            "           9       0.27      0.22      0.24       120\n",
            "          10       0.22      0.19      0.20       120\n",
            "          11       0.37      0.42      0.39       120\n",
            "          12       0.28      0.31      0.29       120\n",
            "          13       0.24      0.31      0.27       120\n",
            "          14       0.37      0.16      0.22       120\n",
            "          15       0.20      0.06      0.09       120\n",
            "          16       0.18      0.26      0.21       120\n",
            "          17       0.19      0.30      0.23       120\n",
            "          18       0.18      0.15      0.17       120\n",
            "          19       0.25      0.36      0.29       120\n",
            "          20       0.22      0.40      0.28       120\n",
            "          21       0.28      0.18      0.22       120\n",
            "          22       0.23      0.28      0.25       120\n",
            "          23       0.17      0.11      0.13       120\n",
            "          24       0.28      0.24      0.26       120\n",
            "          25       0.24      0.23      0.24       120\n",
            "          26       0.26      0.27      0.26       120\n",
            "          27       0.29      0.18      0.22       120\n",
            "          28       0.29      0.33      0.31       120\n",
            "          29       0.31      0.16      0.21       120\n",
            "          30       0.29      0.13      0.18       120\n",
            "          31       0.29      0.32      0.30       120\n",
            "          32       0.46      0.26      0.33       120\n",
            "          33       0.22      0.24      0.23       120\n",
            "          34       0.33      0.16      0.21       120\n",
            "          35       0.41      0.39      0.40       120\n",
            "          36       0.19      0.20      0.19       120\n",
            "          37       0.33      0.33      0.33       120\n",
            "          38       0.30      0.21      0.25       120\n",
            "          39       0.29      0.17      0.22       120\n",
            "          40       0.29      0.33      0.30       120\n",
            "          41       0.26      0.29      0.27       120\n",
            "          42       0.23      0.28      0.25       120\n",
            "          43       0.29      0.26      0.27       112\n",
            "          44       0.36      0.23      0.28       120\n",
            "          45       0.15      0.23      0.18       120\n",
            "          46       0.30      0.13      0.18       120\n",
            "          47       0.29      0.23      0.26       120\n",
            "          48       0.23      0.31      0.26       120\n",
            "          49       0.23      0.28      0.26       120\n",
            "          50       0.38      0.19      0.26       120\n",
            "          51       0.25      0.27      0.26       120\n",
            "          52       0.25      0.32      0.28       120\n",
            "          53       0.35      0.14      0.20       120\n",
            "          54       0.23      0.33      0.27       120\n",
            "          55       0.26      0.23      0.24       120\n",
            "          56       0.22      0.25      0.23       120\n",
            "          57       0.28      0.21      0.24       120\n",
            "          58       0.13      0.17      0.14       120\n",
            "          59       0.22      0.33      0.26       120\n",
            "          60       0.16      0.07      0.09       120\n",
            "          61       0.21      0.19      0.20       120\n",
            "          62       0.15      0.19      0.17       120\n",
            "          63       0.26      0.27      0.26       120\n",
            "          64       0.24      0.23      0.24       112\n",
            "          65       0.34      0.30      0.32       120\n",
            "          66       0.29      0.44      0.35       120\n",
            "          67       0.22      0.20      0.21       120\n",
            "          68       0.28      0.28      0.28       120\n",
            "          69       0.23      0.32      0.27       120\n",
            "          70       0.14      0.25      0.18       113\n",
            "          71       0.28      0.26      0.27       120\n",
            "          72       0.30      0.22      0.25       120\n",
            "          73       0.27      0.38      0.32       120\n",
            "          74       0.24      0.22      0.23       120\n",
            "          75       0.37      0.36      0.36       120\n",
            "          76       0.32      0.38      0.34       120\n",
            "          77       0.29      0.28      0.28       120\n",
            "          78       0.31      0.34      0.33       120\n",
            "          79       0.27      0.37      0.31       120\n",
            "\n",
            "    accuracy                           0.26      9577\n",
            "   macro avg       0.27      0.26      0.25      9577\n",
            "weighted avg       0.27      0.26      0.25      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.63it/s, loss=2.9067, acc=24.98%]\n",
            "Epoch 25/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 236.49it/s, loss=2.5435, acc=25.56%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 25/500\n",
            "Train Loss: 2.8930, Train Acc: 24.98%\n",
            "Val Loss: 2.7753, Val Acc: 25.56%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.24      0.29       120\n",
            "           1       0.23      0.24      0.24       120\n",
            "           2       0.26      0.43      0.33       120\n",
            "           3       0.25      0.27      0.26       120\n",
            "           4       0.27      0.19      0.23       120\n",
            "           5       0.26      0.21      0.23       120\n",
            "           6       0.23      0.21      0.22       120\n",
            "           7       0.29      0.20      0.24       120\n",
            "           8       0.31      0.42      0.36       120\n",
            "           9       0.30      0.05      0.09       120\n",
            "          10       0.29      0.14      0.19       120\n",
            "          11       0.35      0.38      0.36       120\n",
            "          12       0.29      0.34      0.31       120\n",
            "          13       0.28      0.28      0.28       120\n",
            "          14       0.24      0.24      0.24       120\n",
            "          15       0.20      0.19      0.20       120\n",
            "          16       0.14      0.20      0.16       120\n",
            "          17       0.26      0.21      0.23       120\n",
            "          18       0.18      0.18      0.18       120\n",
            "          19       0.30      0.32      0.31       120\n",
            "          20       0.33      0.30      0.31       120\n",
            "          21       0.22      0.21      0.22       120\n",
            "          22       0.23      0.35      0.27       120\n",
            "          23       0.26      0.13      0.18       120\n",
            "          24       0.31      0.30      0.30       120\n",
            "          25       0.20      0.33      0.24       120\n",
            "          26       0.38      0.18      0.25       120\n",
            "          27       0.29      0.20      0.24       120\n",
            "          28       0.24      0.31      0.27       120\n",
            "          29       0.33      0.33      0.33       120\n",
            "          30       0.25      0.22      0.23       120\n",
            "          31       0.35      0.28      0.31       120\n",
            "          32       0.31      0.31      0.31       120\n",
            "          33       0.31      0.17      0.22       120\n",
            "          34       0.24      0.29      0.26       120\n",
            "          35       0.37      0.34      0.35       120\n",
            "          36       0.26      0.26      0.26       120\n",
            "          37       0.25      0.34      0.29       120\n",
            "          38       0.22      0.22      0.22       120\n",
            "          39       0.33      0.33      0.33       120\n",
            "          40       0.22      0.36      0.27       120\n",
            "          41       0.31      0.22      0.25       120\n",
            "          42       0.24      0.33      0.28       120\n",
            "          43       0.21      0.31      0.25       112\n",
            "          44       0.29      0.35      0.32       120\n",
            "          45       0.17      0.30      0.22       120\n",
            "          46       0.25      0.22      0.23       120\n",
            "          47       0.23      0.28      0.25       120\n",
            "          48       0.27      0.17      0.21       120\n",
            "          49       0.18      0.30      0.23       120\n",
            "          50       0.26      0.30      0.28       120\n",
            "          51       0.22      0.23      0.22       120\n",
            "          52       0.19      0.37      0.25       120\n",
            "          53       0.18      0.23      0.20       120\n",
            "          54       0.27      0.33      0.29       120\n",
            "          55       0.30      0.17      0.21       120\n",
            "          56       0.26      0.21      0.23       120\n",
            "          57       0.45      0.16      0.23       120\n",
            "          58       0.22      0.12      0.16       120\n",
            "          59       0.28      0.24      0.26       120\n",
            "          60       0.16      0.24      0.19       120\n",
            "          61       0.26      0.26      0.26       120\n",
            "          62       0.25      0.17      0.21       120\n",
            "          63       0.22      0.19      0.20       120\n",
            "          64       0.25      0.25      0.25       112\n",
            "          65       0.41      0.23      0.29       120\n",
            "          66       0.33      0.36      0.34       120\n",
            "          67       0.22      0.14      0.17       120\n",
            "          68       0.30      0.30      0.30       120\n",
            "          69       0.19      0.32      0.24       120\n",
            "          70       0.23      0.19      0.21       113\n",
            "          71       0.27      0.23      0.25       120\n",
            "          72       0.18      0.16      0.17       120\n",
            "          73       0.36      0.35      0.35       120\n",
            "          74       0.16      0.26      0.20       120\n",
            "          75       0.38      0.28      0.32       120\n",
            "          76       0.34      0.36      0.35       120\n",
            "          77       0.34      0.29      0.31       120\n",
            "          78       0.29      0.25      0.27       120\n",
            "          79       0.33      0.12      0.18       120\n",
            "\n",
            "    accuracy                           0.26      9577\n",
            "   macro avg       0.27      0.26      0.25      9577\n",
            "weighted avg       0.27      0.26      0.25      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.22it/s, loss=3.1569, acc=25.32%]\n",
            "Epoch 26/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 236.32it/s, loss=3.2857, acc=25.58%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 26/500\n",
            "Train Loss: 2.8715, Train Acc: 25.32%\n",
            "Val Loss: 2.7640, Val Acc: 25.58%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.19      0.21       120\n",
            "           1       0.31      0.25      0.28       120\n",
            "           2       0.22      0.46      0.29       120\n",
            "           3       0.27      0.17      0.21       120\n",
            "           4       0.22      0.27      0.24       120\n",
            "           5       0.25      0.23      0.24       120\n",
            "           6       0.22      0.36      0.27       120\n",
            "           7       0.26      0.28      0.27       120\n",
            "           8       0.30      0.24      0.27       120\n",
            "           9       0.34      0.14      0.20       120\n",
            "          10       0.23      0.15      0.18       120\n",
            "          11       0.42      0.28      0.33       120\n",
            "          12       0.28      0.33      0.30       120\n",
            "          13       0.24      0.32      0.27       120\n",
            "          14       0.32      0.26      0.29       120\n",
            "          15       0.29      0.18      0.22       120\n",
            "          16       0.18      0.10      0.13       120\n",
            "          17       0.19      0.33      0.24       120\n",
            "          18       0.21      0.27      0.23       120\n",
            "          19       0.26      0.33      0.29       120\n",
            "          20       0.31      0.38      0.34       120\n",
            "          21       0.18      0.18      0.18       120\n",
            "          22       0.22      0.30      0.26       120\n",
            "          23       0.19      0.33      0.24       120\n",
            "          24       0.27      0.25      0.26       120\n",
            "          25       0.41      0.23      0.30       120\n",
            "          26       0.22      0.17      0.19       120\n",
            "          27       0.29      0.16      0.20       120\n",
            "          28       0.31      0.17      0.22       120\n",
            "          29       0.42      0.21      0.28       120\n",
            "          30       0.25      0.28      0.27       120\n",
            "          31       0.37      0.23      0.28       120\n",
            "          32       0.40      0.18      0.25       120\n",
            "          33       0.24      0.23      0.23       120\n",
            "          34       0.28      0.29      0.29       120\n",
            "          35       0.35      0.43      0.39       120\n",
            "          36       0.21      0.35      0.26       120\n",
            "          37       0.36      0.32      0.34       120\n",
            "          38       0.19      0.16      0.17       120\n",
            "          39       0.26      0.23      0.25       120\n",
            "          40       0.24      0.40      0.30       120\n",
            "          41       0.34      0.26      0.30       120\n",
            "          42       0.20      0.27      0.23       120\n",
            "          43       0.21      0.25      0.23       112\n",
            "          44       0.35      0.28      0.31       120\n",
            "          45       0.19      0.22      0.20       120\n",
            "          46       0.22      0.23      0.22       120\n",
            "          47       0.20      0.29      0.24       120\n",
            "          48       0.30      0.23      0.26       120\n",
            "          49       0.30      0.26      0.28       120\n",
            "          50       0.28      0.33      0.30       120\n",
            "          51       0.26      0.23      0.24       120\n",
            "          52       0.31      0.21      0.25       120\n",
            "          53       0.20      0.27      0.23       120\n",
            "          54       0.25      0.26      0.25       120\n",
            "          55       0.30      0.31      0.30       120\n",
            "          56       0.13      0.28      0.18       120\n",
            "          57       0.28      0.23      0.25       120\n",
            "          58       0.25      0.13      0.17       120\n",
            "          59       0.31      0.17      0.22       120\n",
            "          60       0.20      0.18      0.19       120\n",
            "          61       0.15      0.12      0.13       120\n",
            "          62       0.19      0.20      0.19       120\n",
            "          63       0.34      0.17      0.22       120\n",
            "          64       0.29      0.25      0.27       112\n",
            "          65       0.37      0.25      0.30       120\n",
            "          66       0.33      0.39      0.36       120\n",
            "          67       0.16      0.17      0.16       120\n",
            "          68       0.33      0.40      0.36       120\n",
            "          69       0.23      0.23      0.23       120\n",
            "          70       0.24      0.14      0.18       113\n",
            "          71       0.25      0.20      0.22       120\n",
            "          72       0.25      0.12      0.17       120\n",
            "          73       0.34      0.35      0.34       120\n",
            "          74       0.21      0.32      0.25       120\n",
            "          75       0.25      0.40      0.30       120\n",
            "          76       0.33      0.18      0.24       120\n",
            "          77       0.28      0.39      0.33       120\n",
            "          78       0.28      0.46      0.35       120\n",
            "          79       0.38      0.23      0.29       120\n",
            "\n",
            "    accuracy                           0.26      9577\n",
            "   macro avg       0.27      0.26      0.25      9577\n",
            "weighted avg       0.27      0.26      0.25      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.69it/s, loss=4.1751, acc=25.62%]\n",
            "Epoch 27/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 261.06it/s, loss=3.2078, acc=25.50%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 27/500\n",
            "Train Loss: 2.8532, Train Acc: 25.62%\n",
            "Val Loss: 2.7565, Val Acc: 25.50%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.20      0.22       120\n",
            "           1       0.31      0.28      0.29       120\n",
            "           2       0.22      0.36      0.27       120\n",
            "           3       0.41      0.23      0.29       120\n",
            "           4       0.21      0.27      0.23       120\n",
            "           5       0.15      0.28      0.19       120\n",
            "           6       0.23      0.20      0.22       120\n",
            "           7       0.28      0.22      0.24       120\n",
            "           8       0.27      0.32      0.29       120\n",
            "           9       0.22      0.09      0.13       120\n",
            "          10       0.17      0.17      0.17       120\n",
            "          11       0.47      0.32      0.38       120\n",
            "          12       0.26      0.33      0.29       120\n",
            "          13       0.25      0.28      0.26       120\n",
            "          14       0.21      0.20      0.21       120\n",
            "          15       0.26      0.21      0.23       120\n",
            "          16       0.19      0.17      0.18       120\n",
            "          17       0.21      0.24      0.23       120\n",
            "          18       0.17      0.23      0.20       120\n",
            "          19       0.32      0.27      0.29       120\n",
            "          20       0.51      0.25      0.34       120\n",
            "          21       0.30      0.20      0.24       120\n",
            "          22       0.26      0.23      0.25       120\n",
            "          23       0.24      0.22      0.23       120\n",
            "          24       0.26      0.23      0.24       120\n",
            "          25       0.27      0.27      0.27       120\n",
            "          26       0.32      0.18      0.23       120\n",
            "          27       0.24      0.22      0.23       120\n",
            "          28       0.23      0.39      0.29       120\n",
            "          29       0.20      0.37      0.26       120\n",
            "          30       0.20      0.33      0.25       120\n",
            "          31       0.35      0.27      0.30       120\n",
            "          32       0.31      0.30      0.31       120\n",
            "          33       0.22      0.30      0.26       120\n",
            "          34       0.31      0.33      0.32       120\n",
            "          35       0.25      0.44      0.32       120\n",
            "          36       0.22      0.32      0.26       120\n",
            "          37       0.30      0.38      0.33       120\n",
            "          38       0.23      0.13      0.17       120\n",
            "          39       0.33      0.35      0.34       120\n",
            "          40       0.22      0.23      0.23       120\n",
            "          41       0.24      0.39      0.30       120\n",
            "          42       0.22      0.16      0.19       120\n",
            "          43       0.24      0.25      0.24       112\n",
            "          44       0.34      0.32      0.33       120\n",
            "          45       0.27      0.22      0.24       120\n",
            "          46       0.23      0.26      0.24       120\n",
            "          47       0.18      0.29      0.22       120\n",
            "          48       0.31      0.22      0.25       120\n",
            "          49       0.23      0.25      0.24       120\n",
            "          50       0.27      0.28      0.27       120\n",
            "          51       0.18      0.24      0.20       120\n",
            "          52       0.23      0.35      0.28       120\n",
            "          53       0.18      0.18      0.18       120\n",
            "          54       0.35      0.20      0.25       120\n",
            "          55       0.36      0.20      0.26       120\n",
            "          56       0.29      0.20      0.24       120\n",
            "          57       0.25      0.23      0.24       120\n",
            "          58       0.18      0.08      0.11       120\n",
            "          59       0.28      0.23      0.26       120\n",
            "          60       0.33      0.12      0.17       120\n",
            "          61       0.22      0.27      0.24       120\n",
            "          62       0.26      0.18      0.21       120\n",
            "          63       0.23      0.15      0.18       120\n",
            "          64       0.32      0.35      0.33       112\n",
            "          65       0.28      0.35      0.31       120\n",
            "          66       0.34      0.34      0.34       120\n",
            "          67       0.19      0.12      0.14       120\n",
            "          68       0.33      0.24      0.28       120\n",
            "          69       0.22      0.21      0.21       120\n",
            "          70       0.25      0.12      0.16       113\n",
            "          71       0.31      0.24      0.27       120\n",
            "          72       0.26      0.18      0.21       120\n",
            "          73       0.31      0.41      0.35       120\n",
            "          74       0.15      0.30      0.20       120\n",
            "          75       0.32      0.25      0.28       120\n",
            "          76       0.34      0.26      0.29       120\n",
            "          77       0.35      0.33      0.34       120\n",
            "          78       0.34      0.36      0.35       120\n",
            "          79       0.33      0.30      0.32       120\n",
            "\n",
            "    accuracy                           0.25      9577\n",
            "   macro avg       0.27      0.25      0.25      9577\n",
            "weighted avg       0.27      0.25      0.25      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/500 [Train]: 100%|██████████| 2794/2794 [00:50<00:00, 55.49it/s, loss=2.9990, acc=26.12%]\n",
            "Epoch 28/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 258.12it/s, loss=2.4495, acc=25.80%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 28/500\n",
            "Train Loss: 2.8414, Train Acc: 26.12%\n",
            "Val Loss: 2.7387, Val Acc: 25.80%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.14      0.19       120\n",
            "           1       0.28      0.28      0.28       120\n",
            "           2       0.35      0.31      0.33       120\n",
            "           3       0.35      0.23      0.28       120\n",
            "           4       0.23      0.31      0.27       120\n",
            "           5       0.32      0.28      0.30       120\n",
            "           6       0.33      0.23      0.27       120\n",
            "           7       0.26      0.28      0.27       120\n",
            "           8       0.28      0.39      0.33       120\n",
            "           9       0.20      0.23      0.21       120\n",
            "          10       0.20      0.21      0.20       120\n",
            "          11       0.29      0.35      0.31       120\n",
            "          12       0.31      0.30      0.31       120\n",
            "          13       0.24      0.14      0.18       120\n",
            "          14       0.23      0.23      0.23       120\n",
            "          15       0.22      0.31      0.25       120\n",
            "          16       0.26      0.12      0.17       120\n",
            "          17       0.23      0.28      0.25       120\n",
            "          18       0.12      0.15      0.13       120\n",
            "          19       0.31      0.27      0.29       120\n",
            "          20       0.28      0.40      0.33       120\n",
            "          21       0.21      0.18      0.19       120\n",
            "          22       0.21      0.21      0.21       120\n",
            "          23       0.24      0.19      0.21       120\n",
            "          24       0.23      0.26      0.24       120\n",
            "          25       0.38      0.18      0.25       120\n",
            "          26       0.23      0.42      0.30       120\n",
            "          27       0.28      0.41      0.33       120\n",
            "          28       0.23      0.12      0.15       120\n",
            "          29       0.31      0.38      0.34       120\n",
            "          30       0.26      0.23      0.24       120\n",
            "          31       0.19      0.25      0.21       120\n",
            "          32       0.20      0.45      0.28       120\n",
            "          33       0.23      0.27      0.25       120\n",
            "          34       0.40      0.12      0.18       120\n",
            "          35       0.44      0.43      0.44       120\n",
            "          36       0.18      0.26      0.21       120\n",
            "          37       0.28      0.26      0.27       120\n",
            "          38       0.31      0.18      0.23       120\n",
            "          39       0.25      0.27      0.26       120\n",
            "          40       0.42      0.33      0.37       120\n",
            "          41       0.42      0.31      0.36       120\n",
            "          42       0.35      0.21      0.26       120\n",
            "          43       0.26      0.32      0.29       112\n",
            "          44       0.45      0.33      0.38       120\n",
            "          45       0.19      0.33      0.24       120\n",
            "          46       0.28      0.19      0.23       120\n",
            "          47       0.26      0.21      0.23       120\n",
            "          48       0.28      0.21      0.24       120\n",
            "          49       0.27      0.27      0.27       120\n",
            "          50       0.30      0.23      0.26       120\n",
            "          51       0.27      0.22      0.24       120\n",
            "          52       0.33      0.28      0.30       120\n",
            "          53       0.21      0.14      0.17       120\n",
            "          54       0.34      0.17      0.22       120\n",
            "          55       0.24      0.28      0.26       120\n",
            "          56       0.22      0.22      0.22       120\n",
            "          57       0.29      0.25      0.27       120\n",
            "          58       0.19      0.17      0.18       120\n",
            "          59       0.29      0.19      0.23       120\n",
            "          60       0.20      0.22      0.21       120\n",
            "          61       0.24      0.14      0.18       120\n",
            "          62       0.16      0.26      0.20       120\n",
            "          63       0.28      0.22      0.24       120\n",
            "          64       0.31      0.21      0.25       112\n",
            "          65       0.30      0.34      0.32       120\n",
            "          66       0.34      0.36      0.35       120\n",
            "          67       0.17      0.12      0.14       120\n",
            "          68       0.23      0.39      0.29       120\n",
            "          69       0.14      0.23      0.18       120\n",
            "          70       0.26      0.18      0.21       113\n",
            "          71       0.17      0.27      0.21       120\n",
            "          72       0.21      0.28      0.24       120\n",
            "          73       0.33      0.26      0.29       120\n",
            "          74       0.23      0.18      0.20       120\n",
            "          75       0.37      0.32      0.34       120\n",
            "          76       0.32      0.31      0.31       120\n",
            "          77       0.26      0.28      0.27       120\n",
            "          78       0.30      0.38      0.33       120\n",
            "          79       0.35      0.36      0.35       120\n",
            "\n",
            "    accuracy                           0.26      9577\n",
            "   macro avg       0.27      0.26      0.26      9577\n",
            "weighted avg       0.27      0.26      0.26      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.26it/s, loss=3.0895, acc=26.58%]\n",
            "Epoch 29/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 260.06it/s, loss=3.1282, acc=25.96%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 29/500\n",
            "Train Loss: 2.8217, Train Acc: 26.58%\n",
            "Val Loss: 2.7687, Val Acc: 25.96%\n",
            "Learning rate: 0.000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/500 [Train]: 100%|██████████| 2794/2794 [00:50<00:00, 55.84it/s, loss=4.3535, acc=27.01%]\n",
            "Epoch 30/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 241.19it/s, loss=2.8196, acc=25.99%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 30/500\n",
            "Train Loss: 2.7980, Train Acc: 27.01%\n",
            "Val Loss: 2.7708, Val Acc: 25.99%\n",
            "Learning rate: 0.000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/500 [Train]: 100%|██████████| 2794/2794 [00:50<00:00, 55.55it/s, loss=2.9778, acc=27.46%]\n",
            "Epoch 31/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 250.39it/s, loss=2.6738, acc=25.98%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 31/500\n",
            "Train Loss: 2.7814, Train Acc: 27.46%\n",
            "Val Loss: 2.7645, Val Acc: 25.98%\n",
            "Learning rate: 0.000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.28it/s, loss=3.1734, acc=27.65%]\n",
            "Epoch 32/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 253.76it/s, loss=2.9274, acc=26.40%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 32/500\n",
            "Train Loss: 2.7724, Train Acc: 27.65%\n",
            "Val Loss: 2.7531, Val Acc: 26.40%\n",
            "Learning rate: 0.000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.48it/s, loss=4.3072, acc=28.04%]\n",
            "Epoch 33/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 259.08it/s, loss=3.1009, acc=26.74%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 33/500\n",
            "Train Loss: 2.7589, Train Acc: 28.04%\n",
            "Val Loss: 2.7559, Val Acc: 26.74%\n",
            "Learning rate: 0.000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/500 [Train]: 100%|██████████| 2794/2794 [00:50<00:00, 55.42it/s, loss=2.5498, acc=28.23%]\n",
            "Epoch 34/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 262.16it/s, loss=2.5061, acc=25.77%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 34/500\n",
            "Train Loss: 2.7493, Train Acc: 28.23%\n",
            "Val Loss: 2.7926, Val Acc: 25.77%\n",
            "Learning rate: 0.000250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 35/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.43it/s, loss=2.5255, acc=33.86%]\n",
            "Epoch 35/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 257.89it/s, loss=2.6408, acc=29.05%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 35/500\n",
            "Train Loss: 2.4887, Train Acc: 33.86%\n",
            "Val Loss: 2.6285, Val Acc: 29.05%\n",
            "Learning rate: 0.000250\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.28      0.32      0.29       120\n",
            "           1       0.23      0.38      0.29       120\n",
            "           2       0.34      0.34      0.34       120\n",
            "           3       0.30      0.17      0.22       120\n",
            "           4       0.35      0.25      0.29       120\n",
            "           5       0.40      0.33      0.36       120\n",
            "           6       0.23      0.33      0.27       120\n",
            "           7       0.37      0.27      0.31       120\n",
            "           8       0.28      0.46      0.35       120\n",
            "           9       0.24      0.22      0.23       120\n",
            "          10       0.31      0.25      0.28       120\n",
            "          11       0.41      0.47      0.44       120\n",
            "          12       0.25      0.37      0.30       120\n",
            "          13       0.25      0.25      0.25       120\n",
            "          14       0.34      0.31      0.32       120\n",
            "          15       0.25      0.21      0.23       120\n",
            "          16       0.19      0.12      0.15       120\n",
            "          17       0.25      0.27      0.26       120\n",
            "          18       0.19      0.24      0.21       120\n",
            "          19       0.28      0.32      0.30       120\n",
            "          20       0.40      0.36      0.38       120\n",
            "          21       0.21      0.24      0.23       120\n",
            "          22       0.26      0.35      0.30       120\n",
            "          23       0.30      0.25      0.27       120\n",
            "          24       0.28      0.28      0.28       120\n",
            "          25       0.41      0.22      0.28       120\n",
            "          26       0.24      0.30      0.27       120\n",
            "          27       0.28      0.27      0.27       120\n",
            "          28       0.26      0.33      0.29       120\n",
            "          29       0.30      0.42      0.35       120\n",
            "          30       0.35      0.31      0.33       120\n",
            "          31       0.44      0.31      0.36       120\n",
            "          32       0.31      0.36      0.33       120\n",
            "          33       0.25      0.31      0.28       120\n",
            "          34       0.26      0.26      0.26       120\n",
            "          35       0.40      0.49      0.44       120\n",
            "          36       0.26      0.23      0.24       120\n",
            "          37       0.26      0.43      0.32       120\n",
            "          38       0.31      0.28      0.29       120\n",
            "          39       0.36      0.28      0.31       120\n",
            "          40       0.40      0.41      0.40       120\n",
            "          41       0.43      0.37      0.39       120\n",
            "          42       0.26      0.19      0.22       120\n",
            "          43       0.26      0.29      0.27       112\n",
            "          44       0.37      0.42      0.39       120\n",
            "          45       0.31      0.22      0.25       120\n",
            "          46       0.25      0.15      0.19       120\n",
            "          47       0.29      0.28      0.29       120\n",
            "          48       0.23      0.28      0.25       120\n",
            "          49       0.26      0.32      0.28       120\n",
            "          50       0.27      0.19      0.22       120\n",
            "          51       0.23      0.24      0.24       120\n",
            "          52       0.32      0.24      0.28       120\n",
            "          53       0.25      0.24      0.24       120\n",
            "          54       0.33      0.31      0.32       120\n",
            "          55       0.26      0.19      0.22       120\n",
            "          56       0.28      0.21      0.24       120\n",
            "          57       0.37      0.22      0.27       120\n",
            "          58       0.17      0.18      0.18       120\n",
            "          59       0.26      0.23      0.25       120\n",
            "          60       0.20      0.12      0.15       120\n",
            "          61       0.21      0.16      0.18       120\n",
            "          62       0.26      0.19      0.22       120\n",
            "          63       0.27      0.36      0.31       120\n",
            "          64       0.34      0.43      0.38       112\n",
            "          65       0.26      0.35      0.30       120\n",
            "          66       0.35      0.43      0.39       120\n",
            "          67       0.29      0.24      0.26       120\n",
            "          68       0.28      0.38      0.32       120\n",
            "          69       0.22      0.26      0.24       120\n",
            "          70       0.27      0.16      0.20       113\n",
            "          71       0.30      0.26      0.28       120\n",
            "          72       0.25      0.23      0.24       120\n",
            "          73       0.35      0.42      0.39       120\n",
            "          74       0.23      0.31      0.26       120\n",
            "          75       0.38      0.38      0.38       120\n",
            "          76       0.39      0.33      0.35       120\n",
            "          77       0.31      0.25      0.28       120\n",
            "          78       0.47      0.35      0.40       120\n",
            "          79       0.28      0.37      0.32       120\n",
            "\n",
            "    accuracy                           0.29      9577\n",
            "   macro avg       0.29      0.29      0.29      9577\n",
            "weighted avg       0.29      0.29      0.29      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 36/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 55.95it/s, loss=3.2883, acc=36.55%]\n",
            "Epoch 36/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 240.96it/s, loss=2.7747, acc=29.63%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 36/500\n",
            "Train Loss: 2.3655, Train Acc: 36.55%\n",
            "Val Loss: 2.6263, Val Acc: 29.63%\n",
            "Learning rate: 0.000250\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.27      0.23      0.24       120\n",
            "           1       0.23      0.28      0.26       120\n",
            "           2       0.37      0.34      0.35       120\n",
            "           3       0.45      0.18      0.26       120\n",
            "           4       0.29      0.27      0.28       120\n",
            "           5       0.32      0.30      0.31       120\n",
            "           6       0.27      0.33      0.30       120\n",
            "           7       0.31      0.33      0.32       120\n",
            "           8       0.40      0.46      0.43       120\n",
            "           9       0.20      0.12      0.15       120\n",
            "          10       0.33      0.28      0.30       120\n",
            "          11       0.34      0.53      0.42       120\n",
            "          12       0.26      0.41      0.31       120\n",
            "          13       0.29      0.42      0.34       120\n",
            "          14       0.31      0.23      0.26       120\n",
            "          15       0.28      0.19      0.23       120\n",
            "          16       0.22      0.24      0.23       120\n",
            "          17       0.36      0.30      0.33       120\n",
            "          18       0.28      0.15      0.19       120\n",
            "          19       0.33      0.30      0.32       120\n",
            "          20       0.33      0.33      0.33       120\n",
            "          21       0.27      0.28      0.28       120\n",
            "          22       0.26      0.33      0.29       120\n",
            "          23       0.24      0.34      0.28       120\n",
            "          24       0.29      0.28      0.28       120\n",
            "          25       0.35      0.28      0.31       120\n",
            "          26       0.30      0.23      0.26       120\n",
            "          27       0.31      0.25      0.28       120\n",
            "          28       0.27      0.38      0.32       120\n",
            "          29       0.38      0.35      0.36       120\n",
            "          30       0.23      0.29      0.26       120\n",
            "          31       0.40      0.29      0.34       120\n",
            "          32       0.33      0.34      0.33       120\n",
            "          33       0.25      0.24      0.24       120\n",
            "          34       0.29      0.23      0.25       120\n",
            "          35       0.44      0.42      0.43       120\n",
            "          36       0.34      0.29      0.31       120\n",
            "          37       0.34      0.39      0.36       120\n",
            "          38       0.27      0.30      0.28       120\n",
            "          39       0.27      0.36      0.31       120\n",
            "          40       0.31      0.33      0.32       120\n",
            "          41       0.33      0.38      0.35       120\n",
            "          42       0.27      0.23      0.24       120\n",
            "          43       0.32      0.30      0.31       112\n",
            "          44       0.37      0.39      0.38       120\n",
            "          45       0.21      0.26      0.23       120\n",
            "          46       0.24      0.32      0.27       120\n",
            "          47       0.30      0.31      0.30       120\n",
            "          48       0.28      0.28      0.28       120\n",
            "          49       0.28      0.31      0.29       120\n",
            "          50       0.27      0.28      0.28       120\n",
            "          51       0.26      0.17      0.20       120\n",
            "          52       0.39      0.22      0.28       120\n",
            "          53       0.26      0.21      0.23       120\n",
            "          54       0.32      0.38      0.35       120\n",
            "          55       0.28      0.28      0.28       120\n",
            "          56       0.22      0.30      0.26       120\n",
            "          57       0.28      0.26      0.27       120\n",
            "          58       0.17      0.26      0.20       120\n",
            "          59       0.32      0.24      0.28       120\n",
            "          60       0.20      0.13      0.16       120\n",
            "          61       0.21      0.31      0.25       120\n",
            "          62       0.20      0.25      0.22       120\n",
            "          63       0.31      0.18      0.23       120\n",
            "          64       0.34      0.40      0.37       112\n",
            "          65       0.37      0.37      0.37       120\n",
            "          66       0.36      0.48      0.41       120\n",
            "          67       0.24      0.23      0.24       120\n",
            "          68       0.37      0.32      0.34       120\n",
            "          69       0.21      0.20      0.21       120\n",
            "          70       0.34      0.19      0.24       113\n",
            "          71       0.31      0.23      0.26       120\n",
            "          72       0.25      0.25      0.25       120\n",
            "          73       0.31      0.42      0.36       120\n",
            "          74       0.28      0.26      0.27       120\n",
            "          75       0.36      0.42      0.39       120\n",
            "          76       0.38      0.33      0.35       120\n",
            "          77       0.34      0.31      0.32       120\n",
            "          78       0.37      0.38      0.37       120\n",
            "          79       0.44      0.30      0.36       120\n",
            "\n",
            "    accuracy                           0.30      9577\n",
            "   macro avg       0.30      0.30      0.29      9577\n",
            "weighted avg       0.30      0.30      0.29      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 37/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.66it/s, loss=2.1996, acc=37.64%]\n",
            "Epoch 37/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 252.89it/s, loss=2.0811, acc=29.56%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 37/500\n",
            "Train Loss: 2.3193, Train Acc: 37.64%\n",
            "Val Loss: 2.6388, Val Acc: 29.56%\n",
            "Learning rate: 0.000250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 38/500 [Train]: 100%|██████████| 2794/2794 [00:50<00:00, 55.82it/s, loss=3.2307, acc=38.05%]\n",
            "Epoch 38/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 254.13it/s, loss=2.9338, acc=29.55%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 38/500\n",
            "Train Loss: 2.2820, Train Acc: 38.05%\n",
            "Val Loss: 2.6479, Val Acc: 29.55%\n",
            "Learning rate: 0.000250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 39/500 [Train]: 100%|██████████| 2794/2794 [00:50<00:00, 55.74it/s, loss=3.4993, acc=38.83%]\n",
            "Epoch 39/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 253.02it/s, loss=2.8404, acc=29.80%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 39/500\n",
            "Train Loss: 2.2602, Train Acc: 38.83%\n",
            "Val Loss: 2.6189, Val Acc: 29.80%\n",
            "Learning rate: 0.000250\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.25      0.23      0.24       120\n",
            "           1       0.32      0.17      0.23       120\n",
            "           2       0.35      0.38      0.36       120\n",
            "           3       0.27      0.27      0.27       120\n",
            "           4       0.37      0.22      0.27       120\n",
            "           5       0.31      0.30      0.31       120\n",
            "           6       0.30      0.26      0.28       120\n",
            "           7       0.30      0.30      0.30       120\n",
            "           8       0.36      0.37      0.36       120\n",
            "           9       0.19      0.20      0.19       120\n",
            "          10       0.32      0.26      0.28       120\n",
            "          11       0.45      0.38      0.41       120\n",
            "          12       0.32      0.25      0.28       120\n",
            "          13       0.33      0.31      0.32       120\n",
            "          14       0.31      0.28      0.30       120\n",
            "          15       0.24      0.16      0.19       120\n",
            "          16       0.18      0.26      0.22       120\n",
            "          17       0.34      0.40      0.37       120\n",
            "          18       0.26      0.17      0.21       120\n",
            "          19       0.34      0.28      0.31       120\n",
            "          20       0.34      0.36      0.35       120\n",
            "          21       0.36      0.26      0.30       120\n",
            "          22       0.23      0.43      0.30       120\n",
            "          23       0.23      0.28      0.25       120\n",
            "          24       0.21      0.33      0.26       120\n",
            "          25       0.36      0.28      0.32       120\n",
            "          26       0.20      0.26      0.23       120\n",
            "          27       0.36      0.33      0.35       120\n",
            "          28       0.27      0.36      0.31       120\n",
            "          29       0.35      0.38      0.36       120\n",
            "          30       0.32      0.29      0.31       120\n",
            "          31       0.44      0.27      0.33       120\n",
            "          32       0.28      0.35      0.31       120\n",
            "          33       0.25      0.22      0.23       120\n",
            "          34       0.25      0.22      0.23       120\n",
            "          35       0.38      0.40      0.39       120\n",
            "          36       0.31      0.25      0.28       120\n",
            "          37       0.38      0.32      0.35       120\n",
            "          38       0.27      0.33      0.30       120\n",
            "          39       0.37      0.31      0.34       120\n",
            "          40       0.40      0.24      0.30       120\n",
            "          41       0.48      0.37      0.42       120\n",
            "          42       0.35      0.20      0.25       120\n",
            "          43       0.21      0.35      0.26       112\n",
            "          44       0.33      0.35      0.34       120\n",
            "          45       0.32      0.29      0.30       120\n",
            "          46       0.35      0.17      0.23       120\n",
            "          47       0.27      0.35      0.31       120\n",
            "          48       0.27      0.23      0.25       120\n",
            "          49       0.27      0.25      0.26       120\n",
            "          50       0.34      0.30      0.32       120\n",
            "          51       0.21      0.23      0.22       120\n",
            "          52       0.26      0.35      0.30       120\n",
            "          53       0.22      0.25      0.23       120\n",
            "          54       0.34      0.36      0.35       120\n",
            "          55       0.28      0.33      0.30       120\n",
            "          56       0.30      0.20      0.24       120\n",
            "          57       0.29      0.23      0.25       120\n",
            "          58       0.22      0.25      0.23       120\n",
            "          59       0.23      0.25      0.24       120\n",
            "          60       0.29      0.23      0.25       120\n",
            "          61       0.20      0.32      0.25       120\n",
            "          62       0.24      0.29      0.26       120\n",
            "          63       0.36      0.23      0.28       120\n",
            "          64       0.34      0.43      0.38       112\n",
            "          65       0.31      0.40      0.35       120\n",
            "          66       0.35      0.38      0.37       120\n",
            "          67       0.29      0.23      0.26       120\n",
            "          68       0.33      0.42      0.37       120\n",
            "          69       0.24      0.34      0.28       120\n",
            "          70       0.26      0.18      0.21       113\n",
            "          71       0.33      0.42      0.37       120\n",
            "          72       0.41      0.32      0.36       120\n",
            "          73       0.26      0.39      0.31       120\n",
            "          74       0.30      0.32      0.31       120\n",
            "          75       0.32      0.37      0.34       120\n",
            "          76       0.42      0.30      0.35       120\n",
            "          77       0.29      0.37      0.32       120\n",
            "          78       0.43      0.36      0.39       120\n",
            "          79       0.40      0.36      0.38       120\n",
            "\n",
            "    accuracy                           0.30      9577\n",
            "   macro avg       0.31      0.30      0.30      9577\n",
            "weighted avg       0.31      0.30      0.30      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 40/500 [Train]: 100%|██████████| 2794/2794 [00:50<00:00, 55.36it/s, loss=2.8471, acc=39.29%]\n",
            "Epoch 40/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 243.30it/s, loss=2.6457, acc=29.68%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 40/500\n",
            "Train Loss: 2.2316, Train Acc: 39.29%\n",
            "Val Loss: 2.6319, Val Acc: 29.68%\n",
            "Learning rate: 0.000250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 41/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.52it/s, loss=2.4965, acc=39.53%]\n",
            "Epoch 41/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 248.08it/s, loss=3.2374, acc=29.48%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 41/500\n",
            "Train Loss: 2.2294, Train Acc: 39.53%\n",
            "Val Loss: 2.6564, Val Acc: 29.48%\n",
            "Learning rate: 0.000250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 42/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 55.96it/s, loss=2.6048, acc=40.44%]\n",
            "Epoch 42/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 253.97it/s, loss=2.7186, acc=29.51%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 42/500\n",
            "Train Loss: 2.1932, Train Acc: 40.44%\n",
            "Val Loss: 2.6616, Val Acc: 29.51%\n",
            "Learning rate: 0.000250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 43/500 [Train]: 100%|██████████| 2794/2794 [00:50<00:00, 55.39it/s, loss=3.3755, acc=40.99%]\n",
            "Epoch 43/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 252.25it/s, loss=2.1829, acc=28.41%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 43/500\n",
            "Train Loss: 2.1764, Train Acc: 40.99%\n",
            "Val Loss: 2.6881, Val Acc: 28.41%\n",
            "Learning rate: 0.000250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 44/500 [Train]: 100%|██████████| 2794/2794 [00:50<00:00, 55.77it/s, loss=2.2184, acc=40.93%]\n",
            "Epoch 44/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 252.64it/s, loss=3.2302, acc=28.69%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 44/500\n",
            "Train Loss: 2.1800, Train Acc: 40.93%\n",
            "Val Loss: 2.7071, Val Acc: 28.69%\n",
            "Learning rate: 0.000250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 45/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.55it/s, loss=2.6037, acc=41.37%]\n",
            "Epoch 45/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 266.16it/s, loss=1.9871, acc=28.58%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 45/500\n",
            "Train Loss: 2.1540, Train Acc: 41.37%\n",
            "Val Loss: 2.6980, Val Acc: 28.58%\n",
            "Learning rate: 0.000125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 46/500 [Train]: 100%|██████████| 2794/2794 [00:51<00:00, 54.25it/s, loss=3.0389, acc=45.91%]\n",
            "Epoch 46/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 236.11it/s, loss=2.4886, acc=30.47%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 46/500\n",
            "Train Loss: 1.9675, Train Acc: 45.91%\n",
            "Val Loss: 2.6701, Val Acc: 30.47%\n",
            "Learning rate: 0.000125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 47/500 [Train]: 100%|██████████| 2794/2794 [00:50<00:00, 55.70it/s, loss=1.5663, acc=47.87%]\n",
            "Epoch 47/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 262.34it/s, loss=2.4349, acc=30.45%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 47/500\n",
            "Train Loss: 1.8849, Train Acc: 47.87%\n",
            "Val Loss: 2.6640, Val Acc: 30.45%\n",
            "Learning rate: 0.000125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 48/500 [Train]: 100%|██████████| 2794/2794 [00:50<00:00, 55.54it/s, loss=3.0441, acc=48.97%]\n",
            "Epoch 48/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 255.48it/s, loss=2.2935, acc=30.20%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 48/500\n",
            "Train Loss: 1.8308, Train Acc: 48.97%\n",
            "Val Loss: 2.6922, Val Acc: 30.20%\n",
            "Learning rate: 0.000125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 49/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.40it/s, loss=2.5115, acc=49.89%]\n",
            "Epoch 49/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 248.91it/s, loss=3.1359, acc=30.02%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 49/500\n",
            "Train Loss: 1.8006, Train Acc: 49.89%\n",
            "Val Loss: 2.7162, Val Acc: 30.02%\n",
            "Learning rate: 0.000125\n",
            "Early stopping triggered after 49 epochs\n",
            "\n",
            "Evaluating model on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 599/599 [00:01<00:00, 545.95it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Accuracy: 29.19%\n",
            "\n",
            "Model saved successfully!\n",
            "\n",
            "EEG classification pipeline complete!\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import scipy.signal as signal\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "from scipy.stats import skew, kurtosis\n",
        "import seaborn as sns\n",
        "import gc\n",
        "\n",
        "# STEP 1: Data Loading and Preprocessing\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, data_dict, label_mapping_file=None, transform=None):\n",
        "        self.dataset = data_dict['dataset']\n",
        "        self.labels_list = data_dict['labels']  # List of label IDs\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load the mapping from label ID to text if provided\n",
        "        self.label_id_to_text = {}\n",
        "        if label_mapping_file and os.path.exists(label_mapping_file):\n",
        "            with open(label_mapping_file, 'r') as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) >= 2:\n",
        "                        label_id = parts[0]\n",
        "                        label_text = ' '.join(parts[1:])\n",
        "                        self.label_id_to_text[label_id] = label_text\n",
        "\n",
        "        # Create mapping from label ID to index\n",
        "        unique = sorted(set(self.labels_list))\n",
        "        self.label_to_idx = {lbl:i for i,lbl in enumerate(unique)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.dataset[idx]\n",
        "        eeg_data = sample['eeg_data']  # Shape: [channels, time_points]\n",
        "        label_id = sample['label']  # This is a string ID like 'n02510455'\n",
        "\n",
        "        # Get label text if available\n",
        "        label_text = self.label_id_to_text.get(label_id, label_id)\n",
        "\n",
        "        # Convert string label to numerical index\n",
        "        label_idx = self.label_to_idx.get(label_id, 0)\n",
        "\n",
        "        if self.transform:\n",
        "            eeg_data = self.transform(eeg_data)\n",
        "\n",
        "        return eeg_data, label_idx, label_text\n",
        "\n",
        "# EEG Signal Preprocessing\n",
        "class EEGPreprocessor:\n",
        "    def __init__(self, sampling_rate=1000, notch_freq=50, bandpass_low=0.5, bandpass_high=70):\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.notch_freq = notch_freq\n",
        "        self.bandpass_low = bandpass_low\n",
        "        self.bandpass_high = bandpass_high\n",
        "\n",
        "        # Pre-compute filter coefficients to avoid recomputation\n",
        "        self.sos = signal.butter(\n",
        "            N=4,\n",
        "            Wn=[self.bandpass_low, self.bandpass_high],\n",
        "            btype='bandpass',\n",
        "            fs=self.sampling_rate,\n",
        "            output='sos'\n",
        "        )\n",
        "        self.b_notch, self.a_notch = signal.iirnotch(self.notch_freq, 30, self.sampling_rate)\n",
        "\n",
        "    def __call__(self, eeg_data):\n",
        "        # Convert to numpy if it's a tensor\n",
        "        if isinstance(eeg_data, torch.Tensor):\n",
        "            eeg_data = eeg_data.numpy()\n",
        "\n",
        "        # Transpose to [time, channels] for easier processing\n",
        "        eeg_data = eeg_data.T\n",
        "\n",
        "        # Apply bandpass filter\n",
        "        eeg_filtered = self._bandpass_filter(eeg_data)\n",
        "\n",
        "        # Apply notch filter (to remove power line interference)\n",
        "        eeg_filtered = self._notch_filter(eeg_filtered)\n",
        "\n",
        "        # Re-reference to common average\n",
        "        eeg_filtered = self._common_average_reference(eeg_filtered)\n",
        "\n",
        "        # Z-score normalization\n",
        "        eeg_normalized = self._normalize(eeg_filtered)\n",
        "\n",
        "        # Transpose back to [channels, time]\n",
        "        return torch.tensor(eeg_normalized.T, dtype=torch.float32)\n",
        "\n",
        "    def _bandpass_filter(self, data):\n",
        "        # Apply forward-backward filtering for zero phase distortion\n",
        "        return signal.sosfiltfilt(self.sos, data, axis=0)\n",
        "\n",
        "    def _notch_filter(self, data):\n",
        "        return signal.filtfilt(self.b_notch, self.a_notch, data, axis=0)\n",
        "\n",
        "    def _common_average_reference(self, data):\n",
        "        # Subtract the mean across all channels at each time point\n",
        "        return data - np.mean(data, axis=1, keepdims=True)\n",
        "\n",
        "    def _normalize(self, data):\n",
        "        # Z-score normalization for each channel\n",
        "        return (data - np.mean(data, axis=0)) / (np.std(data, axis=0) + 1e-10)\n",
        "\n",
        "# STEP 2: Feature Extraction\n",
        "class FeatureExtractor:\n",
        "    def __init__(self, sampling_rate=1000):\n",
        "        self.sampling_rate = sampling_rate\n",
        "\n",
        "        # Define frequency bands\n",
        "        self.freq_bands = {\n",
        "          'delta': (1, 4),       # Adjusted lower bound to reduce DC components\n",
        "          'theta': (4, 8),       # Standard theta for cognitive processing\n",
        "          'alpha_low': (8, 10),  # Lower alpha - attention/inhibition\n",
        "          'alpha_high': (10, 13),# Higher alpha - semantic processing\n",
        "          'beta_low': (13, 20),  # Lower beta - motor preparation\n",
        "          'beta_high': (20, 30), # Higher beta - active processing/cognition\n",
        "          'gamma_low': (30, 60), # Expanded gamma_low for visual processing\n",
        "          'gamma_mid': (60, 90), # Added gamma_mid for binding\n",
        "          'gamma_high': (90, 120)# Higher gamma for fine perceptual binding\n",
        "      }\n",
        "\n",
        "    def extract_features(self, eeg_data):\n",
        "        \"\"\"\n",
        "        Extract time and frequency domain features from EEG data\n",
        "\n",
        "        Args:\n",
        "            eeg_data: EEG data of shape [channels, time_points]\n",
        "\n",
        "        Returns:\n",
        "            features: Dictionary of extracted features\n",
        "        \"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Time domain features\n",
        "        features.update(self._extract_time_domain_features(eeg_data))\n",
        "\n",
        "        # Frequency domain features\n",
        "        features.update(self._extract_frequency_domain_features(eeg_data))\n",
        "\n",
        "        # Connectivity features - can improve classification accuracy\n",
        "        features.update(self._extract_connectivity_features(eeg_data))\n",
        "\n",
        "        # Convert dictionary to vector\n",
        "        feature_vector = []\n",
        "        for key, value in features.items():\n",
        "            if isinstance(value, np.ndarray):\n",
        "                feature_vector.append(value.flatten())\n",
        "            else:\n",
        "                feature_vector.append(np.array([value]).flatten())\n",
        "\n",
        "        return np.concatenate(feature_vector)\n",
        "\n",
        "    def _extract_time_domain_features(self, eeg_data):\n",
        "        features = {}\n",
        "\n",
        "        # Statistical features\n",
        "        features['mean'] = np.mean(eeg_data, axis=1)\n",
        "        features['var'] = np.var(eeg_data, axis=1)\n",
        "        features['skewness'] = skew(eeg_data, axis=1)\n",
        "        features['kurtosis'] = kurtosis(eeg_data, axis=1)\n",
        "        features['max'] = np.max(eeg_data, axis=1)\n",
        "        features['min'] = np.min(eeg_data, axis=1)\n",
        "        features['peak_to_peak'] = features['max'] - features['min']  # Reuse computed values\n",
        "        features['rms'] = np.sqrt(np.mean(np.square(eeg_data), axis=1))\n",
        "        features['zero_crossings'] = np.sum(np.diff(np.signbit(eeg_data), axis=1), axis=1)\n",
        "\n",
        "        # Hjorth parameters\n",
        "        features.update(self._compute_hjorth_parameters(eeg_data))\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _compute_hjorth_parameters(self, eeg_data):\n",
        "        \"\"\"Compute Hjorth parameters: Activity, Mobility, and Complexity\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # First derivative\n",
        "        diff1 = np.diff(eeg_data, axis=1)\n",
        "        # Second derivative\n",
        "        diff2 = np.diff(diff1, axis=1)\n",
        "\n",
        "        # Activity: variance of the signal\n",
        "        features['activity'] = np.var(eeg_data, axis=1)\n",
        "\n",
        "        # Mobility: sqrt(variance of first derivative / variance of signal)\n",
        "        var_diff1 = np.var(diff1, axis=1)\n",
        "        mobility1 = np.sqrt(var_diff1 / (features['activity'] + 1e-10))\n",
        "        features['mobility'] = mobility1\n",
        "\n",
        "        # Complexity: mobility of first derivative / mobility of signal\n",
        "        var_diff2 = np.var(diff2, axis=1)\n",
        "        mobility2 = np.sqrt(var_diff2 / (var_diff1 + 1e-10))\n",
        "        features['complexity'] = mobility2 / (mobility1 + 1e-10)\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _extract_frequency_domain_features(self, eeg_data):\n",
        "        features = {}\n",
        "\n",
        "        # Compute power spectral density with Welch's method\n",
        "        nperseg = min(256, eeg_data.shape[1] // 4)  # Adaptive window size\n",
        "        freqs, psd = signal.welch(eeg_data, fs=self.sampling_rate,\n",
        "                                 nperseg=nperseg,\n",
        "                                 noverlap=nperseg // 2,\n",
        "                                 axis=1)\n",
        "\n",
        "        # Calculate total power once\n",
        "        total_power = np.sum(psd, axis=1) + 1e-10\n",
        "\n",
        "        # Band powers and their ratios\n",
        "        for band_name, (low_freq, high_freq) in self.freq_bands.items():\n",
        "            # Find frequencies in the band\n",
        "            idx_band = np.logical_and(freqs >= low_freq, freqs <= high_freq)\n",
        "            # Calculate band power\n",
        "            band_power = np.sum(psd[:, idx_band], axis=1)\n",
        "            features[f'{band_name}_power'] = band_power\n",
        "\n",
        "            # Calculate relative band power\n",
        "            features[f'{band_name}_rel_power'] = band_power / total_power\n",
        "\n",
        "        # Spectral edge frequency (95%)\n",
        "        features['sef_95'] = self._compute_spectral_edge_frequency(freqs, psd, 0.95)\n",
        "\n",
        "        # Spectral entropy\n",
        "        features['spectral_entropy'] = self._compute_spectral_entropy(psd)\n",
        "\n",
        "        # Spectral peak frequency and power\n",
        "        peak_freqs = freqs[np.argmax(psd, axis=1)]\n",
        "        peak_powers = np.max(psd, axis=1)\n",
        "        features['peak_freq'] = peak_freqs\n",
        "        features['peak_power'] = peak_powers\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _compute_spectral_edge_frequency(self, freqs, psd, edge=0.95):\n",
        "        \"\"\"Compute frequency below which edge% of power resides\"\"\"\n",
        "        sef = np.zeros(psd.shape[0])\n",
        "        for i in range(psd.shape[0]):\n",
        "            # Cumulative sum of PSD\n",
        "            cumsum = np.cumsum(psd[i]) / (np.sum(psd[i]) + 1e-10)\n",
        "            # Find frequency below which edge% of power resides\n",
        "            idx = np.where(cumsum >= edge)[0]\n",
        "            if len(idx) > 0:\n",
        "                sef[i] = freqs[idx[0]]\n",
        "            else:\n",
        "                sef[i] = freqs[-1]\n",
        "        return sef\n",
        "\n",
        "    def _compute_spectral_entropy(self, psd):\n",
        "        \"\"\"Compute spectral entropy\"\"\"\n",
        "        entropy = np.zeros(psd.shape[0])\n",
        "        for i in range(psd.shape[0]):\n",
        "            # Normalize PSD\n",
        "            psd_norm = psd[i] / (np.sum(psd[i]) + 1e-10)\n",
        "            # Calculate entropy\n",
        "            entropy[i] = -np.sum(psd_norm * np.log2(psd_norm + 1e-10))\n",
        "        return entropy\n",
        "\n",
        "    def _extract_connectivity_features(self, eeg_data):\n",
        "        \"\"\"Extract connectivity features between EEG channels\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Number of channels\n",
        "        n_channels = eeg_data.shape[0]\n",
        "\n",
        "        # Calculate correlation matrix\n",
        "        corr_matrix = np.corrcoef(eeg_data)\n",
        "\n",
        "        # Extract upper triangle (excluding diagonal)\n",
        "        upper_tri_idx = np.triu_indices(n_channels, k=1)\n",
        "        correlations = corr_matrix[upper_tri_idx]\n",
        "\n",
        "        # Basic statistics of correlations\n",
        "        features['mean_corr'] = np.mean(correlations)\n",
        "        features['std_corr'] = np.std(correlations)\n",
        "        features['max_corr'] = np.max(correlations)\n",
        "        features['min_corr'] = np.min(correlations)\n",
        "\n",
        "        # Phase synchronization - simplified version using Hilbert transform\n",
        "        analytic_signal = signal.hilbert(eeg_data, axis=1)\n",
        "        instantaneous_phase = np.angle(analytic_signal)\n",
        "\n",
        "        # Calculate phase differences between adjacent channels\n",
        "        phase_diff = np.zeros((n_channels-1,) + instantaneous_phase.shape[1:])\n",
        "        for i in range(n_channels-1):\n",
        "            phase_diff[i] = instantaneous_phase[i+1] - instantaneous_phase[i]\n",
        "\n",
        "        # Phase locking value (PLV)\n",
        "        plv_values = np.abs(np.mean(np.exp(1j * phase_diff), axis=1))\n",
        "        features['mean_plv'] = np.mean(plv_values)\n",
        "        features['std_plv'] = np.std(plv_values)\n",
        "\n",
        "        return features\n",
        "\n",
        "# Dataset with precomputed features\n",
        "class EEGFeatureDataset(Dataset):\n",
        "    def __init__(self, features, labels, texts):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.texts = texts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = self.features[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Get text from dictionary if texts is a dictionary\n",
        "        if isinstance(self.texts, dict):\n",
        "            text = self.texts.get(label, f\"Unknown-{label}\")\n",
        "        else:\n",
        "            text = self.texts[idx] if idx < len(self.texts) else f\"Unknown-{label}\"\n",
        "\n",
        "        # Convert to tensors if not already\n",
        "        if not isinstance(feature, torch.Tensor):\n",
        "            feature = torch.tensor(feature, dtype=torch.float32)\n",
        "        if not isinstance(label, torch.Tensor) and not isinstance(label, int):\n",
        "            label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return feature, label, text\n",
        "\n",
        "# STEP 3: Classification Model\n",
        "class EEGClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, n_classes, hidden_dims=[4096, 2048, 1024],\n",
        "                 seq_length=None, n_channels=None, dropout_rate=0.5):\n",
        "        super(EEGClassifier, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.n_classes = n_classes\n",
        "        self.seq_length = seq_length\n",
        "        self.n_channels = n_channels\n",
        "\n",
        "        # Option to reshape as temporal sequence if seq_length and n_channels are provided\n",
        "        self.reshape_input = seq_length is not None and n_channels is not None\n",
        "\n",
        "        # Input normalization layer\n",
        "        self.input_norm = nn.BatchNorm1d(input_dim)\n",
        "\n",
        "        # PART 1: CNN FEATURE EXTRACTION (if seq_length and n_channels provided)\n",
        "        if self.reshape_input:\n",
        "            # CNN for spatial-temporal feature extraction\n",
        "            self.conv_block = nn.Sequential(\n",
        "                nn.Conv2d(1, 32, kernel_size=(1, 16), stride=(1, 2), padding=(0, 7)),\n",
        "                nn.BatchNorm2d(32),\n",
        "                nn.ELU(),\n",
        "                nn.Conv2d(32, 64, kernel_size=(n_channels, 1), stride=1, padding=0),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.ELU(),\n",
        "                nn.AvgPool2d(kernel_size=(1, 4), stride=(1, 4)),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            )\n",
        "\n",
        "            # Calculate output size after convolutions\n",
        "            conv_output_size = self._calculate_conv_output_size()\n",
        "            lstm_input_size = conv_output_size\n",
        "\n",
        "            # LSTM for temporal dynamics\n",
        "            self.lstm = nn.LSTM(\n",
        "                input_size=64,  # Number of features per timestep (output channels from CNN)\n",
        "                hidden_size=128,\n",
        "                num_layers=2,\n",
        "                batch_first=True,\n",
        "                dropout=dropout_rate,\n",
        "                bidirectional=True\n",
        "            )\n",
        "\n",
        "            # Self-attention mechanism for temporal focus\n",
        "            self.attention = SelfAttention(256)  # 256 = 128*2 (bidirectional)\n",
        "\n",
        "            # Set the input dimension for dense layers\n",
        "            dense_input_dim = 256\n",
        "        else:\n",
        "            # If no reshape, use attention on flat input\n",
        "            self.attention = nn.Sequential(\n",
        "                nn.Linear(input_dim, input_dim // 4),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.Linear(input_dim // 4, input_dim),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "            dense_input_dim = input_dim\n",
        "\n",
        "        # PART 2: DENSE NETWORK PATHWAY\n",
        "        layers = []\n",
        "        prev_dim = dense_input_dim\n",
        "\n",
        "        for i, hidden_dim in enumerate(hidden_dims):\n",
        "            # Dense block with residual connection if dimensions match\n",
        "            if prev_dim == hidden_dim:\n",
        "                layers.append(ResidualBlock(prev_dim, hidden_dim, dropout_rate))\n",
        "            else:\n",
        "                layers.append(DenseBlock(prev_dim, hidden_dim, dropout_rate))\n",
        "\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "            # Add Squeeze-and-Excitation blocks for feature recalibration\n",
        "            if i < len(hidden_dims) - 1:  # Not for the last layer\n",
        "                layers.append(SEBlock(hidden_dim))\n",
        "\n",
        "        self.feature_layers = nn.Sequential(*layers)\n",
        "\n",
        "        # Multi-head output with ensemble averaging\n",
        "        self.heads = nn.ModuleList([\n",
        "            nn.Linear(prev_dim, n_classes) for _ in range(3)\n",
        "        ])\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _calculate_conv_output_size(self):\n",
        "        # Calculate output size after convolutions\n",
        "        # This is a placeholder - actual calculation depends on your exact architecture\n",
        "        length_after_conv = ((self.seq_length - 16 + 2*7) // 2) + 1\n",
        "        length_after_pool = length_after_conv // 4\n",
        "        return 64 * length_after_pool  # 64 channels\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
        "            nn.init.constant_(m.weight, 1)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply input normalization\n",
        "        if not self.reshape_input:\n",
        "            x = self.input_norm(x)\n",
        "\n",
        "            # Apply attention mechanism\n",
        "            attn = self.attention(x)\n",
        "            x = x * attn\n",
        "\n",
        "            # Pass through feature layers\n",
        "            features = self.feature_layers(x)\n",
        "        else:\n",
        "            # Reshape input to [batch, 1, channels, time]\n",
        "            batch_size = x.size(0)\n",
        "            x = x.view(batch_size, 1, self.n_channels, self.seq_length)\n",
        "\n",
        "            # Pass through CNN\n",
        "            x = self.conv_block(x)  # -> [batch, 64, 1, reduced_time]\n",
        "\n",
        "            # Reshape for LSTM: [batch, time, features]\n",
        "            x = x.squeeze(2).permute(0, 2, 1)  # -> [batch, reduced_time, 64]\n",
        "\n",
        "            # Pass through LSTM\n",
        "            x, _ = self.lstm(x)  # -> [batch, reduced_time, 256]\n",
        "\n",
        "            # Apply self-attention\n",
        "            x, _ = self.attention(x)  # -> [batch, 256]\n",
        "\n",
        "            # Pass through feature layers\n",
        "            features = self.feature_layers(x)\n",
        "\n",
        "        # Ensemble predictions from multiple heads\n",
        "        logits = torch.stack([head(features) for head in self.heads])\n",
        "        logits = torch.mean(logits, dim=0)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def predict_proba(self, x):\n",
        "        logits = self.forward(x)\n",
        "        return torch.softmax(logits, dim=1)\n",
        "\n",
        "\n",
        "# Helper blocks for enhanced architecture\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, dropout_rate=0.4):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.linear = nn.Linear(in_dim, out_dim)\n",
        "        self.norm = nn.BatchNorm1d(out_dim)\n",
        "        self.activation = nn.LeakyReLU(0.2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.linear(x)\n",
        "        out = self.norm(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "        return out\n",
        "\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, dropout_rate=0.4):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        self.linear = nn.Linear(in_dim, out_dim)\n",
        "        self.norm = nn.BatchNorm1d(out_dim)\n",
        "        self.activation = nn.LeakyReLU(0.2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        out = self.norm(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    \"\"\"Squeeze-and-Excitation block for feature recalibration\"\"\"\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c = x.size()\n",
        "        y = self.avg_pool(x.unsqueeze(-1)).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1)\n",
        "        return x * y.squeeze(-1)\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"Self-attention mechanism for sequential data\"\"\"\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.scale = hidden_dim ** 0.5\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch, seq_len, hidden_dim]\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        attn_scores = torch.bmm(q, k.transpose(1, 2)) / self.scale\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        context = torch.bmm(attn_probs, v)\n",
        "\n",
        "        # Global feature vector (attention-weighted sum)\n",
        "        global_feat = torch.sum(context, dim=1)\n",
        "\n",
        "        return global_feat, attn_probs\n",
        "\n",
        "# STEP 4: Data Preprocessing Helper - OPTIMIZED FOR MEMORY\n",
        "# STEP 4: Data Preprocessing Helper - OPTIMIZED FOR MEMORY\n",
        "def prepare_dataset_with_features(dataset, batch_size=64, device='cuda'):\n",
        "    \"\"\"Pre-compute features for the dataset in memory-efficient batches\"\"\"\n",
        "    feature_extractor = FeatureExtractor()\n",
        "\n",
        "    processed_features = []\n",
        "    labels = []\n",
        "    label_texts = {}\n",
        "\n",
        "    # Process in batches to reduce memory usage\n",
        "    num_batches = (len(dataset) + batch_size - 1) // batch_size\n",
        "\n",
        "    for batch_idx in tqdm(range(num_batches), desc=\"Extracting features in batches\"):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min(start_idx + batch_size, len(dataset))\n",
        "\n",
        "        batch_features = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for idx in range(start_idx, end_idx):\n",
        "            eeg_data, label_idx, label_text = dataset[idx]\n",
        "\n",
        "            # Convert to numpy if needed\n",
        "            if isinstance(eeg_data, torch.Tensor):\n",
        "                eeg_data_np = eeg_data.cpu().numpy()\n",
        "            else:\n",
        "                eeg_data_np = eeg_data\n",
        "\n",
        "            # Extract features\n",
        "            features = feature_extractor.extract_features(eeg_data_np)\n",
        "\n",
        "            batch_features.append(torch.tensor(features, dtype=torch.float32))\n",
        "            batch_labels.append(label_idx)\n",
        "\n",
        "            # Store label text mapping\n",
        "            label_texts[label_idx] = label_text\n",
        "\n",
        "        processed_features.extend(batch_features)\n",
        "        labels.extend(batch_labels)\n",
        "\n",
        "        # Force garbage collection after each batch\n",
        "        gc.collect()\n",
        "\n",
        "        # Clear CUDA cache if using GPU\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Get label texts from dataset if available\n",
        "    if not label_texts and hasattr(dataset, 'get_label_texts'):\n",
        "        label_texts = dataset.get_label_texts()\n",
        "\n",
        "    return processed_features, labels, label_texts\n",
        "\n",
        "# STEP 5: Training Function - MEMORY OPTIMIZED\n",
        "def train_model(model, train_loader, val_loader, num_epochs=500, learning_rate=0.001,\n",
        "               weight_decay=1e-5, device=\"cpu\", class_weights=None):\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialize optimizer with weight decay for regularization\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "\n",
        "    # Loss function with class weights if provided\n",
        "    if class_weights is not None:\n",
        "        class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Track metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    # For early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "    no_improve_epoch = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "\n",
        "        for features, labels, _ in progress_bar:\n",
        "            # Move tensors to device\n",
        "            features = features.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f\"{loss.item():.4f}\",\n",
        "                'acc': f\"{100 * correct / total:.2f}%\"\n",
        "            })\n",
        "\n",
        "            # Clear GPU memory after each batch\n",
        "            del features, labels, outputs, loss, predicted\n",
        "            if device == 'cuda' and torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        epoch_train_loss = train_loss / len(train_loader)\n",
        "        epoch_train_acc = 100 * correct / total\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accs.append(epoch_train_acc)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
        "\n",
        "            for features, labels, _ in progress_bar:\n",
        "                features = features.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Calculate accuracy\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Store predictions for metrics\n",
        "                all_preds.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "                # Update progress bar\n",
        "                progress_bar.set_postfix({\n",
        "                    'loss': f\"{loss.item():.4f}\",\n",
        "                    'acc': f\"{100 * correct / total:.2f}%\"\n",
        "                })\n",
        "\n",
        "                # Clear GPU memory\n",
        "                del features, labels, outputs, loss, predicted\n",
        "                if device == 'cuda' and torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        epoch_val_loss = val_loss / len(val_loader)\n",
        "        epoch_val_acc = 100 * correct / total\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accs.append(epoch_val_acc)\n",
        "\n",
        "        # Learning rate scheduler step\n",
        "        scheduler.step(epoch_val_loss)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%\")\n",
        "        print(f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%\")\n",
        "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        # Check if this is the best model\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            best_model_state = {k: v.cpu().detach() for k, v in model.state_dict().items()}\n",
        "            no_improve_epoch = 0\n",
        "            print(\"New best model saved!\")\n",
        "\n",
        "            # Print classification report\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(classification_report(all_labels, all_preds))\n",
        "        else:\n",
        "            no_improve_epoch += 1\n",
        "\n",
        "        # Early stopping check\n",
        "        if no_improve_epoch >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "        # Force garbage collection after each epoch\n",
        "        gc.collect()\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_state)\n",
        "    model.to(device)  # Make sure model is on the correct device\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Train Accuracy')\n",
        "    plt.plot(val_accs, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_curves.png')\n",
        "    plt.close()\n",
        "\n",
        "    return model\n",
        "\n",
        "# STEP 6: Evaluation Function - MEMORY OPTIMIZED\n",
        "def evaluate_model(model, test_loader, device='cpu'):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, labels, _ in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            features = features.to(device)\n",
        "\n",
        "            # Get predictions\n",
        "            outputs = model(features)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Store results (move to CPU to save GPU memory)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "            # Clear GPU memory\n",
        "            del features, outputs, probs, predicted\n",
        "            if device == 'cuda' and torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(all_labels, all_preds)\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    plt.figure(figsize=(10, 8))\n",
        "    sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    plt.xlabel('Predicted Label')\n",
        "    plt.ylabel('True Label')\n",
        "    plt.title('Confusion Matrix')\n",
        "    plt.savefig('confusion_matrix.png')\n",
        "    plt.close()\n",
        "\n",
        "    return accuracy, report, cm, np.array(all_probs)\n",
        "\n",
        "def main():\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(42)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Check device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Enable memory tracking for PyTorch\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        print(f\"Initial GPU memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "\n",
        "    # Create or use existing label mapping\n",
        "    label_mapping_file = 'labels_txt.txt'\n",
        "\n",
        "    # Set up paths for feature storage\n",
        "    features_path = {\n",
        "        'session_1': \"features_session_1.pt\",\n",
        "        'session_2': \"features_session_2.pt\",\n",
        "        'combined': \"features_combined.pt\"\n",
        "    }\n",
        "\n",
        "    # Track whether we need to recompute the combined features\n",
        "    need_to_combine = False\n",
        "\n",
        "    # Process both sessions sequentially\n",
        "    session_files = [\n",
        "        \"/content/drive/MyDrive/session_1.pth\",\n",
        "        \"/content/drive/MyDrive/session_2.pth\"\n",
        "    ]\n",
        "\n",
        "    # Process each session separately\n",
        "    for i, session_file in enumerate(session_files):\n",
        "        session_name = f\"session_{i+1}\"\n",
        "        session_feature_path = features_path[session_name]\n",
        "\n",
        "        # Check if features for this session already exist\n",
        "        if os.path.exists(session_feature_path):\n",
        "            print(f\"Loading pre-computed features from {session_feature_path}...\")\n",
        "            features_data = torch.load(session_feature_path)\n",
        "            if 'features' not in features_data or 'labels' not in features_data or 'texts' not in features_data:\n",
        "                print(f\"Invalid feature file format for {session_feature_path}. Will recompute.\")\n",
        "                need_to_combine = True\n",
        "                os.remove(session_feature_path)\n",
        "            else:\n",
        "                print(f\"Loaded features with shape {len(features_data['features'])}\")\n",
        "        else:\n",
        "            # If feature file doesn't exist, we need to process and later combine\n",
        "            need_to_combine = True\n",
        "\n",
        "            # Load data for this session\n",
        "            print(f\"Loading data from {session_file}...\")\n",
        "            try:\n",
        "                # Load in CPU memory to avoid GPU memory usage during loading\n",
        "                session_data = torch.load(session_file, map_location='cpu', weights_only=False)\n",
        "                print(f\"Loaded {len(session_data['dataset'])} EEG samples with {len(set(session_data['labels']))} unique classes\")\n",
        "\n",
        "                # Create EEG dataset with preprocessing\n",
        "                preprocessor = EEGPreprocessor(sampling_rate=256)  # Adjust sampling rate to match your data\n",
        "                dataset = EEGDataset(session_data, label_mapping_file, transform=preprocessor)\n",
        "\n",
        "                # Free up memory from raw data\n",
        "                del session_data\n",
        "                gc.collect()\n",
        "\n",
        "                if device.type == 'cuda':\n",
        "                    torch.cuda.empty_cache()\n",
        "                    print(f\"GPU memory after data loading: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "\n",
        "                # Extract features from the dataset in batches\n",
        "                print(f\"Preprocessing and extracting features for {session_name} in batches...\")\n",
        "                features, labels, texts = prepare_dataset_with_features(dataset, batch_size=32, device=device)\n",
        "\n",
        "                # Save features to disk\n",
        "                print(f\"Saving extracted features to {session_feature_path}...\")\n",
        "                torch.save({\n",
        "                    'features': features,\n",
        "                    'labels': labels,\n",
        "                    'texts': texts\n",
        "                }, session_feature_path)\n",
        "                print(\"Features saved successfully!\")\n",
        "\n",
        "                # Free up memory\n",
        "                del dataset, features, labels, texts\n",
        "                gc.collect()\n",
        "\n",
        "                if device.type == 'cuda':\n",
        "                    torch.cuda.empty_cache()\n",
        "                    print(f\"GPU memory after feature extraction: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {session_file}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Check if we need to recombine features\n",
        "    if need_to_combine or not os.path.exists(features_path['combined']):\n",
        "        print(\"Combining features from all sessions...\")\n",
        "        all_features = []\n",
        "        all_labels = []\n",
        "        all_texts = {}\n",
        "\n",
        "        # Load and combine features from each session\n",
        "        for i in range(len(session_files)):\n",
        "            session_name = f\"session_{i+1}\"\n",
        "            session_feature_path = features_path[session_name]\n",
        "\n",
        "            if os.path.exists(session_feature_path):\n",
        "                print(f\"Loading features from {session_feature_path} for combining...\")\n",
        "                features_data = torch.load(session_feature_path)\n",
        "\n",
        "                # Add features and labels\n",
        "                all_features.extend(features_data['features'])\n",
        "                all_labels.extend(features_data['labels'])\n",
        "\n",
        "                # Merge text dictionaries\n",
        "                if isinstance(features_data['texts'], dict):\n",
        "                    all_texts.update(features_data['texts'])\n",
        "                else:\n",
        "                    # Handle case where texts are in list format\n",
        "                    for j, label in enumerate(features_data['labels']):\n",
        "                        if j < len(features_data['texts']):\n",
        "                            all_texts[label] = features_data['texts'][j]\n",
        "\n",
        "                # Clear memory\n",
        "                del features_data\n",
        "                gc.collect()\n",
        "\n",
        "                if device.type == 'cuda':\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        # Save combined features\n",
        "        print(f\"Total combined features: {len(all_features)}\")\n",
        "        print(f\"Saving combined features to {features_path['combined']}...\")\n",
        "        torch.save({\n",
        "            'features': all_features,\n",
        "            'labels': all_labels,\n",
        "            'texts': all_texts\n",
        "        }, features_path['combined'])\n",
        "        print(\"Combined features saved successfully!\")\n",
        "    else:\n",
        "        # Load combined features\n",
        "        print(f\"Loading pre-computed combined features from {features_path['combined']}...\")\n",
        "        combined_data = torch.load(features_path['combined'])\n",
        "        all_features = combined_data['features']\n",
        "        all_labels = combined_data['labels']\n",
        "        all_texts = combined_data['texts']\n",
        "        print(f\"Loaded combined features with shape {len(all_features)}\")\n",
        "\n",
        "        # Clear memory\n",
        "        del combined_data\n",
        "        gc.collect()\n",
        "\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Create feature dataset from combined data\n",
        "    feature_dataset = EEGFeatureDataset(all_features, all_labels, all_texts)\n",
        "\n",
        "    # Free up memory that's no longer needed\n",
        "    del all_features, all_labels\n",
        "    gc.collect()\n",
        "\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"GPU memory before training: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "\n",
        "    # Split data with stratification\n",
        "    train_idx, temp_idx = train_test_split(\n",
        "        range(len(feature_dataset)),\n",
        "        test_size=0.3,\n",
        "        random_state=42,\n",
        "        stratify=feature_dataset.labels\n",
        "    )\n",
        "\n",
        "    val_idx, test_idx = train_test_split(\n",
        "        temp_idx,\n",
        "        test_size=0.5,  # 50% of temp_idx, resulting in 15% of original data\n",
        "        random_state=42,\n",
        "        stratify=[feature_dataset.labels[i] for i in temp_idx]\n",
        "    )\n",
        "\n",
        "    # Create samplers\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "    test_sampler = SubsetRandomSampler(test_idx)\n",
        "\n",
        "    # Create data loaders with appropriate batch size\n",
        "    # Smaller batch size can help with memory usage\n",
        "    batch_size = 16 if device.type == 'cuda' else 32\n",
        "    train_loader = DataLoader(feature_dataset, batch_size=batch_size, sampler=train_sampler, pin_memory=(device.type=='cuda'))\n",
        "    val_loader = DataLoader(feature_dataset, batch_size=batch_size, sampler=val_sampler, pin_memory=(device.type=='cuda'))\n",
        "    test_loader = DataLoader(feature_dataset, batch_size=batch_size, sampler=test_sampler, pin_memory=(device.type=='cuda'))\n",
        "\n",
        "    # Get feature dimension from the first sample\n",
        "    feature_dim = feature_dataset[0][0].shape[0]\n",
        "    n_classes = len(set(feature_dataset.labels))\n",
        "\n",
        "    print(f\"Feature dimension: {feature_dim}\")\n",
        "    print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "    # Calculate class weights for imbalanced data\n",
        "    class_counts = np.bincount(feature_dataset.labels)\n",
        "    class_weights = 1.0 / class_counts\n",
        "    class_weights = class_weights / np.sum(class_weights) * len(class_counts)\n",
        "    print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "    # Create model\n",
        "    model = EEGClassifier(\n",
        "        input_dim=feature_dim,\n",
        "        n_classes=n_classes\n",
        "        # hidden_dims=[512, 256, 128]  # Adjust architecture as needed\n",
        "    )\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total model parameters: {total_params:,}\")\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nTraining model...\")\n",
        "    trained_model = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        num_epochs=500,  # Adjust as needed\n",
        "        learning_rate=0.0005,\n",
        "        weight_decay=5e-5,\n",
        "        device=device,\n",
        "        class_weights=class_weights\n",
        "    )\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"\\nEvaluating model on test set...\")\n",
        "    accuracy, report, cm, probabilities = evaluate_model(trained_model, test_loader, device)\n",
        "\n",
        "    print(f\"\\nFinal Test Accuracy: {accuracy*100:.2f}%\")\n",
        "    # print(\"\\nClassification Report:\")\n",
        "    # print(report)\n",
        "\n",
        "    # Save model\n",
        "    torch.save({\n",
        "        'model_state_dict': trained_model.state_dict(),\n",
        "        'feature_dim': feature_dim,\n",
        "        'n_classes': n_classes,\n",
        "        'accuracy': accuracy\n",
        "    }, \"eeg_classifier_model.pt\")\n",
        "\n",
        "    print(\"\\nModel saved successfully!\")\n",
        "\n",
        "    print(\"\\nEEG classification pipeline complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import Dataset, DataLoader, SubsetRandomSampler\n",
        "import scipy.signal as signal\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.metrics import accuracy_score, classification_report, confusion_matrix\n",
        "from tqdm import tqdm\n",
        "import os\n",
        "import pandas as pd\n",
        "from scipy.stats import skew, kurtosis\n",
        "import seaborn as sns\n",
        "import gc\n",
        "\n",
        "# STEP 1: Data Loading and Preprocessing\n",
        "class EEGDataset(Dataset):\n",
        "    def __init__(self, data_dict, label_mapping_file=None, transform=None):\n",
        "        self.dataset = data_dict['dataset']\n",
        "        self.labels_list = data_dict['labels']  # List of label IDs\n",
        "        self.transform = transform\n",
        "\n",
        "        # Load the mapping from label ID to text if provided\n",
        "        self.label_id_to_text = {}\n",
        "        if label_mapping_file and os.path.exists(label_mapping_file):\n",
        "            with open(label_mapping_file, 'r') as f:\n",
        "                for line in f:\n",
        "                    parts = line.strip().split()\n",
        "                    if len(parts) >= 2:\n",
        "                        label_id = parts[0]\n",
        "                        label_text = ' '.join(parts[1:])\n",
        "                        self.label_id_to_text[label_id] = label_text\n",
        "\n",
        "        # Create mapping from label ID to index\n",
        "        unique = sorted(set(self.labels_list))\n",
        "        self.label_to_idx = {lbl:i for i,lbl in enumerate(unique)}\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.dataset)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        sample = self.dataset[idx]\n",
        "        eeg_data = sample['eeg_data']  # Shape: [channels, time_points]\n",
        "        label_id = sample['label']  # This is a string ID like 'n02510455'\n",
        "\n",
        "        # Get label text if available\n",
        "        label_text = self.label_id_to_text.get(label_id, label_id)\n",
        "\n",
        "        # Convert string label to numerical index\n",
        "        label_idx = self.label_to_idx.get(label_id, 0)\n",
        "\n",
        "        if self.transform:\n",
        "            eeg_data = self.transform(eeg_data)\n",
        "\n",
        "        return eeg_data, label_idx, label_text\n",
        "\n",
        "# EEG Signal Preprocessing\n",
        "class EEGPreprocessor:\n",
        "    def __init__(self, sampling_rate=1000, notch_freq=50, bandpass_low=0.5, bandpass_high=70):\n",
        "        self.sampling_rate = sampling_rate\n",
        "        self.notch_freq = notch_freq\n",
        "        self.bandpass_low = bandpass_low\n",
        "        self.bandpass_high = bandpass_high\n",
        "\n",
        "        # Pre-compute filter coefficients to avoid recomputation\n",
        "        self.sos = signal.butter(\n",
        "            N=4,\n",
        "            Wn=[self.bandpass_low, self.bandpass_high],\n",
        "            btype='bandpass',\n",
        "            fs=self.sampling_rate,\n",
        "            output='sos'\n",
        "        )\n",
        "        self.b_notch, self.a_notch = signal.iirnotch(self.notch_freq, 30, self.sampling_rate)\n",
        "\n",
        "    def __call__(self, eeg_data):\n",
        "        # Convert to numpy if it's a tensor\n",
        "        if isinstance(eeg_data, torch.Tensor):\n",
        "            eeg_data = eeg_data.numpy()\n",
        "\n",
        "        # Transpose to [time, channels] for easier processing\n",
        "        eeg_data = eeg_data.T\n",
        "\n",
        "        # Apply bandpass filter\n",
        "        eeg_filtered = self._bandpass_filter(eeg_data)\n",
        "\n",
        "        # Apply notch filter (to remove power line interference)\n",
        "        eeg_filtered = self._notch_filter(eeg_filtered)\n",
        "\n",
        "        # Re-reference to common average\n",
        "        eeg_filtered = self._common_average_reference(eeg_filtered)\n",
        "\n",
        "        # Z-score normalization\n",
        "        eeg_normalized = self._normalize(eeg_filtered)\n",
        "\n",
        "        # Transpose back to [channels, time]\n",
        "        return torch.tensor(eeg_normalized.T, dtype=torch.float32)\n",
        "\n",
        "    def _bandpass_filter(self, data):\n",
        "        # Apply forward-backward filtering for zero phase distortion\n",
        "        return signal.sosfiltfilt(self.sos, data, axis=0)\n",
        "\n",
        "    def _notch_filter(self, data):\n",
        "        return signal.filtfilt(self.b_notch, self.a_notch, data, axis=0)\n",
        "\n",
        "    def _common_average_reference(self, data):\n",
        "        # Subtract the mean across all channels at each time point\n",
        "        return data - np.mean(data, axis=1, keepdims=True)\n",
        "\n",
        "    def _normalize(self, data):\n",
        "        # Z-score normalization for each channel\n",
        "        return (data - np.mean(data, axis=0)) / (np.std(data, axis=0) + 1e-10)\n",
        "\n",
        "# STEP 2: Feature Extraction\n",
        "class FeatureExtractor:\n",
        "    def __init__(self, sampling_rate=1000):\n",
        "        self.sampling_rate = sampling_rate\n",
        "\n",
        "        # Define frequency bands\n",
        "        self.freq_bands = {\n",
        "          'delta': (1, 4),       # Adjusted lower bound to reduce DC components\n",
        "          'theta': (4, 8),       # Standard theta for cognitive processing\n",
        "          'alpha_low': (8, 10),  # Lower alpha - attention/inhibition\n",
        "          'alpha_high': (10, 13),# Higher alpha - semantic processing\n",
        "          'beta_low': (13, 20),  # Lower beta - motor preparation\n",
        "          'beta_high': (20, 30), # Higher beta - active processing/cognition\n",
        "          'gamma_low': (30, 60), # Expanded gamma_low for visual processing\n",
        "          'gamma_mid': (60, 90), # Added gamma_mid for binding\n",
        "          'gamma_high': (90, 120)# Higher gamma for fine perceptual binding\n",
        "      }\n",
        "\n",
        "    def extract_features(self, eeg_data):\n",
        "        \"\"\"\n",
        "        Extract time and frequency domain features from EEG data\n",
        "\n",
        "        Args:\n",
        "            eeg_data: EEG data of shape [channels, time_points]\n",
        "\n",
        "        Returns:\n",
        "            features: Dictionary of extracted features\n",
        "        \"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Time domain features\n",
        "        features.update(self._extract_time_domain_features(eeg_data))\n",
        "\n",
        "        # Frequency domain features\n",
        "        features.update(self._extract_frequency_domain_features(eeg_data))\n",
        "\n",
        "        # Connectivity features - can improve classification accuracy\n",
        "        features.update(self._extract_connectivity_features(eeg_data))\n",
        "\n",
        "        # Convert dictionary to vector\n",
        "        feature_vector = []\n",
        "        for key, value in features.items():\n",
        "            if isinstance(value, np.ndarray):\n",
        "                feature_vector.append(value.flatten())\n",
        "            else:\n",
        "                feature_vector.append(np.array([value]).flatten())\n",
        "\n",
        "        return np.concatenate(feature_vector)\n",
        "\n",
        "    def _extract_time_domain_features(self, eeg_data):\n",
        "        features = {}\n",
        "\n",
        "        # Statistical features\n",
        "        features['mean'] = np.mean(eeg_data, axis=1)\n",
        "        features['var'] = np.var(eeg_data, axis=1)\n",
        "        features['skewness'] = skew(eeg_data, axis=1)\n",
        "        features['kurtosis'] = kurtosis(eeg_data, axis=1)\n",
        "        features['max'] = np.max(eeg_data, axis=1)\n",
        "        features['min'] = np.min(eeg_data, axis=1)\n",
        "        features['peak_to_peak'] = features['max'] - features['min']  # Reuse computed values\n",
        "        features['rms'] = np.sqrt(np.mean(np.square(eeg_data), axis=1))\n",
        "        features['zero_crossings'] = np.sum(np.diff(np.signbit(eeg_data), axis=1), axis=1)\n",
        "\n",
        "        # Hjorth parameters\n",
        "        features.update(self._compute_hjorth_parameters(eeg_data))\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _compute_hjorth_parameters(self, eeg_data):\n",
        "        \"\"\"Compute Hjorth parameters: Activity, Mobility, and Complexity\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # First derivative\n",
        "        diff1 = np.diff(eeg_data, axis=1)\n",
        "        # Second derivative\n",
        "        diff2 = np.diff(diff1, axis=1)\n",
        "\n",
        "        # Activity: variance of the signal\n",
        "        features['activity'] = np.var(eeg_data, axis=1)\n",
        "\n",
        "        # Mobility: sqrt(variance of first derivative / variance of signal)\n",
        "        var_diff1 = np.var(diff1, axis=1)\n",
        "        mobility1 = np.sqrt(var_diff1 / (features['activity'] + 1e-10))\n",
        "        features['mobility'] = mobility1\n",
        "\n",
        "        # Complexity: mobility of first derivative / mobility of signal\n",
        "        var_diff2 = np.var(diff2, axis=1)\n",
        "        mobility2 = np.sqrt(var_diff2 / (var_diff1 + 1e-10))\n",
        "        features['complexity'] = mobility2 / (mobility1 + 1e-10)\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _extract_frequency_domain_features(self, eeg_data):\n",
        "        features = {}\n",
        "\n",
        "        # Compute power spectral density with Welch's method\n",
        "        nperseg = min(256, eeg_data.shape[1] // 4)  # Adaptive window size\n",
        "        freqs, psd = signal.welch(eeg_data, fs=self.sampling_rate,\n",
        "                                 nperseg=nperseg,\n",
        "                                 noverlap=nperseg // 2,\n",
        "                                 axis=1)\n",
        "\n",
        "        # Calculate total power once\n",
        "        total_power = np.sum(psd, axis=1) + 1e-10\n",
        "\n",
        "        # Band powers and their ratios\n",
        "        for band_name, (low_freq, high_freq) in self.freq_bands.items():\n",
        "            # Find frequencies in the band\n",
        "            idx_band = np.logical_and(freqs >= low_freq, freqs <= high_freq)\n",
        "            # Calculate band power\n",
        "            band_power = np.sum(psd[:, idx_band], axis=1)\n",
        "            features[f'{band_name}_power'] = band_power\n",
        "\n",
        "            # Calculate relative band power\n",
        "            features[f'{band_name}_rel_power'] = band_power / total_power\n",
        "\n",
        "        # Spectral edge frequency (95%)\n",
        "        features['sef_95'] = self._compute_spectral_edge_frequency(freqs, psd, 0.95)\n",
        "\n",
        "        # Spectral entropy\n",
        "        features['spectral_entropy'] = self._compute_spectral_entropy(psd)\n",
        "\n",
        "        # Spectral peak frequency and power\n",
        "        peak_freqs = freqs[np.argmax(psd, axis=1)]\n",
        "        peak_powers = np.max(psd, axis=1)\n",
        "        features['peak_freq'] = peak_freqs\n",
        "        features['peak_power'] = peak_powers\n",
        "\n",
        "        return features\n",
        "\n",
        "    def _compute_spectral_edge_frequency(self, freqs, psd, edge=0.95):\n",
        "        \"\"\"Compute frequency below which edge% of power resides\"\"\"\n",
        "        sef = np.zeros(psd.shape[0])\n",
        "        for i in range(psd.shape[0]):\n",
        "            # Cumulative sum of PSD\n",
        "            cumsum = np.cumsum(psd[i]) / (np.sum(psd[i]) + 1e-10)\n",
        "            # Find frequency below which edge% of power resides\n",
        "            idx = np.where(cumsum >= edge)[0]\n",
        "            if len(idx) > 0:\n",
        "                sef[i] = freqs[idx[0]]\n",
        "            else:\n",
        "                sef[i] = freqs[-1]\n",
        "        return sef\n",
        "\n",
        "    def _compute_spectral_entropy(self, psd):\n",
        "        \"\"\"Compute spectral entropy\"\"\"\n",
        "        entropy = np.zeros(psd.shape[0])\n",
        "        for i in range(psd.shape[0]):\n",
        "            # Normalize PSD\n",
        "            psd_norm = psd[i] / (np.sum(psd[i]) + 1e-10)\n",
        "            # Calculate entropy\n",
        "            entropy[i] = -np.sum(psd_norm * np.log2(psd_norm + 1e-10))\n",
        "        return entropy\n",
        "\n",
        "    def _extract_connectivity_features(self, eeg_data):\n",
        "        \"\"\"Extract connectivity features between EEG channels\"\"\"\n",
        "        features = {}\n",
        "\n",
        "        # Number of channels\n",
        "        n_channels = eeg_data.shape[0]\n",
        "\n",
        "        # Calculate correlation matrix\n",
        "        corr_matrix = np.corrcoef(eeg_data)\n",
        "\n",
        "        # Extract upper triangle (excluding diagonal)\n",
        "        upper_tri_idx = np.triu_indices(n_channels, k=1)\n",
        "        correlations = corr_matrix[upper_tri_idx]\n",
        "\n",
        "        # Basic statistics of correlations\n",
        "        features['mean_corr'] = np.mean(correlations)\n",
        "        features['std_corr'] = np.std(correlations)\n",
        "        features['max_corr'] = np.max(correlations)\n",
        "        features['min_corr'] = np.min(correlations)\n",
        "\n",
        "        # Phase synchronization - simplified version using Hilbert transform\n",
        "        analytic_signal = signal.hilbert(eeg_data, axis=1)\n",
        "        instantaneous_phase = np.angle(analytic_signal)\n",
        "\n",
        "        # Calculate phase differences between adjacent channels\n",
        "        phase_diff = np.zeros((n_channels-1,) + instantaneous_phase.shape[1:])\n",
        "        for i in range(n_channels-1):\n",
        "            phase_diff[i] = instantaneous_phase[i+1] - instantaneous_phase[i]\n",
        "\n",
        "        # Phase locking value (PLV)\n",
        "        plv_values = np.abs(np.mean(np.exp(1j * phase_diff), axis=1))\n",
        "        features['mean_plv'] = np.mean(plv_values)\n",
        "        features['std_plv'] = np.std(plv_values)\n",
        "\n",
        "        return features\n",
        "\n",
        "# Dataset with precomputed features\n",
        "class EEGFeatureDataset(Dataset):\n",
        "    def __init__(self, features, labels, texts):\n",
        "        self.features = features\n",
        "        self.labels = labels\n",
        "        self.texts = texts\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.features)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        feature = self.features[idx]\n",
        "        label = self.labels[idx]\n",
        "\n",
        "        # Get text from dictionary if texts is a dictionary\n",
        "        if isinstance(self.texts, dict):\n",
        "            text = self.texts.get(label, f\"Unknown-{label}\")\n",
        "        else:\n",
        "            text = self.texts[idx] if idx < len(self.texts) else f\"Unknown-{label}\"\n",
        "\n",
        "        # Convert to tensors if not already\n",
        "        if not isinstance(feature, torch.Tensor):\n",
        "            feature = torch.tensor(feature, dtype=torch.float32)\n",
        "        if not isinstance(label, torch.Tensor) and not isinstance(label, int):\n",
        "            label = torch.tensor(label, dtype=torch.long)\n",
        "\n",
        "        return feature, label, text\n",
        "\n",
        "# STEP 3: Classification Model\n",
        "class EEGClassifier(nn.Module):\n",
        "    def __init__(self, input_dim, n_classes, hidden_dims=[4096, 2048, 1024],\n",
        "                 seq_length=None, n_channels=None, dropout_rate=0.3):\n",
        "        super(EEGClassifier, self).__init__()\n",
        "\n",
        "        self.input_dim = input_dim\n",
        "        self.n_classes = n_classes\n",
        "        self.seq_length = seq_length\n",
        "        self.n_channels = n_channels\n",
        "\n",
        "        # Option to reshape as temporal sequence if seq_length and n_channels are provided\n",
        "        self.reshape_input = seq_length is not None and n_channels is not None\n",
        "\n",
        "        # Input normalization layer\n",
        "        self.input_norm = nn.BatchNorm1d(input_dim)\n",
        "\n",
        "        # PART 1: CNN FEATURE EXTRACTION (if seq_length and n_channels provided)\n",
        "        if self.reshape_input:\n",
        "            # CNN for spatial-temporal feature extraction\n",
        "            self.conv_block = nn.Sequential(\n",
        "                nn.Conv2d(1, 32, kernel_size=(1, 16), stride=(1, 2), padding=(0, 7)),\n",
        "                nn.BatchNorm2d(32),\n",
        "                nn.ELU(),\n",
        "                nn.Conv2d(32, 64, kernel_size=(n_channels, 1), stride=1, padding=0),\n",
        "                nn.BatchNorm2d(64),\n",
        "                nn.ELU(),\n",
        "                nn.AvgPool2d(kernel_size=(1, 4), stride=(1, 4)),\n",
        "                nn.Dropout(dropout_rate)\n",
        "            )\n",
        "\n",
        "            # Calculate output size after convolutions\n",
        "            conv_output_size = self._calculate_conv_output_size()\n",
        "            lstm_input_size = conv_output_size\n",
        "\n",
        "            # LSTM for temporal dynamics\n",
        "            self.lstm = nn.LSTM(\n",
        "                input_size=64,  # Number of features per timestep (output channels from CNN)\n",
        "                hidden_size=128,\n",
        "                num_layers=2,\n",
        "                batch_first=True,\n",
        "                dropout=dropout_rate,\n",
        "                bidirectional=True\n",
        "            )\n",
        "\n",
        "            # Self-attention mechanism for temporal focus\n",
        "            self.attention = SelfAttention(256)  # 256 = 128*2 (bidirectional)\n",
        "\n",
        "            # Set the input dimension for dense layers\n",
        "            dense_input_dim = 256\n",
        "        else:\n",
        "            # If no reshape, use attention on flat input\n",
        "            self.attention = nn.Sequential(\n",
        "                nn.Linear(input_dim, input_dim // 4),\n",
        "                nn.LeakyReLU(0.2),\n",
        "                nn.Linear(input_dim // 4, input_dim),\n",
        "                nn.Sigmoid()\n",
        "            )\n",
        "            dense_input_dim = input_dim\n",
        "\n",
        "        # PART 2: DENSE NETWORK PATHWAY\n",
        "        layers = []\n",
        "        prev_dim = dense_input_dim\n",
        "\n",
        "        for i, hidden_dim in enumerate(hidden_dims):\n",
        "            # Dense block with residual connection if dimensions match\n",
        "            if prev_dim == hidden_dim:\n",
        "                layers.append(ResidualBlock(prev_dim, hidden_dim, dropout_rate))\n",
        "            else:\n",
        "                layers.append(DenseBlock(prev_dim, hidden_dim, dropout_rate))\n",
        "\n",
        "            prev_dim = hidden_dim\n",
        "\n",
        "            # Add Squeeze-and-Excitation blocks for feature recalibration\n",
        "            if i < len(hidden_dims) - 1:  # Not for the last layer\n",
        "                layers.append(SEBlock(hidden_dim))\n",
        "\n",
        "        self.feature_layers = nn.Sequential(*layers)\n",
        "\n",
        "        # Multi-head output with ensemble averaging\n",
        "        self.heads = nn.ModuleList([\n",
        "            nn.Linear(prev_dim, n_classes) for _ in range(3)\n",
        "        ])\n",
        "\n",
        "        # Initialize weights\n",
        "        self.apply(self._init_weights)\n",
        "\n",
        "    def _calculate_conv_output_size(self):\n",
        "        # Calculate output size after convolutions\n",
        "        # This is a placeholder - actual calculation depends on your exact architecture\n",
        "        length_after_conv = ((self.seq_length - 16 + 2*7) // 2) + 1\n",
        "        length_after_pool = length_after_conv // 4\n",
        "        return 64 * length_after_pool  # 64 channels\n",
        "\n",
        "    def _init_weights(self, m):\n",
        "        if isinstance(m, (nn.Linear, nn.Conv2d)):\n",
        "            nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
        "            if m.bias is not None:\n",
        "                nn.init.constant_(m.bias, 0)\n",
        "        elif isinstance(m, (nn.BatchNorm1d, nn.BatchNorm2d)):\n",
        "            nn.init.constant_(m.weight, 1)\n",
        "            nn.init.constant_(m.bias, 0)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Apply input normalization\n",
        "        if not self.reshape_input:\n",
        "            x = self.input_norm(x)\n",
        "\n",
        "            # Apply attention mechanism\n",
        "            attn = self.attention(x)\n",
        "            x = x * attn\n",
        "\n",
        "            # Pass through feature layers\n",
        "            features = self.feature_layers(x)\n",
        "        else:\n",
        "            # Reshape input to [batch, 1, channels, time]\n",
        "            batch_size = x.size(0)\n",
        "            x = x.view(batch_size, 1, self.n_channels, self.seq_length)\n",
        "\n",
        "            # Pass through CNN\n",
        "            x = self.conv_block(x)  # -> [batch, 64, 1, reduced_time]\n",
        "\n",
        "            # Reshape for LSTM: [batch, time, features]\n",
        "            x = x.squeeze(2).permute(0, 2, 1)  # -> [batch, reduced_time, 64]\n",
        "\n",
        "            # Pass through LSTM\n",
        "            x, _ = self.lstm(x)  # -> [batch, reduced_time, 256]\n",
        "\n",
        "            # Apply self-attention\n",
        "            x, _ = self.attention(x)  # -> [batch, 256]\n",
        "\n",
        "            # Pass through feature layers\n",
        "            features = self.feature_layers(x)\n",
        "\n",
        "        # Ensemble predictions from multiple heads\n",
        "        logits = torch.stack([head(features) for head in self.heads])\n",
        "        logits = torch.mean(logits, dim=0)\n",
        "\n",
        "        return logits\n",
        "\n",
        "    def predict_proba(self, x):\n",
        "        logits = self.forward(x)\n",
        "        return torch.softmax(logits, dim=1)\n",
        "\n",
        "\n",
        "# Helper blocks for enhanced architecture\n",
        "\n",
        "class ResidualBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, dropout_rate=0.4):\n",
        "        super(ResidualBlock, self).__init__()\n",
        "        self.linear = nn.Linear(in_dim, out_dim)\n",
        "        self.norm = nn.BatchNorm1d(out_dim)\n",
        "        self.activation = nn.LeakyReLU(0.2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        residual = x\n",
        "        out = self.linear(x)\n",
        "        out = self.norm(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.dropout(out)\n",
        "        out += residual\n",
        "        return out\n",
        "\n",
        "\n",
        "class DenseBlock(nn.Module):\n",
        "    def __init__(self, in_dim, out_dim, dropout_rate=0.4):\n",
        "        super(DenseBlock, self).__init__()\n",
        "        self.linear = nn.Linear(in_dim, out_dim)\n",
        "        self.norm = nn.BatchNorm1d(out_dim)\n",
        "        self.activation = nn.LeakyReLU(0.2)\n",
        "        self.dropout = nn.Dropout(dropout_rate)\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.linear(x)\n",
        "        out = self.norm(out)\n",
        "        out = self.activation(out)\n",
        "        out = self.dropout(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "class SEBlock(nn.Module):\n",
        "    \"\"\"Squeeze-and-Excitation block for feature recalibration\"\"\"\n",
        "    def __init__(self, channel, reduction=16):\n",
        "        super(SEBlock, self).__init__()\n",
        "        self.avg_pool = nn.AdaptiveAvgPool1d(1)\n",
        "        self.fc = nn.Sequential(\n",
        "            nn.Linear(channel, channel // reduction, bias=False),\n",
        "            nn.ReLU(inplace=True),\n",
        "            nn.Linear(channel // reduction, channel, bias=False),\n",
        "            nn.Sigmoid()\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        b, c = x.size()\n",
        "        y = self.avg_pool(x.unsqueeze(-1)).view(b, c)\n",
        "        y = self.fc(y).view(b, c, 1)\n",
        "        return x * y.squeeze(-1)\n",
        "\n",
        "\n",
        "class SelfAttention(nn.Module):\n",
        "    \"\"\"Self-attention mechanism for sequential data\"\"\"\n",
        "    def __init__(self, hidden_dim):\n",
        "        super(SelfAttention, self).__init__()\n",
        "        self.query = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.key = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.value = nn.Linear(hidden_dim, hidden_dim)\n",
        "        self.scale = hidden_dim ** 0.5\n",
        "\n",
        "    def forward(self, x):\n",
        "        # x shape: [batch, seq_len, hidden_dim]\n",
        "        q = self.query(x)\n",
        "        k = self.key(x)\n",
        "        v = self.value(x)\n",
        "\n",
        "        # Calculate attention scores\n",
        "        attn_scores = torch.bmm(q, k.transpose(1, 2)) / self.scale\n",
        "        attn_probs = torch.softmax(attn_scores, dim=-1)\n",
        "\n",
        "        # Apply attention to values\n",
        "        context = torch.bmm(attn_probs, v)\n",
        "\n",
        "        # Global feature vector (attention-weighted sum)\n",
        "        global_feat = torch.sum(context, dim=1)\n",
        "\n",
        "        return global_feat, attn_probs\n",
        "\n",
        "# STEP 4: Data Preprocessing Helper - OPTIMIZED FOR MEMORY\n",
        "def prepare_dataset_with_features(dataset, batch_size=64, device='cuda'):\n",
        "    \"\"\"Pre-compute features for the dataset in memory-efficient batches\"\"\"\n",
        "    feature_extractor = FeatureExtractor()\n",
        "\n",
        "    processed_features = []\n",
        "    labels = []\n",
        "    label_texts = {}\n",
        "\n",
        "    # Process in batches to reduce memory usage\n",
        "    num_batches = (len(dataset) + batch_size - 1) // batch_size\n",
        "\n",
        "    for batch_idx in tqdm(range(num_batches), desc=\"Extracting features in batches\"):\n",
        "        start_idx = batch_idx * batch_size\n",
        "        end_idx = min(start_idx + batch_size, len(dataset))\n",
        "\n",
        "        batch_features = []\n",
        "        batch_labels = []\n",
        "\n",
        "        for idx in range(start_idx, end_idx):\n",
        "            eeg_data, label_idx, label_text = dataset[idx]\n",
        "\n",
        "            # Convert to numpy if needed\n",
        "            if isinstance(eeg_data, torch.Tensor):\n",
        "                eeg_data_np = eeg_data.cpu().numpy()\n",
        "            else:\n",
        "                eeg_data_np = eeg_data\n",
        "\n",
        "            # Extract features\n",
        "            features = feature_extractor.extract_features(eeg_data_np)\n",
        "\n",
        "            batch_features.append(torch.tensor(features, dtype=torch.float32))\n",
        "            batch_labels.append(label_idx)\n",
        "\n",
        "            # Store label text mapping\n",
        "            label_texts[label_idx] = label_text\n",
        "\n",
        "        processed_features.extend(batch_features)\n",
        "        labels.extend(batch_labels)\n",
        "\n",
        "        # Force garbage collection after each batch\n",
        "        gc.collect()\n",
        "\n",
        "        # Clear CUDA cache if using GPU\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Get label texts from dataset if available\n",
        "    if not label_texts and hasattr(dataset, 'get_label_texts'):\n",
        "        label_texts = dataset.get_label_texts()\n",
        "\n",
        "    return processed_features, labels, label_texts\n",
        "\n",
        "# STEP 5: Training Function - MEMORY OPTIMIZED\n",
        "def train_model(model, train_loader, val_loader, num_epochs=500, learning_rate=0.001,\n",
        "               weight_decay=1e-5, device=\"cpu\", class_weights=None):\n",
        "\n",
        "    model.to(device)\n",
        "\n",
        "    # Initialize optimizer with weight decay for regularization\n",
        "    optimizer = optim.Adam(model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
        "\n",
        "    # Learning rate scheduler\n",
        "    scheduler = optim.lr_scheduler.ReduceLROnPlateau(\n",
        "        optimizer, mode='min', factor=0.5, patience=5, verbose=True\n",
        "    )\n",
        "\n",
        "    # Loss function with class weights if provided\n",
        "    if class_weights is not None:\n",
        "        class_weights = torch.tensor(class_weights, dtype=torch.float32).to(device)\n",
        "        criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "    else:\n",
        "        criterion = nn.CrossEntropyLoss()\n",
        "\n",
        "    # Track metrics\n",
        "    train_losses = []\n",
        "    val_losses = []\n",
        "    train_accs = []\n",
        "    val_accs = []\n",
        "\n",
        "    # For early stopping\n",
        "    best_val_loss = float('inf')\n",
        "    patience = 10\n",
        "    no_improve_epoch = 0\n",
        "    best_model_state = None\n",
        "\n",
        "    for epoch in range(num_epochs):\n",
        "        # Training phase\n",
        "        model.train()\n",
        "        train_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "\n",
        "        progress_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Train]\")\n",
        "\n",
        "        for features, labels, _ in progress_bar:\n",
        "            # Move tensors to device\n",
        "            features = features.to(device)\n",
        "            labels = labels.to(device)\n",
        "\n",
        "            # Zero gradients\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "            # Forward pass\n",
        "            outputs = model(features)\n",
        "            loss = criterion(outputs, labels)\n",
        "\n",
        "            # Backward pass and optimize\n",
        "            loss.backward()\n",
        "\n",
        "            # Gradient clipping to prevent exploding gradients\n",
        "            torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "\n",
        "            optimizer.step()\n",
        "\n",
        "            # Calculate accuracy\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "            train_loss += loss.item()\n",
        "\n",
        "            # Update progress bar\n",
        "            progress_bar.set_postfix({\n",
        "                'loss': f\"{loss.item():.4f}\",\n",
        "                'acc': f\"{100 * correct / total:.2f}%\"\n",
        "            })\n",
        "\n",
        "            # Clear GPU memory after each batch\n",
        "            del features, labels, outputs, loss, predicted\n",
        "            if device == 'cuda' and torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        epoch_train_loss = train_loss / len(train_loader)\n",
        "        epoch_train_acc = 100 * correct / total\n",
        "        train_losses.append(epoch_train_loss)\n",
        "        train_accs.append(epoch_train_acc)\n",
        "\n",
        "        # Validation phase\n",
        "        model.eval()\n",
        "        val_loss = 0.0\n",
        "        correct = 0\n",
        "        total = 0\n",
        "        all_preds = []\n",
        "        all_labels = []\n",
        "\n",
        "        with torch.no_grad():\n",
        "            progress_bar = tqdm(val_loader, desc=f\"Epoch {epoch+1}/{num_epochs} [Val]\")\n",
        "\n",
        "            for features, labels, _ in progress_bar:\n",
        "                features = features.to(device)\n",
        "                labels = labels.to(device)\n",
        "\n",
        "                # Forward pass\n",
        "                outputs = model(features)\n",
        "                loss = criterion(outputs, labels)\n",
        "\n",
        "                # Calculate accuracy\n",
        "                _, predicted = torch.max(outputs.data, 1)\n",
        "                total += labels.size(0)\n",
        "                correct += (predicted == labels).sum().item()\n",
        "                val_loss += loss.item()\n",
        "\n",
        "                # Store predictions for metrics\n",
        "                all_preds.extend(predicted.cpu().numpy())\n",
        "                all_labels.extend(labels.cpu().numpy())\n",
        "\n",
        "                # Update progress bar\n",
        "                progress_bar.set_postfix({\n",
        "                    'loss': f\"{loss.item():.4f}\",\n",
        "                    'acc': f\"{100 * correct / total:.2f}%\"\n",
        "                })\n",
        "\n",
        "                # Clear GPU memory\n",
        "                del features, labels, outputs, loss, predicted\n",
        "                if device == 'cuda' and torch.cuda.is_available():\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        # Calculate epoch metrics\n",
        "        epoch_val_loss = val_loss / len(val_loader)\n",
        "        epoch_val_acc = 100 * correct / total\n",
        "        val_losses.append(epoch_val_loss)\n",
        "        val_accs.append(epoch_val_acc)\n",
        "\n",
        "        # Learning rate scheduler step\n",
        "        scheduler.step(epoch_val_loss)\n",
        "\n",
        "        # Print epoch summary\n",
        "        print(f\"\\nEpoch {epoch+1}/{num_epochs}\")\n",
        "        print(f\"Train Loss: {epoch_train_loss:.4f}, Train Acc: {epoch_train_acc:.2f}%\")\n",
        "        print(f\"Val Loss: {epoch_val_loss:.4f}, Val Acc: {epoch_val_acc:.2f}%\")\n",
        "        print(f\"Learning rate: {optimizer.param_groups[0]['lr']:.6f}\")\n",
        "\n",
        "        # Check if this is the best model\n",
        "        if epoch_val_loss < best_val_loss:\n",
        "            best_val_loss = epoch_val_loss\n",
        "            best_model_state = {k: v.cpu().detach() for k, v in model.state_dict().items()}\n",
        "            no_improve_epoch = 0\n",
        "            print(\"New best model saved!\")\n",
        "\n",
        "            # Print classification report\n",
        "            print(\"\\nClassification Report:\")\n",
        "            print(classification_report(all_labels, all_preds))\n",
        "        else:\n",
        "            no_improve_epoch += 1\n",
        "\n",
        "        # Early stopping check\n",
        "        if no_improve_epoch >= patience:\n",
        "            print(f\"Early stopping triggered after {epoch+1} epochs\")\n",
        "            break\n",
        "\n",
        "        # Force garbage collection after each epoch\n",
        "        gc.collect()\n",
        "        if device == 'cuda' and torch.cuda.is_available():\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Load best model weights\n",
        "    model.load_state_dict(best_model_state)\n",
        "    model.to(device)  # Make sure model is on the correct device\n",
        "\n",
        "    # Plot training curves\n",
        "    plt.figure(figsize=(12, 5))\n",
        "\n",
        "    plt.subplot(1, 2, 1)\n",
        "    plt.plot(train_losses, label='Train Loss')\n",
        "    plt.plot(val_losses, label='Validation Loss')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Loss')\n",
        "    plt.title('Training and Validation Loss')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.subplot(1, 2, 2)\n",
        "    plt.plot(train_accs, label='Train Accuracy')\n",
        "    plt.plot(val_accs, label='Validation Accuracy')\n",
        "    plt.xlabel('Epochs')\n",
        "    plt.ylabel('Accuracy (%)')\n",
        "    plt.title('Training and Validation Accuracy')\n",
        "    plt.legend()\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.savefig('training_curves.png')\n",
        "    plt.close()\n",
        "\n",
        "    return model\n",
        "\n",
        "# STEP 6: Evaluation Function - MEMORY OPTIMIZED\n",
        "def evaluate_model(model, test_loader, device='cpu'):\n",
        "    model.eval()\n",
        "    all_preds = []\n",
        "    all_labels = []\n",
        "    all_probs = []\n",
        "\n",
        "    with torch.no_grad():\n",
        "        for features, labels, _ in tqdm(test_loader, desc=\"Evaluating\"):\n",
        "            features = features.to(device)\n",
        "\n",
        "            # Get predictions\n",
        "            outputs = model(features)\n",
        "            probs = torch.softmax(outputs, dim=1)\n",
        "            _, predicted = torch.max(outputs, 1)\n",
        "\n",
        "            # Store results (move to CPU to save GPU memory)\n",
        "            all_preds.extend(predicted.cpu().numpy())\n",
        "            all_labels.extend(labels.numpy())\n",
        "            all_probs.extend(probs.cpu().numpy())\n",
        "\n",
        "            # Clear GPU memory\n",
        "            del features, outputs, probs, predicted\n",
        "            if device == 'cuda' and torch.cuda.is_available():\n",
        "                torch.cuda.empty_cache()\n",
        "\n",
        "    # Calculate accuracy\n",
        "    accuracy = accuracy_score(all_labels, all_preds)\n",
        "\n",
        "    # Generate classification report\n",
        "    report = classification_report(all_labels, all_preds)\n",
        "\n",
        "    # Generate confusion matrix\n",
        "    cm = confusion_matrix(all_labels, all_preds)\n",
        "\n",
        "    # Plot confusion matrix\n",
        "    # plt.figure(figsize=(10, 8))\n",
        "    # sns.heatmap(cm, annot=True, fmt='d', cmap='Blues')\n",
        "    # plt.xlabel('Predicted Label')\n",
        "    # plt.ylabel('True Label')\n",
        "    # plt.title('Confusion Matrix')\n",
        "    # plt.savefig('confusion_matrix.png')\n",
        "    # plt.close()\n",
        "\n",
        "    return accuracy, report, cm, np.array(all_probs)\n",
        "\n",
        "def main():\n",
        "    # Set random seeds for reproducibility\n",
        "    torch.manual_seed(42)\n",
        "    np.random.seed(42)\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.manual_seed_all(42)\n",
        "        torch.backends.cudnn.deterministic = True\n",
        "        torch.backends.cudnn.benchmark = False\n",
        "\n",
        "    # Check device\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(f\"Using device: {device}\")\n",
        "\n",
        "    # Enable memory tracking for PyTorch\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.reset_peak_memory_stats()\n",
        "        print(f\"Initial GPU memory: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "\n",
        "    # Create or use existing label mapping\n",
        "    label_mapping_file = 'labels_txt.txt'\n",
        "\n",
        "    # Set up paths for feature storage\n",
        "    features_path = {\n",
        "        'session_1': \"features_session_1.pt\",\n",
        "        'session_2': \"features_session_2.pt\",\n",
        "        'combined': \"features_combined.pt\"\n",
        "    }\n",
        "\n",
        "    # Track whether we need to recompute the combined features\n",
        "    need_to_combine = False\n",
        "\n",
        "    # Process both sessions sequentially\n",
        "    session_files = [\n",
        "        \"/content/drive/MyDrive/session_1.pth\",\n",
        "        \"/content/drive/MyDrive/session_2.pth\"\n",
        "    ]\n",
        "\n",
        "    # Process each session separately\n",
        "    for i, session_file in enumerate(session_files):\n",
        "        session_name = f\"session_{i+1}\"\n",
        "        session_feature_path = features_path[session_name]\n",
        "\n",
        "        # Check if features for this session already exist\n",
        "        if os.path.exists(session_feature_path):\n",
        "            print(f\"Loading pre-computed features from {session_feature_path}...\")\n",
        "            features_data = torch.load(session_feature_path)\n",
        "            if 'features' not in features_data or 'labels' not in features_data or 'texts' not in features_data:\n",
        "                print(f\"Invalid feature file format for {session_feature_path}. Will recompute.\")\n",
        "                need_to_combine = True\n",
        "                os.remove(session_feature_path)\n",
        "            else:\n",
        "                print(f\"Loaded features with shape {len(features_data['features'])}\")\n",
        "        else:\n",
        "            # If feature file doesn't exist, we need to process and later combine\n",
        "            need_to_combine = True\n",
        "\n",
        "            # Load data for this session\n",
        "            print(f\"Loading data from {session_file}...\")\n",
        "            try:\n",
        "                # Load in CPU memory to avoid GPU memory usage during loading\n",
        "                session_data = torch.load(session_file, map_location='cpu', weights_only=False)\n",
        "                print(f\"Loaded {len(session_data['dataset'])} EEG samples with {len(set(session_data['labels']))} unique classes\")\n",
        "\n",
        "                # Create EEG dataset with preprocessing\n",
        "                preprocessor = EEGPreprocessor(sampling_rate=1000)  # Adjust sampling rate to match your data\n",
        "                dataset = EEGDataset(session_data, label_mapping_file, transform=preprocessor)\n",
        "\n",
        "                # Free up memory from raw data\n",
        "                del session_data\n",
        "                gc.collect()\n",
        "\n",
        "                if device.type == 'cuda':\n",
        "                    torch.cuda.empty_cache()\n",
        "                    print(f\"GPU memory after data loading: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "\n",
        "                # Extract features from the dataset in batches\n",
        "                print(f\"Preprocessing and extracting features for {session_name} in batches...\")\n",
        "                features, labels, texts = prepare_dataset_with_features(dataset, batch_size=32, device=device)\n",
        "\n",
        "                # Save features to disk\n",
        "                print(f\"Saving extracted features to {session_feature_path}...\")\n",
        "                torch.save({\n",
        "                    'features': features,\n",
        "                    'labels': labels,\n",
        "                    'texts': texts\n",
        "                }, session_feature_path)\n",
        "                print(\"Features saved successfully!\")\n",
        "\n",
        "                # Free up memory\n",
        "                del dataset, features, labels, texts\n",
        "                gc.collect()\n",
        "\n",
        "                if device.type == 'cuda':\n",
        "                    torch.cuda.empty_cache()\n",
        "                    print(f\"GPU memory after feature extraction: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "\n",
        "            except Exception as e:\n",
        "                print(f\"Error processing {session_file}: {e}\")\n",
        "                continue\n",
        "\n",
        "    # Check if we need to recombine features\n",
        "    if need_to_combine or not os.path.exists(features_path['combined']):\n",
        "        print(\"Combining features from all sessions...\")\n",
        "        all_features = []\n",
        "        all_labels = []\n",
        "        all_texts = {}\n",
        "\n",
        "        # Load and combine features from each session\n",
        "        for i in range(len(session_files)):\n",
        "            session_name = f\"session_{i+1}\"\n",
        "            session_feature_path = features_path[session_name]\n",
        "\n",
        "            if os.path.exists(session_feature_path):\n",
        "                print(f\"Loading features from {session_feature_path} for combining...\")\n",
        "                features_data = torch.load(session_feature_path)\n",
        "\n",
        "                # Add features and labels\n",
        "                all_features.extend(features_data['features'])\n",
        "                all_labels.extend(features_data['labels'])\n",
        "\n",
        "                # Merge text dictionaries\n",
        "                if isinstance(features_data['texts'], dict):\n",
        "                    all_texts.update(features_data['texts'])\n",
        "                else:\n",
        "                    # Handle case where texts are in list format\n",
        "                    for j, label in enumerate(features_data['labels']):\n",
        "                        if j < len(features_data['texts']):\n",
        "                            all_texts[label] = features_data['texts'][j]\n",
        "\n",
        "                # Clear memory\n",
        "                del features_data\n",
        "                gc.collect()\n",
        "\n",
        "                if device.type == 'cuda':\n",
        "                    torch.cuda.empty_cache()\n",
        "\n",
        "        # Save combined features\n",
        "        print(f\"Total combined features: {len(all_features)}\")\n",
        "        print(f\"Saving combined features to {features_path['combined']}...\")\n",
        "        torch.save({\n",
        "            'features': all_features,\n",
        "            'labels': all_labels,\n",
        "            'texts': all_texts\n",
        "        }, features_path['combined'])\n",
        "        print(\"Combined features saved successfully!\")\n",
        "    else:\n",
        "        # Load combined features\n",
        "        print(f\"Loading pre-computed combined features from {features_path['combined']}...\")\n",
        "        combined_data = torch.load(features_path['combined'])\n",
        "        all_features = combined_data['features']\n",
        "        all_labels = combined_data['labels']\n",
        "        all_texts = combined_data['texts']\n",
        "        print(f\"Loaded combined features with shape {len(all_features)}\")\n",
        "\n",
        "        # Clear memory\n",
        "        del combined_data\n",
        "        gc.collect()\n",
        "\n",
        "        if device.type == 'cuda':\n",
        "            torch.cuda.empty_cache()\n",
        "\n",
        "    # Create feature dataset from combined data\n",
        "    feature_dataset = EEGFeatureDataset(all_features, all_labels, all_texts)\n",
        "\n",
        "    # Free up memory that's no longer needed\n",
        "    del all_features, all_labels\n",
        "    gc.collect()\n",
        "\n",
        "    if device.type == 'cuda':\n",
        "        torch.cuda.empty_cache()\n",
        "        print(f\"GPU memory before training: {torch.cuda.memory_allocated() / 1024**2:.2f} MB\")\n",
        "\n",
        "    # Split data with stratification\n",
        "    train_idx, temp_idx = train_test_split(\n",
        "        range(len(feature_dataset)),\n",
        "        test_size=0.3,\n",
        "        random_state=42,\n",
        "        stratify=feature_dataset.labels\n",
        "    )\n",
        "\n",
        "    val_idx, test_idx = train_test_split(\n",
        "        temp_idx,\n",
        "        test_size=0.5,  # 50% of temp_idx, resulting in 15% of original data\n",
        "        random_state=42,\n",
        "        stratify=[feature_dataset.labels[i] for i in temp_idx]\n",
        "    )\n",
        "\n",
        "    # Create samplers\n",
        "    train_sampler = SubsetRandomSampler(train_idx)\n",
        "    val_sampler = SubsetRandomSampler(val_idx)\n",
        "    test_sampler = SubsetRandomSampler(test_idx)\n",
        "\n",
        "    # Create data loaders with appropriate batch size\n",
        "    # Smaller batch size can help with memory usage\n",
        "    batch_size = 16 if device.type == 'cuda' else 32\n",
        "    train_loader = DataLoader(feature_dataset, batch_size=batch_size, sampler=train_sampler, pin_memory=(device.type=='cuda'))\n",
        "    val_loader = DataLoader(feature_dataset, batch_size=batch_size, sampler=val_sampler, pin_memory=(device.type=='cuda'))\n",
        "    test_loader = DataLoader(feature_dataset, batch_size=batch_size, sampler=test_sampler, pin_memory=(device.type=='cuda'))\n",
        "\n",
        "    # Get feature dimension from the first sample\n",
        "    feature_dim = feature_dataset[0][0].shape[0]\n",
        "    n_classes = len(set(feature_dataset.labels))\n",
        "\n",
        "    print(f\"Feature dimension: {feature_dim}\")\n",
        "    print(f\"Number of classes: {n_classes}\")\n",
        "\n",
        "    # Calculate class weights for imbalanced data\n",
        "    class_counts = np.bincount(feature_dataset.labels)\n",
        "    class_weights = 1.0 / class_counts\n",
        "    class_weights = class_weights / np.sum(class_weights) * len(class_counts)\n",
        "    print(f\"Class weights: {class_weights}\")\n",
        "\n",
        "    # Create model\n",
        "    model = EEGClassifier(\n",
        "        input_dim=feature_dim,\n",
        "        n_classes=n_classes\n",
        "        # hidden_dims=[512, 256, 128]  # Adjust architecture as needed\n",
        "    )\n",
        "\n",
        "    # Count parameters\n",
        "    total_params = sum(p.numel() for p in model.parameters())\n",
        "    print(f\"Total model parameters: {total_params:,}\")\n",
        "\n",
        "    # Train model\n",
        "    print(\"\\nTraining model...\")\n",
        "    trained_model = train_model(\n",
        "        model=model,\n",
        "        train_loader=train_loader,\n",
        "        val_loader=val_loader,\n",
        "        num_epochs=500,  # Adjust as needed\n",
        "        learning_rate=0.0005,\n",
        "        weight_decay=1e-5,\n",
        "        device=device,\n",
        "        class_weights=class_weights\n",
        "    )\n",
        "\n",
        "    # Evaluate model\n",
        "    print(\"\\nEvaluating model on test set...\")\n",
        "    accuracy, report, cm, probabilities = evaluate_model(trained_model, test_loader, device)\n",
        "\n",
        "    print(f\"\\nFinal Test Accuracy: {accuracy*100:.2f}%\")\n",
        "    # print(\"\\nClassification Report:\")\n",
        "    # print(report)\n",
        "\n",
        "    # Save model\n",
        "    torch.save({\n",
        "        'model_state_dict': trained_model.state_dict(),\n",
        "        'feature_dim': feature_dim,\n",
        "        'n_classes': n_classes,\n",
        "        'accuracy': accuracy\n",
        "    }, \"eeg_classifier_model.pt\")\n",
        "\n",
        "    print(\"\\nModel saved successfully!\")\n",
        "\n",
        "    print(\"\\nEEG classification pipeline complete!\")\n",
        "\n",
        "if __name__ == \"__main__\":\n",
        "    main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "zjlkBycfZvL4",
        "outputId": "da8c2307-23d1-492e-dde6-0cc9945b81e3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using device: cuda\n",
            "Initial GPU memory: 16.25 MB\n",
            "Loading pre-computed features from features_session_1.pt...\n",
            "Loaded features with shape 31950\n",
            "Loading pre-computed features from features_session_2.pt...\n",
            "Loaded features with shape 31900\n",
            "Loading pre-computed combined features from features_combined.pt...\n",
            "Loaded combined features with shape 63850\n",
            "GPU memory before training: 16.25 MB\n",
            "Feature dimension: 2114\n",
            "Number of classes: 80\n",
            "Class weights: [0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 1.06400665 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 1.06400665 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 1.06400665 0.99750623\n",
            " 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623 0.99750623\n",
            " 0.99750623 0.99750623]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/torch/optim/lr_scheduler.py:62: UserWarning: The verbose parameter is deprecated. Please use get_last_lr() to access the learning rate.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total model parameters: 24,272,902\n",
            "\n",
            "Training model...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 1/500 [Train]: 100%|██████████| 2794/2794 [00:47<00:00, 58.45it/s, loss=6.0113, acc=2.00%]\n",
            "Epoch 1/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 297.47it/s, loss=4.4330, acc=3.28%]\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n",
            "/usr/local/lib/python3.11/dist-packages/sklearn/metrics/_classification.py:1565: UndefinedMetricWarning: Precision is ill-defined and being set to 0.0 in labels with no predicted samples. Use `zero_division` parameter to control this behavior.\n",
            "  _warn_prf(average, modifier, f\"{metric.capitalize()} is\", len(result))\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 1/500\n",
            "Train Loss: 5.0208, Train Acc: 2.00%\n",
            "Val Loss: 4.6019, Val Acc: 3.28%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.40      0.02      0.03       120\n",
            "           1       0.20      0.01      0.02       120\n",
            "           2       0.25      0.01      0.02       120\n",
            "           3       0.00      0.00      0.00       120\n",
            "           4       0.16      0.03      0.04       120\n",
            "           5       0.00      0.00      0.00       120\n",
            "           6       0.00      0.00      0.00       120\n",
            "           7       0.09      0.02      0.03       120\n",
            "           8       0.05      0.02      0.02       120\n",
            "           9       0.17      0.02      0.03       120\n",
            "          10       0.00      0.00      0.00       120\n",
            "          11       0.03      0.08      0.04       120\n",
            "          12       0.04      0.03      0.04       120\n",
            "          13       0.01      0.03      0.02       120\n",
            "          14       0.00      0.00      0.00       120\n",
            "          15       0.01      0.01      0.01       120\n",
            "          16       0.02      0.28      0.04       120\n",
            "          17       0.04      0.12      0.06       120\n",
            "          18       0.00      0.00      0.00       120\n",
            "          19       0.06      0.01      0.01       120\n",
            "          20       0.20      0.02      0.03       120\n",
            "          21       0.03      0.27      0.05       120\n",
            "          22       0.00      0.00      0.00       120\n",
            "          23       0.05      0.03      0.03       120\n",
            "          24       0.00      0.00      0.00       120\n",
            "          25       0.00      0.00      0.00       120\n",
            "          26       0.02      0.03      0.02       120\n",
            "          27       0.00      0.00      0.00       120\n",
            "          28       0.02      0.10      0.03       120\n",
            "          29       0.00      0.00      0.00       120\n",
            "          30       0.01      0.01      0.01       120\n",
            "          31       0.02      0.13      0.04       120\n",
            "          32       0.03      0.14      0.05       120\n",
            "          33       0.03      0.03      0.03       120\n",
            "          34       0.05      0.04      0.04       120\n",
            "          35       0.02      0.02      0.02       120\n",
            "          36       0.00      0.00      0.00       120\n",
            "          37       0.03      0.09      0.04       120\n",
            "          38       0.00      0.00      0.00       120\n",
            "          39       0.06      0.17      0.09       120\n",
            "          40       0.03      0.05      0.04       120\n",
            "          41       0.25      0.01      0.02       120\n",
            "          42       0.00      0.00      0.00       120\n",
            "          43       0.00      0.00      0.00       112\n",
            "          44       0.00      0.00      0.00       120\n",
            "          45       0.00      0.00      0.00       120\n",
            "          46       0.05      0.02      0.03       120\n",
            "          47       0.16      0.04      0.07       120\n",
            "          48       0.03      0.11      0.05       120\n",
            "          49       0.00      0.00      0.00       120\n",
            "          50       0.00      0.00      0.00       120\n",
            "          51       0.01      0.01      0.01       120\n",
            "          52       0.00      0.00      0.00       120\n",
            "          53       0.03      0.08      0.05       120\n",
            "          54       0.00      0.00      0.00       120\n",
            "          55       0.09      0.04      0.06       120\n",
            "          56       0.00      0.00      0.00       120\n",
            "          57       0.00      0.00      0.00       120\n",
            "          58       0.03      0.03      0.03       120\n",
            "          59       0.01      0.02      0.02       120\n",
            "          60       0.03      0.04      0.03       120\n",
            "          61       0.00      0.00      0.00       120\n",
            "          62       0.00      0.00      0.00       120\n",
            "          63       0.00      0.00      0.00       120\n",
            "          64       0.27      0.05      0.09       112\n",
            "          65       0.20      0.03      0.04       120\n",
            "          66       0.06      0.01      0.01       120\n",
            "          67       0.00      0.00      0.00       120\n",
            "          68       0.06      0.04      0.05       120\n",
            "          69       0.05      0.01      0.01       120\n",
            "          70       0.03      0.03      0.03       113\n",
            "          71       0.03      0.04      0.04       120\n",
            "          72       0.03      0.03      0.03       120\n",
            "          73       0.10      0.05      0.07       120\n",
            "          74       0.00      0.00      0.00       120\n",
            "          75       0.45      0.08      0.14       120\n",
            "          76       0.00      0.00      0.00       120\n",
            "          77       0.00      0.00      0.00       120\n",
            "          78       0.01      0.01      0.01       120\n",
            "          79       0.18      0.05      0.08       120\n",
            "\n",
            "    accuracy                           0.03      9577\n",
            "   macro avg       0.05      0.03      0.02      9577\n",
            "weighted avg       0.05      0.03      0.02      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 2/500 [Train]: 100%|██████████| 2794/2794 [00:50<00:00, 55.52it/s, loss=4.1438, acc=4.21%]\n",
            "Epoch 2/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 295.07it/s, loss=4.4488, acc=7.10%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 2/500\n",
            "Train Loss: 4.5233, Train Acc: 4.21%\n",
            "Val Loss: 4.0736, Val Acc: 7.10%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.04      0.07      0.05       120\n",
            "           1       0.09      0.05      0.06       120\n",
            "           2       0.04      0.06      0.05       120\n",
            "           3       0.00      0.00      0.00       120\n",
            "           4       0.16      0.11      0.13       120\n",
            "           5       0.00      0.00      0.00       120\n",
            "           6       0.07      0.03      0.05       120\n",
            "           7       0.11      0.14      0.12       120\n",
            "           8       0.14      0.05      0.07       120\n",
            "           9       0.03      0.01      0.01       120\n",
            "          10       0.03      0.02      0.02       120\n",
            "          11       0.06      0.25      0.09       120\n",
            "          12       0.12      0.23      0.16       120\n",
            "          13       0.27      0.03      0.06       120\n",
            "          14       0.11      0.15      0.13       120\n",
            "          15       0.04      0.02      0.02       120\n",
            "          16       0.00      0.00      0.00       120\n",
            "          17       0.00      0.00      0.00       120\n",
            "          18       0.17      0.07      0.10       120\n",
            "          19       0.07      0.27      0.11       120\n",
            "          20       0.16      0.13      0.14       120\n",
            "          21       0.03      0.02      0.02       120\n",
            "          22       0.00      0.00      0.00       120\n",
            "          23       0.08      0.05      0.06       120\n",
            "          24       0.17      0.17      0.17       120\n",
            "          25       0.31      0.07      0.12       120\n",
            "          26       0.09      0.05      0.06       120\n",
            "          27       0.09      0.03      0.04       120\n",
            "          28       0.04      0.13      0.06       120\n",
            "          29       0.07      0.13      0.09       120\n",
            "          30       0.15      0.03      0.05       120\n",
            "          31       0.17      0.11      0.13       120\n",
            "          32       0.05      0.19      0.09       120\n",
            "          33       0.62      0.04      0.08       120\n",
            "          34       0.03      0.13      0.05       120\n",
            "          35       0.10      0.09      0.10       120\n",
            "          36       0.05      0.09      0.06       120\n",
            "          37       0.00      0.00      0.00       120\n",
            "          38       0.11      0.03      0.04       120\n",
            "          39       0.06      0.08      0.07       120\n",
            "          40       0.14      0.03      0.04       120\n",
            "          41       0.05      0.24      0.08       120\n",
            "          42       0.06      0.12      0.08       120\n",
            "          43       0.09      0.16      0.12       112\n",
            "          44       0.00      0.00      0.00       120\n",
            "          45       0.02      0.03      0.02       120\n",
            "          46       0.10      0.08      0.09       120\n",
            "          47       0.09      0.04      0.06       120\n",
            "          48       0.15      0.10      0.12       120\n",
            "          49       0.10      0.07      0.08       120\n",
            "          50       0.05      0.03      0.03       120\n",
            "          51       0.06      0.10      0.08       120\n",
            "          52       0.04      0.07      0.05       120\n",
            "          53       0.07      0.04      0.05       120\n",
            "          54       0.06      0.17      0.09       120\n",
            "          55       0.08      0.03      0.04       120\n",
            "          56       0.06      0.01      0.01       120\n",
            "          57       0.11      0.04      0.06       120\n",
            "          58       0.06      0.08      0.07       120\n",
            "          59       0.00      0.00      0.00       120\n",
            "          60       0.02      0.03      0.03       120\n",
            "          61       0.11      0.03      0.05       120\n",
            "          62       0.10      0.03      0.04       120\n",
            "          63       0.67      0.02      0.03       120\n",
            "          64       0.05      0.08      0.06       112\n",
            "          65       0.00      0.00      0.00       120\n",
            "          66       0.11      0.11      0.11       120\n",
            "          67       0.00      0.00      0.00       120\n",
            "          68       0.24      0.08      0.12       120\n",
            "          69       0.10      0.10      0.10       120\n",
            "          70       0.00      0.00      0.00       113\n",
            "          71       0.04      0.03      0.04       120\n",
            "          72       0.15      0.07      0.09       120\n",
            "          73       0.21      0.13      0.16       120\n",
            "          74       0.00      0.00      0.00       120\n",
            "          75       0.09      0.07      0.08       120\n",
            "          76       0.03      0.07      0.04       120\n",
            "          77       0.10      0.03      0.04       120\n",
            "          78       0.21      0.12      0.15       120\n",
            "          79       0.13      0.11      0.12       120\n",
            "\n",
            "    accuracy                           0.07      9577\n",
            "   macro avg       0.10      0.07      0.06      9577\n",
            "weighted avg       0.10      0.07      0.06      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 3/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 58.02it/s, loss=4.2672, acc=7.68%]\n",
            "Epoch 3/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 283.67it/s, loss=3.3428, acc=12.75%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 3/500\n",
            "Train Loss: 4.0724, Train Acc: 7.68%\n",
            "Val Loss: 3.5721, Val Acc: 12.75%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.14      0.13      0.14       120\n",
            "           1       0.13      0.03      0.05       120\n",
            "           2       0.14      0.12      0.13       120\n",
            "           3       0.11      0.12      0.11       120\n",
            "           4       0.15      0.09      0.11       120\n",
            "           5       0.10      0.05      0.07       120\n",
            "           6       0.14      0.07      0.09       120\n",
            "           7       0.07      0.09      0.08       120\n",
            "           8       0.14      0.20      0.17       120\n",
            "           9       0.05      0.02      0.02       120\n",
            "          10       0.08      0.11      0.09       120\n",
            "          11       0.10      0.26      0.14       120\n",
            "          12       0.21      0.15      0.17       120\n",
            "          13       0.12      0.09      0.10       120\n",
            "          14       0.20      0.17      0.18       120\n",
            "          15       0.12      0.07      0.08       120\n",
            "          16       0.09      0.07      0.08       120\n",
            "          17       0.11      0.11      0.11       120\n",
            "          18       0.10      0.05      0.07       120\n",
            "          19       0.11      0.19      0.14       120\n",
            "          20       0.29      0.22      0.25       120\n",
            "          21       0.09      0.11      0.10       120\n",
            "          22       0.08      0.05      0.06       120\n",
            "          23       0.16      0.12      0.13       120\n",
            "          24       0.14      0.36      0.20       120\n",
            "          25       0.11      0.10      0.10       120\n",
            "          26       0.12      0.20      0.15       120\n",
            "          27       0.17      0.19      0.18       120\n",
            "          28       0.11      0.24      0.15       120\n",
            "          29       0.20      0.10      0.13       120\n",
            "          30       0.09      0.11      0.10       120\n",
            "          31       0.27      0.07      0.11       120\n",
            "          32       0.10      0.04      0.06       120\n",
            "          33       0.14      0.12      0.13       120\n",
            "          34       0.11      0.17      0.13       120\n",
            "          35       0.08      0.23      0.11       120\n",
            "          36       0.16      0.12      0.14       120\n",
            "          37       0.11      0.15      0.12       120\n",
            "          38       0.05      0.01      0.01       120\n",
            "          39       0.08      0.11      0.09       120\n",
            "          40       0.14      0.13      0.14       120\n",
            "          41       0.13      0.06      0.08       120\n",
            "          42       0.10      0.12      0.11       120\n",
            "          43       0.20      0.12      0.15       112\n",
            "          44       0.29      0.17      0.21       120\n",
            "          45       0.11      0.13      0.12       120\n",
            "          46       0.23      0.07      0.10       120\n",
            "          47       0.11      0.17      0.14       120\n",
            "          48       0.16      0.20      0.18       120\n",
            "          49       0.24      0.12      0.16       120\n",
            "          50       0.20      0.17      0.18       120\n",
            "          51       0.07      0.03      0.04       120\n",
            "          52       0.15      0.12      0.13       120\n",
            "          53       0.13      0.10      0.11       120\n",
            "          54       0.12      0.23      0.15       120\n",
            "          55       0.07      0.04      0.05       120\n",
            "          56       0.09      0.21      0.12       120\n",
            "          57       0.15      0.12      0.14       120\n",
            "          58       0.07      0.05      0.06       120\n",
            "          59       0.06      0.12      0.08       120\n",
            "          60       0.08      0.17      0.11       120\n",
            "          61       0.10      0.04      0.06       120\n",
            "          62       0.17      0.15      0.16       120\n",
            "          63       0.08      0.04      0.05       120\n",
            "          64       0.09      0.16      0.12       112\n",
            "          65       0.26      0.17      0.20       120\n",
            "          66       0.18      0.19      0.19       120\n",
            "          67       0.09      0.07      0.08       120\n",
            "          68       0.16      0.20      0.18       120\n",
            "          69       0.11      0.09      0.10       120\n",
            "          70       0.15      0.08      0.10       113\n",
            "          71       0.15      0.08      0.11       120\n",
            "          72       0.24      0.10      0.14       120\n",
            "          73       0.12      0.15      0.13       120\n",
            "          74       0.16      0.07      0.09       120\n",
            "          75       0.26      0.30      0.28       120\n",
            "          76       0.20      0.22      0.21       120\n",
            "          77       0.15      0.17      0.16       120\n",
            "          78       0.18      0.18      0.18       120\n",
            "          79       0.18      0.07      0.11       120\n",
            "\n",
            "    accuracy                           0.13      9577\n",
            "   macro avg       0.14      0.13      0.12      9577\n",
            "weighted avg       0.14      0.13      0.12      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 4/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.79it/s, loss=3.8497, acc=11.93%]\n",
            "Epoch 4/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 240.41it/s, loss=2.9864, acc=16.97%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 4/500\n",
            "Train Loss: 3.6777, Train Acc: 11.93%\n",
            "Val Loss: 3.2689, Val Acc: 16.97%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.16      0.13      0.15       120\n",
            "           1       0.21      0.04      0.07       120\n",
            "           2       0.09      0.12      0.10       120\n",
            "           3       0.14      0.12      0.13       120\n",
            "           4       0.10      0.19      0.13       120\n",
            "           5       0.16      0.17      0.16       120\n",
            "           6       0.13      0.31      0.18       120\n",
            "           7       0.29      0.04      0.07       120\n",
            "           8       0.13      0.17      0.15       120\n",
            "           9       0.23      0.12      0.16       120\n",
            "          10       0.19      0.10      0.13       120\n",
            "          11       0.26      0.21      0.23       120\n",
            "          12       0.23      0.18      0.20       120\n",
            "          13       0.11      0.24      0.15       120\n",
            "          14       0.18      0.20      0.19       120\n",
            "          15       0.27      0.11      0.15       120\n",
            "          16       0.16      0.07      0.10       120\n",
            "          17       0.17      0.24      0.20       120\n",
            "          18       0.13      0.10      0.11       120\n",
            "          19       0.19      0.32      0.24       120\n",
            "          20       0.25      0.34      0.29       120\n",
            "          21       0.12      0.24      0.16       120\n",
            "          22       0.11      0.13      0.12       120\n",
            "          23       0.15      0.09      0.11       120\n",
            "          24       0.24      0.28      0.26       120\n",
            "          25       0.21      0.19      0.20       120\n",
            "          26       0.17      0.28      0.21       120\n",
            "          27       0.18      0.34      0.23       120\n",
            "          28       0.15      0.11      0.12       120\n",
            "          29       0.18      0.14      0.16       120\n",
            "          30       0.10      0.14      0.12       120\n",
            "          31       0.28      0.24      0.26       120\n",
            "          32       0.20      0.16      0.18       120\n",
            "          33       0.20      0.23      0.21       120\n",
            "          34       0.12      0.16      0.13       120\n",
            "          35       0.21      0.19      0.20       120\n",
            "          36       0.34      0.11      0.16       120\n",
            "          37       0.20      0.13      0.16       120\n",
            "          38       0.20      0.20      0.20       120\n",
            "          39       0.15      0.17      0.16       120\n",
            "          40       0.16      0.22      0.18       120\n",
            "          41       0.21      0.21      0.21       120\n",
            "          42       0.20      0.26      0.23       120\n",
            "          43       0.21      0.10      0.13       112\n",
            "          44       0.19      0.12      0.15       120\n",
            "          45       0.10      0.07      0.08       120\n",
            "          46       0.26      0.14      0.18       120\n",
            "          47       0.18      0.14      0.16       120\n",
            "          48       0.24      0.20      0.22       120\n",
            "          49       0.17      0.11      0.13       120\n",
            "          50       0.18      0.13      0.15       120\n",
            "          51       0.16      0.13      0.15       120\n",
            "          52       0.07      0.09      0.08       120\n",
            "          53       0.09      0.09      0.09       120\n",
            "          54       0.19      0.24      0.21       120\n",
            "          55       0.17      0.13      0.15       120\n",
            "          56       0.11      0.15      0.13       120\n",
            "          57       0.20      0.07      0.11       120\n",
            "          58       0.11      0.07      0.09       120\n",
            "          59       0.12      0.18      0.15       120\n",
            "          60       0.10      0.12      0.11       120\n",
            "          61       0.10      0.08      0.09       120\n",
            "          62       0.16      0.12      0.14       120\n",
            "          63       0.20      0.16      0.18       120\n",
            "          64       0.24      0.20      0.22       112\n",
            "          65       0.30      0.27      0.28       120\n",
            "          66       0.45      0.14      0.22       120\n",
            "          67       0.09      0.09      0.09       120\n",
            "          68       0.27      0.22      0.24       120\n",
            "          69       0.14      0.27      0.18       120\n",
            "          70       0.11      0.11      0.11       113\n",
            "          71       0.15      0.16      0.16       120\n",
            "          72       0.19      0.16      0.17       120\n",
            "          73       0.14      0.13      0.14       120\n",
            "          74       0.14      0.17      0.16       120\n",
            "          75       0.35      0.28      0.31       120\n",
            "          76       0.19      0.40      0.26       120\n",
            "          77       0.36      0.16      0.22       120\n",
            "          78       0.28      0.20      0.23       120\n",
            "          79       0.26      0.17      0.21       120\n",
            "\n",
            "    accuracy                           0.17      9577\n",
            "   macro avg       0.19      0.17      0.17      9577\n",
            "weighted avg       0.19      0.17      0.17      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 5/500 [Train]: 100%|██████████| 2794/2794 [00:47<00:00, 59.01it/s, loss=3.3552, acc=16.12%]\n",
            "Epoch 5/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 281.39it/s, loss=3.3737, acc=20.66%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 5/500\n",
            "Train Loss: 3.3913, Train Acc: 16.12%\n",
            "Val Loss: 3.0624, Val Acc: 20.66%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.19      0.22      0.20       120\n",
            "           1       0.16      0.15      0.15       120\n",
            "           2       0.23      0.22      0.22       120\n",
            "           3       0.22      0.26      0.24       120\n",
            "           4       0.21      0.18      0.19       120\n",
            "           5       0.29      0.10      0.15       120\n",
            "           6       0.33      0.16      0.21       120\n",
            "           7       0.24      0.17      0.20       120\n",
            "           8       0.28      0.19      0.23       120\n",
            "           9       0.15      0.23      0.18       120\n",
            "          10       0.13      0.18      0.15       120\n",
            "          11       0.33      0.29      0.31       120\n",
            "          12       0.19      0.42      0.26       120\n",
            "          13       0.24      0.22      0.23       120\n",
            "          14       0.17      0.24      0.20       120\n",
            "          15       0.17      0.16      0.16       120\n",
            "          16       0.16      0.05      0.08       120\n",
            "          17       0.13      0.26      0.17       120\n",
            "          18       0.20      0.20      0.20       120\n",
            "          19       0.25      0.28      0.26       120\n",
            "          20       0.23      0.33      0.27       120\n",
            "          21       0.21      0.15      0.17       120\n",
            "          22       0.17      0.20      0.18       120\n",
            "          23       0.22      0.17      0.20       120\n",
            "          24       0.29      0.22      0.25       120\n",
            "          25       0.21      0.15      0.18       120\n",
            "          26       0.20      0.07      0.10       120\n",
            "          27       0.19      0.16      0.17       120\n",
            "          28       0.17      0.14      0.16       120\n",
            "          29       0.22      0.14      0.17       120\n",
            "          30       0.15      0.28      0.19       120\n",
            "          31       0.16      0.17      0.17       120\n",
            "          32       0.25      0.17      0.21       120\n",
            "          33       0.34      0.11      0.16       120\n",
            "          34       0.21      0.19      0.20       120\n",
            "          35       0.22      0.33      0.26       120\n",
            "          36       0.16      0.27      0.20       120\n",
            "          37       0.20      0.31      0.25       120\n",
            "          38       0.33      0.17      0.22       120\n",
            "          39       0.18      0.17      0.18       120\n",
            "          40       0.30      0.28      0.29       120\n",
            "          41       0.19      0.31      0.23       120\n",
            "          42       0.20      0.34      0.25       120\n",
            "          43       0.18      0.17      0.17       112\n",
            "          44       0.24      0.31      0.27       120\n",
            "          45       0.10      0.17      0.13       120\n",
            "          46       0.22      0.17      0.19       120\n",
            "          47       0.29      0.16      0.21       120\n",
            "          48       0.23      0.18      0.20       120\n",
            "          49       0.23      0.24      0.24       120\n",
            "          50       0.16      0.21      0.18       120\n",
            "          51       0.19      0.21      0.20       120\n",
            "          52       0.32      0.22      0.26       120\n",
            "          53       0.30      0.24      0.27       120\n",
            "          54       0.19      0.18      0.19       120\n",
            "          55       0.17      0.15      0.16       120\n",
            "          56       0.24      0.15      0.19       120\n",
            "          57       0.22      0.19      0.21       120\n",
            "          58       0.15      0.17      0.16       120\n",
            "          59       0.21      0.11      0.14       120\n",
            "          60       0.17      0.23      0.20       120\n",
            "          61       0.14      0.20      0.16       120\n",
            "          62       0.13      0.13      0.13       120\n",
            "          63       0.20      0.16      0.18       120\n",
            "          64       0.31      0.24      0.27       112\n",
            "          65       0.25      0.28      0.26       120\n",
            "          66       0.45      0.28      0.35       120\n",
            "          67       0.23      0.11      0.15       120\n",
            "          68       0.28      0.29      0.29       120\n",
            "          69       0.29      0.10      0.15       120\n",
            "          70       0.07      0.04      0.05       113\n",
            "          71       0.22      0.17      0.20       120\n",
            "          72       0.27      0.16      0.20       120\n",
            "          73       0.25      0.24      0.24       120\n",
            "          74       0.21      0.17      0.19       120\n",
            "          75       0.26      0.38      0.31       120\n",
            "          76       0.23      0.24      0.23       120\n",
            "          77       0.16      0.23      0.19       120\n",
            "          78       0.26      0.28      0.27       120\n",
            "          79       0.24      0.34      0.28       120\n",
            "\n",
            "    accuracy                           0.21      9577\n",
            "   macro avg       0.22      0.21      0.20      9577\n",
            "weighted avg       0.22      0.21      0.20      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 6/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.45it/s, loss=4.0776, acc=19.73%]\n",
            "Epoch 6/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 285.39it/s, loss=2.6424, acc=23.98%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 6/500\n",
            "Train Loss: 3.1651, Train Acc: 19.73%\n",
            "Val Loss: 2.8624, Val Acc: 23.98%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.20      0.24       120\n",
            "           1       0.19      0.26      0.22       120\n",
            "           2       0.25      0.17      0.20       120\n",
            "           3       0.32      0.20      0.24       120\n",
            "           4       0.21      0.23      0.22       120\n",
            "           5       0.24      0.20      0.22       120\n",
            "           6       0.34      0.23      0.27       120\n",
            "           7       0.23      0.18      0.21       120\n",
            "           8       0.31      0.24      0.27       120\n",
            "           9       0.32      0.23      0.26       120\n",
            "          10       0.19      0.25      0.22       120\n",
            "          11       0.35      0.31      0.33       120\n",
            "          12       0.18      0.25      0.21       120\n",
            "          13       0.19      0.25      0.22       120\n",
            "          14       0.29      0.23      0.25       120\n",
            "          15       0.31      0.24      0.27       120\n",
            "          16       0.17      0.12      0.14       120\n",
            "          17       0.28      0.27      0.27       120\n",
            "          18       0.19      0.17      0.18       120\n",
            "          19       0.20      0.26      0.22       120\n",
            "          20       0.27      0.22      0.24       120\n",
            "          21       0.30      0.19      0.23       120\n",
            "          22       0.25      0.15      0.19       120\n",
            "          23       0.20      0.26      0.22       120\n",
            "          24       0.25      0.30      0.27       120\n",
            "          25       0.23      0.24      0.24       120\n",
            "          26       0.26      0.12      0.16       120\n",
            "          27       0.44      0.20      0.28       120\n",
            "          28       0.17      0.33      0.23       120\n",
            "          29       0.22      0.33      0.26       120\n",
            "          30       0.46      0.25      0.32       120\n",
            "          31       0.22      0.34      0.27       120\n",
            "          32       0.37      0.22      0.27       120\n",
            "          33       0.37      0.25      0.30       120\n",
            "          34       0.22      0.26      0.24       120\n",
            "          35       0.36      0.35      0.35       120\n",
            "          36       0.23      0.23      0.23       120\n",
            "          37       0.23      0.23      0.23       120\n",
            "          38       0.27      0.16      0.20       120\n",
            "          39       0.28      0.24      0.26       120\n",
            "          40       0.27      0.39      0.32       120\n",
            "          41       0.26      0.22      0.24       120\n",
            "          42       0.15      0.28      0.20       120\n",
            "          43       0.32      0.25      0.28       112\n",
            "          44       0.33      0.23      0.27       120\n",
            "          45       0.14      0.14      0.14       120\n",
            "          46       0.17      0.17      0.17       120\n",
            "          47       0.19      0.17      0.18       120\n",
            "          48       0.30      0.32      0.31       120\n",
            "          49       0.26      0.27      0.26       120\n",
            "          50       0.25      0.33      0.29       120\n",
            "          51       0.23      0.46      0.30       120\n",
            "          52       0.25      0.28      0.27       120\n",
            "          53       0.16      0.15      0.15       120\n",
            "          54       0.38      0.20      0.26       120\n",
            "          55       0.23      0.13      0.17       120\n",
            "          56       0.24      0.12      0.16       120\n",
            "          57       0.24      0.27      0.25       120\n",
            "          58       0.16      0.19      0.17       120\n",
            "          59       0.22      0.31      0.26       120\n",
            "          60       0.18      0.17      0.18       120\n",
            "          61       0.21      0.17      0.19       120\n",
            "          62       0.13      0.15      0.14       120\n",
            "          63       0.21      0.15      0.18       120\n",
            "          64       0.17      0.30      0.22       112\n",
            "          65       0.33      0.42      0.37       120\n",
            "          66       0.43      0.17      0.24       120\n",
            "          67       0.15      0.12      0.13       120\n",
            "          68       0.35      0.23      0.28       120\n",
            "          69       0.25      0.26      0.25       120\n",
            "          70       0.17      0.26      0.21       113\n",
            "          71       0.22      0.28      0.25       120\n",
            "          72       0.17      0.15      0.16       120\n",
            "          73       0.40      0.33      0.36       120\n",
            "          74       0.21      0.17      0.19       120\n",
            "          75       0.28      0.40      0.33       120\n",
            "          76       0.28      0.31      0.29       120\n",
            "          77       0.20      0.38      0.26       120\n",
            "          78       0.26      0.42      0.32       120\n",
            "          79       0.35      0.15      0.21       120\n",
            "\n",
            "    accuracy                           0.24      9577\n",
            "   macro avg       0.25      0.24      0.24      9577\n",
            "weighted avg       0.25      0.24      0.24      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 7/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.96it/s, loss=2.9894, acc=23.03%]\n",
            "Epoch 7/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 288.53it/s, loss=3.8121, acc=25.51%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 7/500\n",
            "Train Loss: 2.9791, Train Acc: 23.03%\n",
            "Val Loss: 2.7721, Val Acc: 25.51%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.19      0.23      0.21       120\n",
            "           1       0.26      0.21      0.23       120\n",
            "           2       0.21      0.24      0.22       120\n",
            "           3       0.34      0.30      0.32       120\n",
            "           4       0.23      0.23      0.23       120\n",
            "           5       0.34      0.26      0.30       120\n",
            "           6       0.28      0.30      0.29       120\n",
            "           7       0.32      0.28      0.30       120\n",
            "           8       0.21      0.15      0.18       120\n",
            "           9       0.32      0.19      0.24       120\n",
            "          10       0.18      0.34      0.24       120\n",
            "          11       0.42      0.30      0.35       120\n",
            "          12       0.27      0.38      0.31       120\n",
            "          13       0.18      0.23      0.20       120\n",
            "          14       0.26      0.38      0.31       120\n",
            "          15       0.21      0.27      0.24       120\n",
            "          16       0.24      0.10      0.14       120\n",
            "          17       0.20      0.24      0.22       120\n",
            "          18       0.21      0.14      0.17       120\n",
            "          19       0.39      0.29      0.33       120\n",
            "          20       0.32      0.33      0.33       120\n",
            "          21       0.23      0.16      0.19       120\n",
            "          22       0.36      0.25      0.29       120\n",
            "          23       0.17      0.28      0.21       120\n",
            "          24       0.33      0.28      0.30       120\n",
            "          25       0.21      0.25      0.23       120\n",
            "          26       0.17      0.18      0.18       120\n",
            "          27       0.30      0.26      0.28       120\n",
            "          28       0.18      0.32      0.23       120\n",
            "          29       0.29      0.23      0.26       120\n",
            "          30       0.24      0.33      0.28       120\n",
            "          31       0.39      0.28      0.32       120\n",
            "          32       0.24      0.27      0.25       120\n",
            "          33       0.28      0.26      0.27       120\n",
            "          34       0.40      0.19      0.26       120\n",
            "          35       0.43      0.41      0.42       120\n",
            "          36       0.19      0.21      0.20       120\n",
            "          37       0.27      0.37      0.31       120\n",
            "          38       0.22      0.19      0.21       120\n",
            "          39       0.33      0.34      0.33       120\n",
            "          40       0.27      0.33      0.30       120\n",
            "          41       0.21      0.32      0.25       120\n",
            "          42       0.16      0.18      0.17       120\n",
            "          43       0.33      0.29      0.31       112\n",
            "          44       0.27      0.33      0.29       120\n",
            "          45       0.15      0.13      0.14       120\n",
            "          46       0.30      0.26      0.28       120\n",
            "          47       0.19      0.31      0.23       120\n",
            "          48       0.48      0.19      0.27       120\n",
            "          49       0.24      0.20      0.22       120\n",
            "          50       0.34      0.22      0.27       120\n",
            "          51       0.31      0.17      0.22       120\n",
            "          52       0.19      0.28      0.23       120\n",
            "          53       0.25      0.28      0.26       120\n",
            "          54       0.35      0.26      0.30       120\n",
            "          55       0.30      0.19      0.23       120\n",
            "          56       0.27      0.21      0.24       120\n",
            "          57       0.29      0.28      0.28       120\n",
            "          58       0.17      0.17      0.17       120\n",
            "          59       0.16      0.25      0.20       120\n",
            "          60       0.17      0.15      0.16       120\n",
            "          61       0.21      0.30      0.25       120\n",
            "          62       0.21      0.16      0.18       120\n",
            "          63       0.31      0.21      0.25       120\n",
            "          64       0.29      0.27      0.28       112\n",
            "          65       0.38      0.44      0.41       120\n",
            "          66       0.26      0.24      0.25       120\n",
            "          67       0.22      0.23      0.22       120\n",
            "          68       0.40      0.36      0.38       120\n",
            "          69       0.30      0.13      0.18       120\n",
            "          70       0.27      0.12      0.16       113\n",
            "          71       0.22      0.30      0.26       120\n",
            "          72       0.19      0.23      0.20       120\n",
            "          73       0.29      0.30      0.30       120\n",
            "          74       0.26      0.26      0.26       120\n",
            "          75       0.23      0.36      0.28       120\n",
            "          76       0.30      0.18      0.23       120\n",
            "          77       0.34      0.28      0.31       120\n",
            "          78       0.27      0.31      0.29       120\n",
            "          79       0.38      0.28      0.32       120\n",
            "\n",
            "    accuracy                           0.26      9577\n",
            "   macro avg       0.27      0.26      0.25      9577\n",
            "weighted avg       0.27      0.26      0.25      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 8/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.75it/s, loss=4.6163, acc=25.68%]\n",
            "Epoch 8/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 283.85it/s, loss=2.1794, acc=28.17%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 8/500\n",
            "Train Loss: 2.8254, Train Acc: 25.68%\n",
            "Val Loss: 2.6410, Val Acc: 28.17%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.37      0.24      0.29       120\n",
            "           1       0.27      0.28      0.28       120\n",
            "           2       0.28      0.31      0.29       120\n",
            "           3       0.30      0.28      0.29       120\n",
            "           4       0.25      0.33      0.29       120\n",
            "           5       0.31      0.30      0.30       120\n",
            "           6       0.28      0.31      0.29       120\n",
            "           7       0.20      0.39      0.26       120\n",
            "           8       0.33      0.30      0.31       120\n",
            "           9       0.26      0.24      0.25       120\n",
            "          10       0.22      0.34      0.27       120\n",
            "          11       0.45      0.40      0.42       120\n",
            "          12       0.28      0.30      0.29       120\n",
            "          13       0.35      0.27      0.30       120\n",
            "          14       0.29      0.26      0.27       120\n",
            "          15       0.28      0.23      0.25       120\n",
            "          16       0.17      0.11      0.13       120\n",
            "          17       0.28      0.29      0.29       120\n",
            "          18       0.23      0.15      0.18       120\n",
            "          19       0.31      0.39      0.35       120\n",
            "          20       0.29      0.30      0.29       120\n",
            "          21       0.41      0.14      0.21       120\n",
            "          22       0.36      0.24      0.29       120\n",
            "          23       0.27      0.29      0.28       120\n",
            "          24       0.23      0.30      0.26       120\n",
            "          25       0.21      0.23      0.22       120\n",
            "          26       0.29      0.30      0.29       120\n",
            "          27       0.29      0.30      0.29       120\n",
            "          28       0.29      0.34      0.31       120\n",
            "          29       0.29      0.27      0.28       120\n",
            "          30       0.31      0.27      0.29       120\n",
            "          31       0.32      0.33      0.33       120\n",
            "          32       0.39      0.35      0.37       120\n",
            "          33       0.45      0.25      0.32       120\n",
            "          34       0.31      0.24      0.27       120\n",
            "          35       0.31      0.41      0.36       120\n",
            "          36       0.21      0.23      0.22       120\n",
            "          37       0.27      0.38      0.31       120\n",
            "          38       0.24      0.30      0.27       120\n",
            "          39       0.32      0.37      0.34       120\n",
            "          40       0.33      0.39      0.36       120\n",
            "          41       0.42      0.31      0.35       120\n",
            "          42       0.32      0.21      0.25       120\n",
            "          43       0.32      0.34      0.33       112\n",
            "          44       0.28      0.32      0.30       120\n",
            "          45       0.17      0.21      0.19       120\n",
            "          46       0.27      0.23      0.25       120\n",
            "          47       0.36      0.22      0.27       120\n",
            "          48       0.32      0.28      0.30       120\n",
            "          49       0.23      0.31      0.27       120\n",
            "          50       0.42      0.27      0.32       120\n",
            "          51       0.25      0.26      0.25       120\n",
            "          52       0.31      0.31      0.31       120\n",
            "          53       0.23      0.25      0.24       120\n",
            "          54       0.29      0.27      0.28       120\n",
            "          55       0.29      0.21      0.24       120\n",
            "          56       0.28      0.25      0.27       120\n",
            "          57       0.36      0.23      0.28       120\n",
            "          58       0.16      0.24      0.20       120\n",
            "          59       0.23      0.22      0.22       120\n",
            "          60       0.28      0.18      0.22       120\n",
            "          61       0.17      0.20      0.18       120\n",
            "          62       0.17      0.27      0.21       120\n",
            "          63       0.32      0.36      0.34       120\n",
            "          64       0.30      0.24      0.27       112\n",
            "          65       0.38      0.40      0.39       120\n",
            "          66       0.35      0.37      0.36       120\n",
            "          67       0.23      0.25      0.24       120\n",
            "          68       0.31      0.39      0.34       120\n",
            "          69       0.25      0.30      0.27       120\n",
            "          70       0.20      0.19      0.20       113\n",
            "          71       0.29      0.23      0.26       120\n",
            "          72       0.25      0.33      0.28       120\n",
            "          73       0.38      0.21      0.27       120\n",
            "          74       0.36      0.24      0.29       120\n",
            "          75       0.30      0.28      0.29       120\n",
            "          76       0.29      0.29      0.29       120\n",
            "          77       0.27      0.39      0.32       120\n",
            "          78       0.33      0.30      0.31       120\n",
            "          79       0.33      0.26      0.29       120\n",
            "\n",
            "    accuracy                           0.28      9577\n",
            "   macro avg       0.29      0.28      0.28      9577\n",
            "weighted avg       0.29      0.28      0.28      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 9/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.15it/s, loss=2.7948, acc=28.90%]\n",
            "Epoch 9/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 281.06it/s, loss=1.9833, acc=29.39%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 9/500\n",
            "Train Loss: 2.6704, Train Acc: 28.90%\n",
            "Val Loss: 2.5673, Val Acc: 29.39%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.23      0.26       120\n",
            "           1       0.29      0.34      0.31       120\n",
            "           2       0.34      0.40      0.37       120\n",
            "           3       0.24      0.27      0.25       120\n",
            "           4       0.24      0.21      0.22       120\n",
            "           5       0.24      0.37      0.29       120\n",
            "           6       0.27      0.31      0.29       120\n",
            "           7       0.45      0.25      0.32       120\n",
            "           8       0.40      0.35      0.37       120\n",
            "           9       0.25      0.15      0.19       120\n",
            "          10       0.25      0.23      0.24       120\n",
            "          11       0.36      0.42      0.39       120\n",
            "          12       0.36      0.29      0.32       120\n",
            "          13       0.27      0.31      0.29       120\n",
            "          14       0.28      0.34      0.31       120\n",
            "          15       0.25      0.26      0.25       120\n",
            "          16       0.18      0.27      0.22       120\n",
            "          17       0.32      0.31      0.31       120\n",
            "          18       0.25      0.18      0.21       120\n",
            "          19       0.34      0.28      0.31       120\n",
            "          20       0.37      0.33      0.35       120\n",
            "          21       0.34      0.28      0.31       120\n",
            "          22       0.31      0.32      0.32       120\n",
            "          23       0.28      0.27      0.27       120\n",
            "          24       0.33      0.31      0.32       120\n",
            "          25       0.37      0.25      0.30       120\n",
            "          26       0.28      0.38      0.33       120\n",
            "          27       0.28      0.26      0.27       120\n",
            "          28       0.23      0.21      0.22       120\n",
            "          29       0.27      0.27      0.27       120\n",
            "          30       0.27      0.31      0.29       120\n",
            "          31       0.33      0.29      0.31       120\n",
            "          32       0.34      0.35      0.35       120\n",
            "          33       0.40      0.33      0.37       120\n",
            "          34       0.26      0.26      0.26       120\n",
            "          35       0.40      0.44      0.42       120\n",
            "          36       0.19      0.23      0.20       120\n",
            "          37       0.46      0.20      0.28       120\n",
            "          38       0.40      0.25      0.31       120\n",
            "          39       0.32      0.35      0.33       120\n",
            "          40       0.35      0.39      0.37       120\n",
            "          41       0.30      0.40      0.35       120\n",
            "          42       0.25      0.18      0.21       120\n",
            "          43       0.28      0.24      0.26       112\n",
            "          44       0.24      0.27      0.25       120\n",
            "          45       0.25      0.23      0.24       120\n",
            "          46       0.21      0.30      0.24       120\n",
            "          47       0.29      0.22      0.25       120\n",
            "          48       0.23      0.23      0.23       120\n",
            "          49       0.35      0.31      0.33       120\n",
            "          50       0.34      0.33      0.34       120\n",
            "          51       0.22      0.38      0.27       120\n",
            "          52       0.35      0.33      0.34       120\n",
            "          53       0.35      0.19      0.25       120\n",
            "          54       0.32      0.37      0.34       120\n",
            "          55       0.34      0.18      0.24       120\n",
            "          56       0.34      0.35      0.34       120\n",
            "          57       0.24      0.20      0.22       120\n",
            "          58       0.22      0.23      0.22       120\n",
            "          59       0.29      0.33      0.31       120\n",
            "          60       0.22      0.27      0.24       120\n",
            "          61       0.23      0.30      0.26       120\n",
            "          62       0.25      0.17      0.20       120\n",
            "          63       0.36      0.24      0.29       120\n",
            "          64       0.34      0.31      0.33       112\n",
            "          65       0.30      0.47      0.36       120\n",
            "          66       0.35      0.39      0.37       120\n",
            "          67       0.30      0.20      0.24       120\n",
            "          68       0.29      0.36      0.32       120\n",
            "          69       0.35      0.32      0.33       120\n",
            "          70       0.24      0.22      0.23       113\n",
            "          71       0.25      0.23      0.24       120\n",
            "          72       0.27      0.28      0.28       120\n",
            "          73       0.36      0.36      0.36       120\n",
            "          74       0.19      0.22      0.20       120\n",
            "          75       0.32      0.37      0.34       120\n",
            "          76       0.34      0.29      0.31       120\n",
            "          77       0.28      0.41      0.34       120\n",
            "          78       0.36      0.39      0.38       120\n",
            "          79       0.30      0.42      0.35       120\n",
            "\n",
            "    accuracy                           0.29      9577\n",
            "   macro avg       0.30      0.29      0.29      9577\n",
            "weighted avg       0.30      0.29      0.29      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 10/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.66it/s, loss=3.2106, acc=31.39%]\n",
            "Epoch 10/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 245.65it/s, loss=2.4121, acc=30.98%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 10/500\n",
            "Train Loss: 2.5656, Train Acc: 31.39%\n",
            "Val Loss: 2.5142, Val Acc: 30.98%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.24      0.27      0.25       120\n",
            "           1       0.36      0.23      0.28       120\n",
            "           2       0.42      0.37      0.39       120\n",
            "           3       0.36      0.26      0.30       120\n",
            "           4       0.35      0.24      0.29       120\n",
            "           5       0.35      0.29      0.32       120\n",
            "           6       0.33      0.34      0.34       120\n",
            "           7       0.29      0.28      0.29       120\n",
            "           8       0.36      0.36      0.36       120\n",
            "           9       0.30      0.24      0.27       120\n",
            "          10       0.24      0.28      0.25       120\n",
            "          11       0.38      0.42      0.40       120\n",
            "          12       0.34      0.35      0.35       120\n",
            "          13       0.27      0.34      0.30       120\n",
            "          14       0.30      0.45      0.36       120\n",
            "          15       0.29      0.26      0.27       120\n",
            "          16       0.31      0.24      0.27       120\n",
            "          17       0.38      0.30      0.34       120\n",
            "          18       0.22      0.12      0.16       120\n",
            "          19       0.33      0.36      0.35       120\n",
            "          20       0.31      0.28      0.29       120\n",
            "          21       0.28      0.34      0.31       120\n",
            "          22       0.36      0.29      0.32       120\n",
            "          23       0.33      0.21      0.26       120\n",
            "          24       0.32      0.38      0.35       120\n",
            "          25       0.35      0.36      0.36       120\n",
            "          26       0.27      0.33      0.30       120\n",
            "          27       0.38      0.23      0.28       120\n",
            "          28       0.29      0.32      0.30       120\n",
            "          29       0.25      0.22      0.23       120\n",
            "          30       0.27      0.32      0.29       120\n",
            "          31       0.45      0.32      0.37       120\n",
            "          32       0.40      0.40      0.40       120\n",
            "          33       0.37      0.35      0.36       120\n",
            "          34       0.28      0.40      0.33       120\n",
            "          35       0.46      0.35      0.40       120\n",
            "          36       0.36      0.35      0.36       120\n",
            "          37       0.40      0.33      0.37       120\n",
            "          38       0.32      0.25      0.28       120\n",
            "          39       0.31      0.27      0.29       120\n",
            "          40       0.31      0.43      0.36       120\n",
            "          41       0.44      0.34      0.38       120\n",
            "          42       0.20      0.33      0.25       120\n",
            "          43       0.31      0.40      0.35       112\n",
            "          44       0.35      0.31      0.33       120\n",
            "          45       0.32      0.17      0.22       120\n",
            "          46       0.30      0.38      0.33       120\n",
            "          47       0.17      0.27      0.21       120\n",
            "          48       0.39      0.33      0.36       120\n",
            "          49       0.27      0.33      0.29       120\n",
            "          50       0.29      0.35      0.32       120\n",
            "          51       0.30      0.35      0.32       120\n",
            "          52       0.39      0.23      0.29       120\n",
            "          53       0.25      0.25      0.25       120\n",
            "          54       0.34      0.23      0.27       120\n",
            "          55       0.21      0.35      0.26       120\n",
            "          56       0.46      0.23      0.30       120\n",
            "          57       0.44      0.28      0.34       120\n",
            "          58       0.25      0.23      0.24       120\n",
            "          59       0.27      0.28      0.28       120\n",
            "          60       0.34      0.38      0.36       120\n",
            "          61       0.25      0.23      0.24       120\n",
            "          62       0.17      0.23      0.20       120\n",
            "          63       0.39      0.23      0.28       120\n",
            "          64       0.32      0.32      0.32       112\n",
            "          65       0.37      0.41      0.39       120\n",
            "          66       0.40      0.48      0.44       120\n",
            "          67       0.23      0.23      0.23       120\n",
            "          68       0.27      0.51      0.35       120\n",
            "          69       0.30      0.32      0.31       120\n",
            "          70       0.17      0.26      0.20       113\n",
            "          71       0.27      0.36      0.31       120\n",
            "          72       0.32      0.18      0.23       120\n",
            "          73       0.35      0.42      0.38       120\n",
            "          74       0.25      0.19      0.22       120\n",
            "          75       0.36      0.38      0.37       120\n",
            "          76       0.34      0.33      0.33       120\n",
            "          77       0.33      0.31      0.32       120\n",
            "          78       0.36      0.38      0.37       120\n",
            "          79       0.42      0.32      0.36       120\n",
            "\n",
            "    accuracy                           0.31      9577\n",
            "   macro avg       0.32      0.31      0.31      9577\n",
            "weighted avg       0.32      0.31      0.31      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 11/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.76it/s, loss=2.2931, acc=33.51%]\n",
            "Epoch 11/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 265.29it/s, loss=2.3867, acc=32.47%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 11/500\n",
            "Train Loss: 2.4489, Train Acc: 33.51%\n",
            "Val Loss: 2.4589, Val Acc: 32.47%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.32      0.31      0.31       120\n",
            "           1       0.27      0.39      0.32       120\n",
            "           2       0.31      0.40      0.35       120\n",
            "           3       0.37      0.38      0.38       120\n",
            "           4       0.31      0.32      0.32       120\n",
            "           5       0.40      0.29      0.34       120\n",
            "           6       0.37      0.28      0.32       120\n",
            "           7       0.38      0.31      0.34       120\n",
            "           8       0.39      0.31      0.35       120\n",
            "           9       0.28      0.32      0.30       120\n",
            "          10       0.43      0.19      0.26       120\n",
            "          11       0.45      0.39      0.42       120\n",
            "          12       0.39      0.38      0.38       120\n",
            "          13       0.29      0.35      0.32       120\n",
            "          14       0.34      0.37      0.35       120\n",
            "          15       0.35      0.22      0.27       120\n",
            "          16       0.22      0.18      0.20       120\n",
            "          17       0.32      0.32      0.32       120\n",
            "          18       0.32      0.26      0.29       120\n",
            "          19       0.39      0.38      0.38       120\n",
            "          20       0.29      0.32      0.31       120\n",
            "          21       0.24      0.41      0.30       120\n",
            "          22       0.32      0.28      0.30       120\n",
            "          23       0.29      0.38      0.33       120\n",
            "          24       0.37      0.28      0.32       120\n",
            "          25       0.25      0.30      0.27       120\n",
            "          26       0.35      0.31      0.33       120\n",
            "          27       0.39      0.31      0.35       120\n",
            "          28       0.30      0.35      0.32       120\n",
            "          29       0.38      0.33      0.35       120\n",
            "          30       0.28      0.23      0.25       120\n",
            "          31       0.46      0.30      0.36       120\n",
            "          32       0.35      0.37      0.36       120\n",
            "          33       0.38      0.34      0.36       120\n",
            "          34       0.27      0.29      0.28       120\n",
            "          35       0.39      0.48      0.43       120\n",
            "          36       0.32      0.27      0.29       120\n",
            "          37       0.45      0.29      0.36       120\n",
            "          38       0.23      0.47      0.30       120\n",
            "          39       0.43      0.33      0.38       120\n",
            "          40       0.37      0.31      0.34       120\n",
            "          41       0.41      0.44      0.42       120\n",
            "          42       0.26      0.23      0.25       120\n",
            "          43       0.32      0.31      0.32       112\n",
            "          44       0.35      0.25      0.29       120\n",
            "          45       0.29      0.26      0.27       120\n",
            "          46       0.23      0.36      0.28       120\n",
            "          47       0.25      0.32      0.28       120\n",
            "          48       0.28      0.43      0.34       120\n",
            "          49       0.31      0.30      0.31       120\n",
            "          50       0.52      0.26      0.34       120\n",
            "          51       0.40      0.33      0.36       120\n",
            "          52       0.62      0.29      0.40       120\n",
            "          53       0.31      0.30      0.31       120\n",
            "          54       0.35      0.34      0.34       120\n",
            "          55       0.27      0.43      0.33       120\n",
            "          56       0.26      0.35      0.30       120\n",
            "          57       0.35      0.24      0.29       120\n",
            "          58       0.21      0.13      0.16       120\n",
            "          59       0.31      0.42      0.36       120\n",
            "          60       0.26      0.25      0.25       120\n",
            "          61       0.29      0.20      0.24       120\n",
            "          62       0.22      0.23      0.22       120\n",
            "          63       0.31      0.45      0.36       120\n",
            "          64       0.35      0.34      0.34       112\n",
            "          65       0.34      0.52      0.41       120\n",
            "          66       0.37      0.32      0.34       120\n",
            "          67       0.29      0.35      0.32       120\n",
            "          68       0.38      0.40      0.39       120\n",
            "          69       0.27      0.33      0.30       120\n",
            "          70       0.23      0.23      0.23       113\n",
            "          71       0.36      0.27      0.31       120\n",
            "          72       0.43      0.24      0.31       120\n",
            "          73       0.35      0.42      0.38       120\n",
            "          74       0.29      0.22      0.25       120\n",
            "          75       0.38      0.45      0.41       120\n",
            "          76       0.42      0.31      0.36       120\n",
            "          77       0.43      0.39      0.41       120\n",
            "          78       0.32      0.46      0.38       120\n",
            "          79       0.34      0.35      0.34       120\n",
            "\n",
            "    accuracy                           0.32      9577\n",
            "   macro avg       0.34      0.32      0.32      9577\n",
            "weighted avg       0.34      0.32      0.32      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 12/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.43it/s, loss=3.4983, acc=35.24%]\n",
            "Epoch 12/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 275.53it/s, loss=3.0021, acc=32.78%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 12/500\n",
            "Train Loss: 2.3752, Train Acc: 35.24%\n",
            "Val Loss: 2.4646, Val Acc: 32.78%\n",
            "Learning rate: 0.000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 13/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.21it/s, loss=2.1625, acc=37.43%]\n",
            "Epoch 13/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 277.32it/s, loss=2.0999, acc=32.08%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 13/500\n",
            "Train Loss: 2.2891, Train Acc: 37.43%\n",
            "Val Loss: 2.4586, Val Acc: 32.08%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.38      0.30      0.33       120\n",
            "           1       0.30      0.38      0.33       120\n",
            "           2       0.36      0.42      0.39       120\n",
            "           3       0.35      0.27      0.30       120\n",
            "           4       0.37      0.25      0.30       120\n",
            "           5       0.23      0.26      0.24       120\n",
            "           6       0.33      0.45      0.38       120\n",
            "           7       0.31      0.33      0.32       120\n",
            "           8       0.34      0.34      0.34       120\n",
            "           9       0.29      0.32      0.31       120\n",
            "          10       0.28      0.33      0.30       120\n",
            "          11       0.45      0.44      0.45       120\n",
            "          12       0.43      0.37      0.40       120\n",
            "          13       0.30      0.31      0.30       120\n",
            "          14       0.38      0.30      0.34       120\n",
            "          15       0.24      0.37      0.29       120\n",
            "          16       0.18      0.23      0.20       120\n",
            "          17       0.24      0.35      0.28       120\n",
            "          18       0.25      0.26      0.25       120\n",
            "          19       0.31      0.33      0.32       120\n",
            "          20       0.27      0.42      0.33       120\n",
            "          21       0.32      0.34      0.33       120\n",
            "          22       0.40      0.28      0.33       120\n",
            "          23       0.36      0.44      0.40       120\n",
            "          24       0.47      0.31      0.37       120\n",
            "          25       0.33      0.32      0.32       120\n",
            "          26       0.29      0.23      0.25       120\n",
            "          27       0.45      0.33      0.38       120\n",
            "          28       0.28      0.31      0.29       120\n",
            "          29       0.33      0.31      0.32       120\n",
            "          30       0.28      0.37      0.32       120\n",
            "          31       0.40      0.31      0.35       120\n",
            "          32       0.51      0.28      0.36       120\n",
            "          33       0.37      0.39      0.38       120\n",
            "          34       0.22      0.35      0.27       120\n",
            "          35       0.42      0.45      0.43       120\n",
            "          36       0.29      0.28      0.28       120\n",
            "          37       0.29      0.53      0.37       120\n",
            "          38       0.33      0.23      0.27       120\n",
            "          39       0.45      0.42      0.44       120\n",
            "          40       0.33      0.40      0.36       120\n",
            "          41       0.43      0.40      0.41       120\n",
            "          42       0.29      0.24      0.26       120\n",
            "          43       0.38      0.36      0.37       112\n",
            "          44       0.38      0.33      0.35       120\n",
            "          45       0.26      0.28      0.27       120\n",
            "          46       0.38      0.24      0.29       120\n",
            "          47       0.27      0.28      0.28       120\n",
            "          48       0.30      0.36      0.33       120\n",
            "          49       0.24      0.23      0.23       120\n",
            "          50       0.54      0.33      0.41       120\n",
            "          51       0.30      0.23      0.26       120\n",
            "          52       0.38      0.31      0.34       120\n",
            "          53       0.28      0.23      0.25       120\n",
            "          54       0.31      0.28      0.30       120\n",
            "          55       0.31      0.38      0.34       120\n",
            "          56       0.29      0.23      0.25       120\n",
            "          57       0.42      0.21      0.28       120\n",
            "          58       0.23      0.16      0.19       120\n",
            "          59       0.43      0.27      0.33       120\n",
            "          60       0.32      0.28      0.30       120\n",
            "          61       0.26      0.24      0.25       120\n",
            "          62       0.15      0.34      0.21       120\n",
            "          63       0.39      0.41      0.40       120\n",
            "          64       0.38      0.41      0.39       112\n",
            "          65       0.51      0.35      0.41       120\n",
            "          66       0.33      0.33      0.33       120\n",
            "          67       0.39      0.22      0.28       120\n",
            "          68       0.26      0.47      0.33       120\n",
            "          69       0.28      0.23      0.25       120\n",
            "          70       0.21      0.29      0.24       113\n",
            "          71       0.31      0.21      0.25       120\n",
            "          72       0.34      0.27      0.30       120\n",
            "          73       0.32      0.38      0.35       120\n",
            "          74       0.25      0.18      0.21       120\n",
            "          75       0.36      0.25      0.29       120\n",
            "          76       0.26      0.31      0.28       120\n",
            "          77       0.41      0.47      0.44       120\n",
            "          78       0.49      0.36      0.41       120\n",
            "          79       0.39      0.47      0.42       120\n",
            "\n",
            "    accuracy                           0.32      9577\n",
            "   macro avg       0.33      0.32      0.32      9577\n",
            "weighted avg       0.33      0.32      0.32      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 14/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.09it/s, loss=2.6738, acc=38.90%]\n",
            "Epoch 14/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 275.10it/s, loss=2.5460, acc=33.99%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 14/500\n",
            "Train Loss: 2.2128, Train Acc: 38.90%\n",
            "Val Loss: 2.4033, Val Acc: 33.99%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.26      0.38      0.31       120\n",
            "           1       0.35      0.43      0.39       120\n",
            "           2       0.36      0.45      0.40       120\n",
            "           3       0.30      0.41      0.35       120\n",
            "           4       0.32      0.33      0.33       120\n",
            "           5       0.34      0.29      0.31       120\n",
            "           6       0.35      0.30      0.32       120\n",
            "           7       0.32      0.30      0.31       120\n",
            "           8       0.34      0.33      0.33       120\n",
            "           9       0.30      0.38      0.33       120\n",
            "          10       0.37      0.28      0.32       120\n",
            "          11       0.46      0.49      0.47       120\n",
            "          12       0.32      0.45      0.37       120\n",
            "          13       0.28      0.27      0.27       120\n",
            "          14       0.39      0.29      0.33       120\n",
            "          15       0.37      0.24      0.29       120\n",
            "          16       0.27      0.17      0.21       120\n",
            "          17       0.32      0.35      0.33       120\n",
            "          18       0.30      0.34      0.32       120\n",
            "          19       0.39      0.37      0.38       120\n",
            "          20       0.34      0.44      0.38       120\n",
            "          21       0.34      0.26      0.29       120\n",
            "          22       0.29      0.33      0.31       120\n",
            "          23       0.32      0.38      0.35       120\n",
            "          24       0.47      0.34      0.39       120\n",
            "          25       0.27      0.42      0.33       120\n",
            "          26       0.41      0.33      0.37       120\n",
            "          27       0.28      0.45      0.35       120\n",
            "          28       0.32      0.27      0.29       120\n",
            "          29       0.42      0.31      0.35       120\n",
            "          30       0.39      0.38      0.38       120\n",
            "          31       0.46      0.37      0.41       120\n",
            "          32       0.37      0.46      0.41       120\n",
            "          33       0.40      0.35      0.38       120\n",
            "          34       0.32      0.33      0.32       120\n",
            "          35       0.44      0.51      0.47       120\n",
            "          36       0.33      0.29      0.31       120\n",
            "          37       0.30      0.43      0.35       120\n",
            "          38       0.29      0.23      0.26       120\n",
            "          39       0.45      0.39      0.42       120\n",
            "          40       0.50      0.41      0.45       120\n",
            "          41       0.48      0.40      0.43       120\n",
            "          42       0.33      0.21      0.26       120\n",
            "          43       0.31      0.33      0.32       112\n",
            "          44       0.36      0.37      0.36       120\n",
            "          45       0.28      0.21      0.24       120\n",
            "          46       0.32      0.21      0.25       120\n",
            "          47       0.29      0.40      0.34       120\n",
            "          48       0.34      0.40      0.37       120\n",
            "          49       0.33      0.43      0.37       120\n",
            "          50       0.37      0.33      0.35       120\n",
            "          51       0.30      0.30      0.30       120\n",
            "          52       0.38      0.34      0.36       120\n",
            "          53       0.34      0.27      0.30       120\n",
            "          54       0.33      0.38      0.35       120\n",
            "          55       0.38      0.28      0.32       120\n",
            "          56       0.42      0.28      0.34       120\n",
            "          57       0.31      0.23      0.27       120\n",
            "          58       0.28      0.24      0.26       120\n",
            "          59       0.28      0.37      0.32       120\n",
            "          60       0.29      0.27      0.28       120\n",
            "          61       0.31      0.28      0.30       120\n",
            "          62       0.26      0.23      0.24       120\n",
            "          63       0.32      0.27      0.29       120\n",
            "          64       0.33      0.42      0.37       112\n",
            "          65       0.42      0.37      0.39       120\n",
            "          66       0.37      0.35      0.36       120\n",
            "          67       0.34      0.33      0.33       120\n",
            "          68       0.43      0.36      0.39       120\n",
            "          69       0.38      0.34      0.36       120\n",
            "          70       0.21      0.23      0.22       113\n",
            "          71       0.29      0.30      0.30       120\n",
            "          72       0.27      0.22      0.24       120\n",
            "          73       0.36      0.44      0.39       120\n",
            "          74       0.31      0.28      0.29       120\n",
            "          75       0.42      0.46      0.44       120\n",
            "          76       0.35      0.42      0.39       120\n",
            "          77       0.27      0.38      0.32       120\n",
            "          78       0.45      0.38      0.41       120\n",
            "          79       0.36      0.37      0.36       120\n",
            "\n",
            "    accuracy                           0.34      9577\n",
            "   macro avg       0.34      0.34      0.34      9577\n",
            "weighted avg       0.34      0.34      0.34      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 15/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.75it/s, loss=1.9956, acc=41.07%]\n",
            "Epoch 15/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 271.08it/s, loss=1.7115, acc=34.43%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 15/500\n",
            "Train Loss: 2.1281, Train Acc: 41.07%\n",
            "Val Loss: 2.3916, Val Acc: 34.43%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.32      0.34       120\n",
            "           1       0.37      0.34      0.35       120\n",
            "           2       0.50      0.43      0.46       120\n",
            "           3       0.39      0.31      0.35       120\n",
            "           4       0.27      0.31      0.29       120\n",
            "           5       0.30      0.26      0.28       120\n",
            "           6       0.36      0.38      0.37       120\n",
            "           7       0.32      0.28      0.30       120\n",
            "           8       0.38      0.30      0.33       120\n",
            "           9       0.40      0.30      0.34       120\n",
            "          10       0.29      0.31      0.30       120\n",
            "          11       0.39      0.46      0.42       120\n",
            "          12       0.35      0.34      0.34       120\n",
            "          13       0.29      0.33      0.31       120\n",
            "          14       0.31      0.30      0.30       120\n",
            "          15       0.35      0.44      0.39       120\n",
            "          16       0.24      0.29      0.27       120\n",
            "          17       0.36      0.39      0.37       120\n",
            "          18       0.33      0.25      0.28       120\n",
            "          19       0.46      0.38      0.41       120\n",
            "          20       0.35      0.47      0.40       120\n",
            "          21       0.36      0.36      0.36       120\n",
            "          22       0.28      0.34      0.31       120\n",
            "          23       0.36      0.42      0.39       120\n",
            "          24       0.37      0.48      0.42       120\n",
            "          25       0.33      0.30      0.31       120\n",
            "          26       0.45      0.36      0.40       120\n",
            "          27       0.37      0.38      0.38       120\n",
            "          28       0.36      0.30      0.33       120\n",
            "          29       0.32      0.23      0.27       120\n",
            "          30       0.39      0.33      0.36       120\n",
            "          31       0.37      0.46      0.41       120\n",
            "          32       0.34      0.45      0.39       120\n",
            "          33       0.35      0.47      0.40       120\n",
            "          34       0.38      0.27      0.31       120\n",
            "          35       0.37      0.43      0.40       120\n",
            "          36       0.32      0.37      0.34       120\n",
            "          37       0.41      0.38      0.39       120\n",
            "          38       0.30      0.30      0.30       120\n",
            "          39       0.39      0.40      0.40       120\n",
            "          40       0.43      0.38      0.41       120\n",
            "          41       0.35      0.40      0.37       120\n",
            "          42       0.28      0.29      0.29       120\n",
            "          43       0.31      0.34      0.33       112\n",
            "          44       0.32      0.31      0.32       120\n",
            "          45       0.28      0.33      0.30       120\n",
            "          46       0.30      0.30      0.30       120\n",
            "          47       0.31      0.37      0.34       120\n",
            "          48       0.38      0.42      0.40       120\n",
            "          49       0.29      0.29      0.29       120\n",
            "          50       0.40      0.42      0.41       120\n",
            "          51       0.35      0.30      0.32       120\n",
            "          52       0.32      0.29      0.30       120\n",
            "          53       0.30      0.42      0.35       120\n",
            "          54       0.38      0.30      0.33       120\n",
            "          55       0.31      0.23      0.26       120\n",
            "          56       0.32      0.23      0.26       120\n",
            "          57       0.38      0.29      0.33       120\n",
            "          58       0.39      0.16      0.22       120\n",
            "          59       0.31      0.21      0.25       120\n",
            "          60       0.29      0.23      0.26       120\n",
            "          61       0.28      0.31      0.30       120\n",
            "          62       0.20      0.28      0.23       120\n",
            "          63       0.52      0.38      0.44       120\n",
            "          64       0.32      0.42      0.36       112\n",
            "          65       0.40      0.49      0.44       120\n",
            "          66       0.38      0.28      0.32       120\n",
            "          67       0.33      0.30      0.31       120\n",
            "          68       0.30      0.46      0.37       120\n",
            "          69       0.30      0.40      0.34       120\n",
            "          70       0.24      0.19      0.21       113\n",
            "          71       0.40      0.23      0.29       120\n",
            "          72       0.31      0.32      0.31       120\n",
            "          73       0.36      0.53      0.43       120\n",
            "          74       0.36      0.28      0.32       120\n",
            "          75       0.38      0.41      0.40       120\n",
            "          76       0.48      0.41      0.44       120\n",
            "          77       0.34      0.41      0.37       120\n",
            "          78       0.45      0.38      0.41       120\n",
            "          79       0.32      0.36      0.34       120\n",
            "\n",
            "    accuracy                           0.34      9577\n",
            "   macro avg       0.35      0.34      0.34      9577\n",
            "weighted avg       0.35      0.34      0.34      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 16/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.11it/s, loss=1.9514, acc=42.42%]\n",
            "Epoch 16/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 238.99it/s, loss=1.8921, acc=33.87%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 16/500\n",
            "Train Loss: 2.0693, Train Acc: 42.42%\n",
            "Val Loss: 2.3859, Val Acc: 33.87%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.31      0.28      0.29       120\n",
            "           1       0.45      0.33      0.38       120\n",
            "           2       0.41      0.33      0.37       120\n",
            "           3       0.42      0.34      0.38       120\n",
            "           4       0.38      0.29      0.33       120\n",
            "           5       0.45      0.23      0.30       120\n",
            "           6       0.38      0.33      0.35       120\n",
            "           7       0.37      0.32      0.34       120\n",
            "           8       0.34      0.42      0.38       120\n",
            "           9       0.30      0.27      0.28       120\n",
            "          10       0.30      0.27      0.28       120\n",
            "          11       0.58      0.40      0.47       120\n",
            "          12       0.36      0.39      0.38       120\n",
            "          13       0.26      0.33      0.29       120\n",
            "          14       0.43      0.29      0.35       120\n",
            "          15       0.27      0.29      0.28       120\n",
            "          16       0.22      0.26      0.24       120\n",
            "          17       0.44      0.34      0.38       120\n",
            "          18       0.24      0.24      0.24       120\n",
            "          19       0.39      0.40      0.39       120\n",
            "          20       0.43      0.41      0.42       120\n",
            "          21       0.37      0.31      0.33       120\n",
            "          22       0.39      0.27      0.32       120\n",
            "          23       0.37      0.35      0.36       120\n",
            "          24       0.42      0.29      0.34       120\n",
            "          25       0.24      0.16      0.19       120\n",
            "          26       0.36      0.29      0.32       120\n",
            "          27       0.34      0.36      0.35       120\n",
            "          28       0.30      0.30      0.30       120\n",
            "          29       0.35      0.40      0.37       120\n",
            "          30       0.39      0.28      0.32       120\n",
            "          31       0.29      0.42      0.34       120\n",
            "          32       0.32      0.42      0.36       120\n",
            "          33       0.33      0.36      0.34       120\n",
            "          34       0.42      0.35      0.38       120\n",
            "          35       0.49      0.52      0.50       120\n",
            "          36       0.35      0.38      0.36       120\n",
            "          37       0.32      0.40      0.35       120\n",
            "          38       0.25      0.40      0.31       120\n",
            "          39       0.40      0.31      0.35       120\n",
            "          40       0.31      0.39      0.35       120\n",
            "          41       0.30      0.39      0.34       120\n",
            "          42       0.31      0.29      0.30       120\n",
            "          43       0.27      0.35      0.30       112\n",
            "          44       0.37      0.41      0.39       120\n",
            "          45       0.31      0.33      0.32       120\n",
            "          46       0.37      0.27      0.31       120\n",
            "          47       0.33      0.27      0.30       120\n",
            "          48       0.35      0.38      0.36       120\n",
            "          49       0.26      0.28      0.27       120\n",
            "          50       0.49      0.38      0.43       120\n",
            "          51       0.29      0.32      0.30       120\n",
            "          52       0.33      0.37      0.35       120\n",
            "          53       0.30      0.24      0.27       120\n",
            "          54       0.46      0.28      0.34       120\n",
            "          55       0.28      0.38      0.32       120\n",
            "          56       0.31      0.30      0.30       120\n",
            "          57       0.28      0.28      0.28       120\n",
            "          58       0.25      0.19      0.22       120\n",
            "          59       0.37      0.34      0.36       120\n",
            "          60       0.24      0.36      0.29       120\n",
            "          61       0.34      0.29      0.31       120\n",
            "          62       0.24      0.33      0.28       120\n",
            "          63       0.40      0.34      0.37       120\n",
            "          64       0.30      0.43      0.35       112\n",
            "          65       0.50      0.43      0.47       120\n",
            "          66       0.32      0.43      0.36       120\n",
            "          67       0.43      0.31      0.36       120\n",
            "          68       0.34      0.42      0.38       120\n",
            "          69       0.35      0.41      0.38       120\n",
            "          70       0.29      0.28      0.28       113\n",
            "          71       0.32      0.31      0.31       120\n",
            "          72       0.29      0.30      0.30       120\n",
            "          73       0.33      0.43      0.37       120\n",
            "          74       0.28      0.30      0.29       120\n",
            "          75       0.38      0.32      0.35       120\n",
            "          76       0.36      0.46      0.40       120\n",
            "          77       0.36      0.45      0.40       120\n",
            "          78       0.42      0.39      0.40       120\n",
            "          79       0.39      0.38      0.38       120\n",
            "\n",
            "    accuracy                           0.34      9577\n",
            "   macro avg       0.35      0.34      0.34      9577\n",
            "weighted avg       0.35      0.34      0.34      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 17/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.18it/s, loss=3.6720, acc=44.04%]\n",
            "Epoch 17/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 254.08it/s, loss=2.3777, acc=35.50%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 17/500\n",
            "Train Loss: 2.0088, Train Acc: 44.04%\n",
            "Val Loss: 2.3726, Val Acc: 35.50%\n",
            "Learning rate: 0.000500\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.30      0.23      0.26       120\n",
            "           1       0.42      0.41      0.41       120\n",
            "           2       0.39      0.39      0.39       120\n",
            "           3       0.36      0.32      0.33       120\n",
            "           4       0.26      0.44      0.33       120\n",
            "           5       0.37      0.37      0.37       120\n",
            "           6       0.43      0.36      0.39       120\n",
            "           7       0.38      0.39      0.39       120\n",
            "           8       0.46      0.33      0.39       120\n",
            "           9       0.30      0.32      0.31       120\n",
            "          10       0.35      0.33      0.34       120\n",
            "          11       0.43      0.47      0.45       120\n",
            "          12       0.37      0.44      0.40       120\n",
            "          13       0.39      0.38      0.38       120\n",
            "          14       0.40      0.38      0.39       120\n",
            "          15       0.45      0.29      0.35       120\n",
            "          16       0.27      0.28      0.27       120\n",
            "          17       0.25      0.34      0.29       120\n",
            "          18       0.27      0.28      0.28       120\n",
            "          19       0.44      0.42      0.43       120\n",
            "          20       0.45      0.38      0.41       120\n",
            "          21       0.39      0.32      0.35       120\n",
            "          22       0.37      0.39      0.38       120\n",
            "          23       0.38      0.39      0.39       120\n",
            "          24       0.35      0.40      0.37       120\n",
            "          25       0.35      0.35      0.35       120\n",
            "          26       0.29      0.29      0.29       120\n",
            "          27       0.35      0.33      0.33       120\n",
            "          28       0.50      0.26      0.34       120\n",
            "          29       0.34      0.34      0.34       120\n",
            "          30       0.38      0.33      0.35       120\n",
            "          31       0.36      0.43      0.39       120\n",
            "          32       0.37      0.42      0.39       120\n",
            "          33       0.41      0.36      0.38       120\n",
            "          34       0.31      0.33      0.32       120\n",
            "          35       0.45      0.57      0.50       120\n",
            "          36       0.28      0.40      0.33       120\n",
            "          37       0.41      0.44      0.43       120\n",
            "          38       0.38      0.36      0.37       120\n",
            "          39       0.38      0.40      0.39       120\n",
            "          40       0.48      0.39      0.43       120\n",
            "          41       0.43      0.36      0.39       120\n",
            "          42       0.27      0.37      0.31       120\n",
            "          43       0.31      0.28      0.29       112\n",
            "          44       0.31      0.38      0.34       120\n",
            "          45       0.32      0.28      0.30       120\n",
            "          46       0.33      0.35      0.34       120\n",
            "          47       0.32      0.28      0.30       120\n",
            "          48       0.39      0.29      0.33       120\n",
            "          49       0.36      0.39      0.37       120\n",
            "          50       0.41      0.40      0.41       120\n",
            "          51       0.32      0.40      0.35       120\n",
            "          52       0.41      0.31      0.35       120\n",
            "          53       0.41      0.22      0.28       120\n",
            "          54       0.37      0.35      0.36       120\n",
            "          55       0.31      0.22      0.26       120\n",
            "          56       0.25      0.31      0.28       120\n",
            "          57       0.44      0.30      0.36       120\n",
            "          58       0.22      0.31      0.26       120\n",
            "          59       0.36      0.33      0.34       120\n",
            "          60       0.43      0.28      0.34       120\n",
            "          61       0.32      0.31      0.32       120\n",
            "          62       0.29      0.28      0.28       120\n",
            "          63       0.33      0.33      0.33       120\n",
            "          64       0.38      0.47      0.42       112\n",
            "          65       0.42      0.43      0.42       120\n",
            "          66       0.33      0.44      0.38       120\n",
            "          67       0.38      0.23      0.29       120\n",
            "          68       0.41      0.47      0.44       120\n",
            "          69       0.32      0.30      0.31       120\n",
            "          70       0.24      0.24      0.24       113\n",
            "          71       0.34      0.22      0.26       120\n",
            "          72       0.29      0.31      0.30       120\n",
            "          73       0.34      0.48      0.40       120\n",
            "          74       0.30      0.30      0.30       120\n",
            "          75       0.43      0.46      0.44       120\n",
            "          76       0.38      0.38      0.38       120\n",
            "          77       0.39      0.42      0.40       120\n",
            "          78       0.43      0.41      0.42       120\n",
            "          79       0.35      0.47      0.40       120\n",
            "\n",
            "    accuracy                           0.36      9577\n",
            "   macro avg       0.36      0.35      0.35      9577\n",
            "weighted avg       0.36      0.36      0.35      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 18/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 57.00it/s, loss=2.3097, acc=45.77%]\n",
            "Epoch 18/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 266.90it/s, loss=3.0505, acc=35.32%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 18/500\n",
            "Train Loss: 1.9418, Train Acc: 45.77%\n",
            "Val Loss: 2.3976, Val Acc: 35.32%\n",
            "Learning rate: 0.000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 19/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.81it/s, loss=2.0897, acc=46.75%]\n",
            "Epoch 19/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 269.18it/s, loss=1.8921, acc=35.78%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 19/500\n",
            "Train Loss: 1.8987, Train Acc: 46.75%\n",
            "Val Loss: 2.3922, Val Acc: 35.78%\n",
            "Learning rate: 0.000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 20/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.95it/s, loss=2.7499, acc=48.28%]\n",
            "Epoch 20/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 266.44it/s, loss=2.2913, acc=35.44%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 20/500\n",
            "Train Loss: 1.8430, Train Acc: 48.28%\n",
            "Val Loss: 2.4106, Val Acc: 35.44%\n",
            "Learning rate: 0.000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 21/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.42it/s, loss=2.1185, acc=49.52%]\n",
            "Epoch 21/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 231.53it/s, loss=2.4021, acc=35.13%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 21/500\n",
            "Train Loss: 1.7803, Train Acc: 49.52%\n",
            "Val Loss: 2.4223, Val Acc: 35.13%\n",
            "Learning rate: 0.000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 22/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.70it/s, loss=3.0028, acc=51.09%]\n",
            "Epoch 22/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 249.28it/s, loss=3.1948, acc=35.37%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 22/500\n",
            "Train Loss: 1.7302, Train Acc: 51.09%\n",
            "Val Loss: 2.4152, Val Acc: 35.37%\n",
            "Learning rate: 0.000500\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 23/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.15it/s, loss=2.4447, acc=51.86%]\n",
            "Epoch 23/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 262.68it/s, loss=2.5199, acc=35.56%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 23/500\n",
            "Train Loss: 1.7005, Train Acc: 51.86%\n",
            "Val Loss: 2.4166, Val Acc: 35.56%\n",
            "Learning rate: 0.000250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 24/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.80it/s, loss=0.9939, acc=60.26%]\n",
            "Epoch 24/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 272.51it/s, loss=2.1231, acc=38.19%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 24/500\n",
            "Train Loss: 1.3768, Train Acc: 60.26%\n",
            "Val Loss: 2.2974, Val Acc: 38.19%\n",
            "Learning rate: 0.000250\n",
            "New best model saved!\n",
            "\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "           0       0.36      0.38      0.37       120\n",
            "           1       0.45      0.40      0.42       120\n",
            "           2       0.44      0.43      0.44       120\n",
            "           3       0.48      0.35      0.40       120\n",
            "           4       0.35      0.42      0.38       120\n",
            "           5       0.35      0.39      0.37       120\n",
            "           6       0.40      0.41      0.40       120\n",
            "           7       0.43      0.35      0.39       120\n",
            "           8       0.43      0.46      0.44       120\n",
            "           9       0.31      0.33      0.32       120\n",
            "          10       0.36      0.29      0.32       120\n",
            "          11       0.51      0.50      0.51       120\n",
            "          12       0.40      0.42      0.41       120\n",
            "          13       0.36      0.41      0.38       120\n",
            "          14       0.40      0.40      0.40       120\n",
            "          15       0.26      0.28      0.27       120\n",
            "          16       0.22      0.25      0.23       120\n",
            "          17       0.29      0.46      0.35       120\n",
            "          18       0.27      0.44      0.34       120\n",
            "          19       0.42      0.43      0.43       120\n",
            "          20       0.40      0.46      0.43       120\n",
            "          21       0.43      0.38      0.40       120\n",
            "          22       0.33      0.39      0.36       120\n",
            "          23       0.40      0.36      0.38       120\n",
            "          24       0.44      0.39      0.42       120\n",
            "          25       0.35      0.32      0.33       120\n",
            "          26       0.43      0.33      0.37       120\n",
            "          27       0.37      0.41      0.39       120\n",
            "          28       0.36      0.36      0.36       120\n",
            "          29       0.41      0.38      0.39       120\n",
            "          30       0.46      0.26      0.33       120\n",
            "          31       0.46      0.44      0.45       120\n",
            "          32       0.48      0.40      0.44       120\n",
            "          33       0.52      0.39      0.45       120\n",
            "          34       0.43      0.31      0.36       120\n",
            "          35       0.54      0.57      0.56       120\n",
            "          36       0.34      0.41      0.37       120\n",
            "          37       0.38      0.44      0.41       120\n",
            "          38       0.38      0.42      0.40       120\n",
            "          39       0.43      0.42      0.42       120\n",
            "          40       0.51      0.45      0.48       120\n",
            "          41       0.42      0.37      0.39       120\n",
            "          42       0.35      0.28      0.31       120\n",
            "          43       0.35      0.39      0.37       112\n",
            "          44       0.44      0.38      0.41       120\n",
            "          45       0.27      0.29      0.28       120\n",
            "          46       0.51      0.33      0.40       120\n",
            "          47       0.34      0.30      0.32       120\n",
            "          48       0.37      0.43      0.40       120\n",
            "          49       0.44      0.35      0.39       120\n",
            "          50       0.41      0.35      0.38       120\n",
            "          51       0.28      0.32      0.29       120\n",
            "          52       0.40      0.34      0.37       120\n",
            "          53       0.44      0.37      0.40       120\n",
            "          54       0.41      0.34      0.37       120\n",
            "          55       0.37      0.26      0.30       120\n",
            "          56       0.45      0.32      0.37       120\n",
            "          57       0.38      0.37      0.37       120\n",
            "          58       0.26      0.28      0.27       120\n",
            "          59       0.29      0.35      0.32       120\n",
            "          60       0.45      0.39      0.42       120\n",
            "          61       0.34      0.35      0.35       120\n",
            "          62       0.25      0.38      0.30       120\n",
            "          63       0.38      0.42      0.40       120\n",
            "          64       0.35      0.49      0.41       112\n",
            "          65       0.50      0.49      0.50       120\n",
            "          66       0.39      0.41      0.40       120\n",
            "          67       0.41      0.31      0.35       120\n",
            "          68       0.36      0.46      0.40       120\n",
            "          69       0.36      0.45      0.40       120\n",
            "          70       0.27      0.30      0.28       113\n",
            "          71       0.28      0.29      0.29       120\n",
            "          72       0.36      0.31      0.33       120\n",
            "          73       0.41      0.55      0.47       120\n",
            "          74       0.34      0.23      0.28       120\n",
            "          75       0.45      0.42      0.43       120\n",
            "          76       0.43      0.43      0.43       120\n",
            "          77       0.50      0.42      0.45       120\n",
            "          78       0.45      0.47      0.46       120\n",
            "          79       0.38      0.49      0.43       120\n",
            "\n",
            "    accuracy                           0.38      9577\n",
            "   macro avg       0.39      0.38      0.38      9577\n",
            "weighted avg       0.39      0.38      0.38      9577\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 25/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.84it/s, loss=3.0019, acc=63.50%]\n",
            "Epoch 25/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 273.73it/s, loss=2.1444, acc=37.91%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 25/500\n",
            "Train Loss: 1.2408, Train Acc: 63.50%\n",
            "Val Loss: 2.3682, Val Acc: 37.91%\n",
            "Learning rate: 0.000250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 26/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.66it/s, loss=2.3432, acc=65.94%]\n",
            "Epoch 26/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 267.14it/s, loss=2.1130, acc=37.95%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 26/500\n",
            "Train Loss: 1.1613, Train Acc: 65.94%\n",
            "Val Loss: 2.4135, Val Acc: 37.95%\n",
            "Learning rate: 0.000250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 27/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.55it/s, loss=1.6739, acc=67.58%]\n",
            "Epoch 27/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 266.40it/s, loss=2.4837, acc=37.79%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 27/500\n",
            "Train Loss: 1.0947, Train Acc: 67.58%\n",
            "Val Loss: 2.4314, Val Acc: 37.79%\n",
            "Learning rate: 0.000250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 28/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.39it/s, loss=0.8826, acc=68.99%]\n",
            "Epoch 28/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 251.42it/s, loss=2.3061, acc=38.44%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 28/500\n",
            "Train Loss: 1.0523, Train Acc: 68.99%\n",
            "Val Loss: 2.4601, Val Acc: 38.44%\n",
            "Learning rate: 0.000250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 29/500 [Train]: 100%|██████████| 2794/2794 [00:48<00:00, 57.04it/s, loss=1.3826, acc=69.97%]\n",
            "Epoch 29/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 255.39it/s, loss=2.7166, acc=37.69%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 29/500\n",
            "Train Loss: 1.0113, Train Acc: 69.97%\n",
            "Val Loss: 2.5185, Val Acc: 37.69%\n",
            "Learning rate: 0.000250\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 30/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.76it/s, loss=2.3506, acc=71.06%]\n",
            "Epoch 30/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 267.35it/s, loss=2.9438, acc=38.24%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 30/500\n",
            "Train Loss: 0.9684, Train Acc: 71.06%\n",
            "Val Loss: 2.5319, Val Acc: 38.24%\n",
            "Learning rate: 0.000125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 31/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.98it/s, loss=2.7477, acc=75.58%]\n",
            "Epoch 31/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 267.52it/s, loss=2.1529, acc=39.25%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 31/500\n",
            "Train Loss: 0.8149, Train Acc: 75.58%\n",
            "Val Loss: 2.5312, Val Acc: 39.25%\n",
            "Learning rate: 0.000125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 32/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.60it/s, loss=2.3420, acc=77.81%]\n",
            "Epoch 32/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 264.58it/s, loss=2.7265, acc=39.24%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 32/500\n",
            "Train Loss: 0.7271, Train Acc: 77.81%\n",
            "Val Loss: 2.5409, Val Acc: 39.24%\n",
            "Learning rate: 0.000125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 33/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 57.00it/s, loss=2.6924, acc=78.62%]\n",
            "Epoch 33/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 248.26it/s, loss=2.1401, acc=39.44%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 33/500\n",
            "Train Loss: 0.7018, Train Acc: 78.62%\n",
            "Val Loss: 2.5464, Val Acc: 39.44%\n",
            "Learning rate: 0.000125\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Epoch 34/500 [Train]: 100%|██████████| 2794/2794 [00:49<00:00, 56.69it/s, loss=1.3944, acc=79.83%]\n",
            "Epoch 34/500 [Val]: 100%|██████████| 599/599 [00:02<00:00, 256.69it/s, loss=3.9435, acc=39.24%]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Epoch 34/500\n",
            "Train Loss: 0.6625, Train Acc: 79.83%\n",
            "Val Loss: 2.6649, Val Acc: 39.24%\n",
            "Learning rate: 0.000125\n",
            "Early stopping triggered after 34 epochs\n",
            "\n",
            "Evaluating model on test set...\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Evaluating: 100%|██████████| 599/599 [00:01<00:00, 411.14it/s]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "Final Test Accuracy: 38.19%\n",
            "\n",
            "Model saved successfully!\n",
            "\n",
            "EEG classification pipeline complete!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "mJsFAH8U-qN7"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}